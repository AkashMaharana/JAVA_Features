
So what is an equivalence relation? Loosely speaking, it’s an operator that
partitions a set of elements into subsets whose elements are deemed equal to
one another. These subsets are known as equivalence classes. For an equals
method to be useful, all of the elements in each equivalence class must be
interchangeable from the perspective of the user. Now let’s examine the five
requirements in turn:
Reflexivity—The first requirement says merely that an object must be
equal to itself. It’s hard to imagine violating this one unintentionally. If you
were to violate it and then add an instance of your class to a collection, the
contains method might well say that the collection didn’t contain the
instance that you just added.
Symmetry—The second requirement says that any two objects must agree
on whether they are equal. Unlike the first requirement, it’s not hard to
imagine violating this one unintentionally. For example, consider the
following class, which implements a case-insensitive string. The case of the
string is preserved by toString but ignored in equals comparisons:
Click here to view code image
// Broken - violates symmetry!
public final class CaseInsensitiveString {
private final String s;
public CaseInsensitiveString(String s) {
this.s = Objects.requireNonNull(s);
60
}
// Broken - violates symmetry!
@Override public boolean equals(Object o) {
if (o instanceof CaseInsensitiveString)
return s.equalsIgnoreCase(
((CaseInsensitiveString) o).s);
if (o instanceof String) // One-way
interoperability!
return s.equalsIgnoreCase((String) o);
return false;
}
... // Remainder omitted
}
The well-intentioned equals method in this class naively attempts to
interoperate with ordinary strings. Let’s suppose that we have one caseinsensitive string and one ordinary one:
Click here to view code image
CaseInsensitiveString cis = new
CaseInsensitiveString("Polish");
String s = "polish";
As expected, cis.equals(s) returns true. The problem is that while
the equals method in CaseInsensitiveString knows about ordinary
strings, the equals method in String is oblivious to case-insensitive
strings. Therefore, s.equals(cis) returns false, a clear violation of
symmetry. Suppose you put a case-insensitive string into a collection:
Click here to view code image
List<CaseInsensitiveString> list = new ArrayList<>();
list.add(cis);
What does list.contains(s) return at this point? Who knows? In the
current OpenJDK implementation, it happens to return false, but that’s just
an implementation artifact. In another implementation, it could just as easily
return true or throw a runtime exception. Once you’ve violated the
equals contract, you simply don’t know how other objects will behave
when confronted with your object.
To eliminate the problem, merely remove the ill-conceived attempt to
interoperate with String from the equals method. Once you do this, you
can refactor the method into a single return statement:
Click here to view code image
@Override public boolean equals(Object o) {
61
return o instanceof CaseInsensitiveString &&
((CaseInsensitiveString)
o).s.equalsIgnoreCase(s);
}
Transitivity—The third requirement of the equals contract says that if
one object is equal to a second and the second object is equal to a third, then
the first object must be equal to the third. Again, it’s not hard to imagine
violating this requirement unintentionally. Consider the case of a subclass that
adds a new value component to its superclass. In other words, the subclass
adds a piece of information that affects equals comparisons. Let’s start with
a simple immutable two-dimensional integer point class:
Click here to view code image
public class Point {
private final int x;
private final int y;
public Point(int x, int y) {
this.x = x;
this.y = y;
}
@Override public boolean equals(Object o) {
if (!(o instanceof Point))
return false;
Point p = (Point)o;
return p.x == x && p.y == y;
}
... // Remainder omitted
}
Suppose you want to extend this class, adding the notion of color to a point:
Click here to view code image
public class ColorPoint extends Point {
private final Color color;
public ColorPoint(int x, int y, Color color) {
super(x, y);
this.color = color;
}
... // Remainder omitted
}
62
How should the equals method look? If you leave it out entirely, the
implementation is inherited from Point and color information is ignored in
equals comparisons. While this does not violate the equals contract, it is
clearly unacceptable. Suppose you write an equals method that returns
true only if its argument is another color point with the same position and
color:
Click here to view code image
// Broken - violates symmetry!
@Override public boolean equals(Object o) {
if (!(o instanceof ColorPoint))
return false;
return super.equals(o) && ((ColorPoint) o).color ==
color;
}
The problem with this method is that you might get different results when
comparing a point to a color point and vice versa. The former comparison
ignores color, while the latter comparison always returns false because the
type of the argument is incorrect. To make this concrete, let’s create one point
and one color point:
Click here to view code image
Point p = new Point(1, 2);
ColorPoint cp = new ColorPoint(1, 2, Color.RED);
Then p.equals(cp) returns true, while cp.equals(p) returns
false. You might try to fix the problem by having ColorPoint.equals
ignore color when doing “mixed comparisons”:
Click here to view code image
// Broken - violates transitivity!
@Override public boolean equals(Object o) {
if (!(o instanceof Point))
return false;
// If o is a normal Point, do a color-blind
comparison
if (!(o instanceof ColorPoint))
return o.equals(this);
// o is a ColorPoint; do a full comparison
return super.equals(o) && ((ColorPoint) o).color ==
color;
}
63
This approach does provide symmetry, but at the expense of transitivity:
Click here to view code image
ColorPoint p1 = new ColorPoint(1, 2, Color.RED);
Point p2 = new Point(1, 2);
ColorPoint p3 = new ColorPoint(1, 2, Color.BLUE);
Now p1.equals(p2) and p2.equals(p3) return true, while
p1.equals(p3) returns false, a clear violation of transitivity. The first
two comparisons are “color-blind,” while the third takes color into account.
Also, this approach can cause infinite recursion: Suppose there are two
subclasses of Point, say ColorPoint and SmellPoint, each with this
sort of equals method. Then a call to
myColorPoint.equals(mySmellPoint) will throw a
StackOverflowError.
So what’s the solution? It turns out that this is a fundamental problem of
equivalence relations in object-oriented languages. There is no way to
extend an instantiable class and add a value component while preserving
the equals contract, unless you’re willing to forgo the benefits of objectoriented abstraction.
You may hear it said that you can extend an instantiable class and add a
value component while preserving the equals contract by using a
getClass test in place of the instanceof test in the equals method:
Click here to view code image
// Broken - violates Liskov substitution principle (page
43)
@Override public boolean equals(Object o) {
if (o == null || o.getClass() != getClass())
return false;
Point p = (Point) o;
return p.x == x && p.y == y;
}
This has the effect of equating objects only if they have the same
implementation class. This may not seem so bad, but the consequences are
unacceptable: An instance of a subclass of Point is still a Point, and it still
needs to function as one, but it fails to do so if you take this approach! Let’s
suppose we want to write a method to tell whether a point is on the unit circle.
Here is one way we could do it:
Click here to view code image
// Initialize unitCircle to contain all Points on the
unit circle
64
private static final Set<Point> unitCircle = Set.of(
new Point( 1, 0), new Point( 0, 1),
new Point(-1, 0), new Point( 0, -1));
public static boolean onUnitCircle(Point p) {
return unitCircle.contains(p);
}
While this may not be the fastest way to implement the functionality, it works
fine. Suppose you extend Point in some trivial way that doesn’t add a value
component, say, by having its constructor keep track of how many instances
have been created:
Click here to view code image
public class CounterPoint extends Point {
private static final AtomicInteger counter =
new AtomicInteger();
public CounterPoint(int x, int y) {
super(x, y);
counter.incrementAndGet();
}
public static int numberCreated() { return
counter.get(); }
}
The Liskov substitution principle says that any important property of a type
should also hold for all its subtypes so that any method written for the type
should work equally well on its subtypes [Liskov87]. This is the formal
statement of our earlier claim that a subclass of Point (such as
CounterPoint) is still a Point and must act as one. But suppose we pass
a CounterPoint to the onUnitCircle method. If the Point class uses
a getClass-based equals method, the onUnitCircle method will
return false regardless of the CounterPoint instance’s x and y
coordinates. This is so because most collections, including the HashSet
used by the onUnitCircle method, use the equals method to test for
containment, and no CounterPoint instance is equal to any Point. If,
however, you use a proper instanceof-based equals method on Point,
the same onUnitCircle method works fine when presented with a
CounterPoint instance.
While there is no satisfactory way to extend an instantiable class and add a
value component, there is a fine workaround: Follow the advice of Item 18,
“Favor composition over inheritance.” Instead of having ColorPoint
extend Point, give ColorPoint a private Point field and a public view
65
method (Item 6) that returns the point at the same position as this color point:
Click here to view code image
// Adds a value component without violating the equals
contract
public class ColorPoint {
private final Point point;
private final Color color;
public ColorPoint(int x, int y, Color color) {
point = new Point(x, y);
this.color = Objects.requireNonNull(color);
}
/**
* Returns the point-view of this color point.
*/
public Point asPoint() {
return point;
}
@Override public boolean equals(Object o) {
if (!(o instanceof ColorPoint))
return false;
ColorPoint cp = (ColorPoint) o;
return cp.point.equals(point) &&
cp.color.equals(color);
}
... // Remainder omitted
}
There are some classes in the Java platform libraries that do extend an
instantiable class and add a value component. For example,
java.sql.Timestamp extends java.util.Date and adds a
nanoseconds field. The equals implementation for Timestamp does
violate symmetry and can cause erratic behavior if Timestamp and Date
objects are used in the same collection or are otherwise intermixed. The
Timestamp class has a disclaimer cautioning programmers against mixing
dates and timestamps. While you won’t get into trouble as long as you keep
them separate, there’s nothing to prevent you from mixing them, and the
resulting errors can be hard to debug. This behavior of the Timestamp class
was a mistake and should not be emulated.
Note that you can add a value component to a subclass of an abstract class
without violating the equals contract. This is important for the sort of class
hierarchies that you get by following the advice in Item 23, “Prefer class
66
hierarchies to tagged classes.” For example, you could have an abstract class
Shape with no value components, a subclass Circle that adds a radius
field, and a subclass Rectangle that adds length and width fields.
Problems of the sort shown earlier won’t occur so long as it is impossible to
create a superclass instance directly.
Consistency—The fourth requirement of the equals contract says that if
two objects are equal, they must remain equal for all time unless one (or both)
of them is modified. In other words, mutable objects can be equal to different
objects at different times while immutable objects can’t. When you write a
class, think hard about whether it should be immutable (Item 17). If you
conclude that it should, make sure that your equals method enforces the
restriction that equal objects remain equal and unequal objects remain
unequal for all time.
Whether or not a class is immutable, do not write an equals method
that depends on unreliable resources. It’s extremely difficult to satisfy the
consistency requirement if you violate this prohibition. For example,
java.net.URL’s equals method relies on comparison of the IP
addresses of the hosts associated with the URLs. Translating a host name to
an IP address can require network access, and it isn’t guaranteed to yield the
same results over time. This can cause the URL equals method to violate
the equals contract and has caused problems in practice. The behavior of
URL’s equals method was a big mistake and should not be emulated.
Unfortunately, it cannot be changed due to compatibility requirements. To
avoid this sort of problem, equals methods should perform only
deterministic computations on memory-resident objects.
Non-nullity—The final requirement lacks an official name, so I have taken
the liberty of calling it “non-nullity.” It says that all objects must be unequal
to null. While it is hard to imagine accidentally returning true in response
to the invocation o.equals(null), it isn’t hard to imagine accidentally
throwing a NullPointerException. The general contract prohibits this.
Many classes have equals methods that guard against it with an explicit test
for null:
Click here to view code image
@Override public boolean equals(Object o) {
if (o == null)
return false;
...
}
This test is unnecessary. To test its argument for equality, the equals
method must first cast its argument to an appropriate type so its accessors can
67
be invoked or its fields accessed. Before doing the cast, the method must use
the instanceof operator to check that its argument is of the correct type:
Click here to view code image
@Override public boolean equals(Object o) {
if (!(o instanceof MyType))
return false;
MyType mt = (MyType) o;
...
}
If this type check were missing and the equals method were passed an
argument of the wrong type, the equals method would throw a
ClassCastException, which violates the equals contract. But the
instanceof operator is specified to return false if its first operand is
null, regardless of what type appears in the second operand [JLS, 15.20.2].
Therefore, the type check will return false if null is passed in, so you
don’t need an explicit null check.
Putting it all together, here’s a recipe for a high-quality equals method:
1. Use the == operator to check if the argument is a reference to this
object. If so, return true. This is just a performance optimization but
one that is worth doing if the comparison is potentially expensive.
2. Use the instanceof operator to check if the argument has the
correct type. If not, return false. Typically, the correct type is the class
in which the method occurs. Occasionally, it is some interface
implemented by this class. Use an interface if the class implements an
interface that refines the equals contract to permit comparisons across
classes that implement the interface. Collection interfaces such as Set,
List, Map, and Map.Entry have this property.
3. Cast the argument to the correct type. Because this cast was preceded
by an instanceof test, it is guaranteed to succeed.
4. For each “significant” field in the class, check if that field of the
argument matches the corresponding field of this object. If all these
tests succeed, return true; otherwise, return false. If the type in Step
2 is an interface, you must access the argument’s fields via interface
methods; if the type is a class, you may be able to access the fields
directly, depending on their accessibility.
For primitive fields whose type is not float or double, use the ==
operator for comparisons; for object reference fields, call the equals
68
method recursively; for float fields, use the static
Float.compare(float, float) method; and for double fields,
use Double.compare(double, double). The special treatment of
float and double fields is made necessary by the existence of
Float.NaN, -0.0f and the analogous double values; see JLS 15.21.1
or the documentation of Float.equals for details. While you could
compare float and double fields with the static methods
Float.equals and Double.equals, this would entail autoboxing on
every comparison, which would have poor performance. For array fields,
apply these guidelines to each element. If every element in an array field is
significant, use one of the Arrays.equals methods.
Some object reference fields may legitimately contain null. To avoid the
possibility of a NullPointerException, check such fields for
equality using the static method Objects.equals(Object,
Object).
For some classes, such as CaseInsensitiveString above, field
comparisons are more complex than simple equality tests. If this is the
case, you may want to store a canonical form of the field so the equals
method can do a cheap exact comparison on canonical forms rather than a
more costly nonstandard comparison. This technique is most appropriate
for immutable classes (Item 17); if the object can change, you must keep
the canonical form up to date.
The performance of the equals method may be affected by the order in
which fields are compared. For best performance, you should first compare
fields that are more likely to differ, less expensive to compare, or, ideally,
both. You must not compare fields that are not part of an object’s logical
state, such as lock fields used to synchronize operations. You need not
compare derived fields, which can be calculated from “significant fields,”
but doing so may improve the performance of the equals method. If a
derived field amounts to a summary description of the entire object,
comparing this field will save you the expense of comparing the actual data
if the comparison fails. For example, suppose you have a Polygon class,
and you cache the area. If two polygons have unequal areas, you needn’t
bother comparing their edges and vertices.
When you are finished writing your equals method, ask yourself
three questions: Is it symmetric? Is it transitive? Is it consistent? And
don’t just ask yourself; write unit tests to check, unless you used AutoValue
(page 49) to generate your equals method, in which case you can safely
omit the tests. If the properties fail to hold, figure out why, and modify the
69
equals method accordingly. Of course your equals method must also
satisfy the other two properties (reflexivity and non-nullity), but these two
usually take care of themselves.
An equals method constructed according to the previous recipe is shown
in this simplistic PhoneNumber class:
Click here to view code image
// Class with a typical equals method
public final class PhoneNumber {
private final short areaCode, prefix, lineNum;
public PhoneNumber(int areaCode, int prefix, int
lineNum) {
this.areaCode = rangeCheck(areaCode, 999, "area
code");
this.prefix = rangeCheck(prefix, 999,
"prefix");
this.lineNum = rangeCheck(lineNum, 9999, "line
num");
}
private static short rangeCheck(int val, int max,
String arg) {
if (val < 0 || val > max)
throw new IllegalArgumentException(arg + ": "
+ val);
return (short) val;
}
@Override public boolean equals(Object o) {
if (o == this)
return true;
if (!(o instanceof PhoneNumber))
return false;
PhoneNumber pn = (PhoneNumber)o;
return pn.lineNum == lineNum && pn.prefix ==
prefix
&& pn.areaCode == areaCode;
}
... // Remainder omitted
}
Here are a few final caveats:
• Always override hashCode when you override equals (Item 11).
• Don’t try to be too clever. If you simply test fields for equality, it’s not
70
hard to adhere to the equals contract. If you are overly aggressive in
searching for equivalence, it’s easy to get into trouble. It is generally a bad
idea to take any form of aliasing into account. For example, the File class
shouldn’t attempt to equate symbolic links referring to the same file.
Thankfully, it doesn’t.
• Don’t substitute another type for Object in the equals declaration.
It is not uncommon for a programmer to write an equals method that
looks like this and then spend hours puzzling over why it doesn’t work
properly:
Click here to view code image
// Broken - parameter type must be Object!
public boolean equals(MyClass o) {
...
}
The problem is that this method does not override Object.equals,
whose argument is of type Object, but overloads it instead (Item 52).
It is unacceptable to provide such a “strongly typed” equals method
even in addition to the normal one, because it can cause Override
annotations in subclasses to generate false positives and provide a false
sense of security.
Consistent use of the Override annotation, as illustrated throughout
this item, will prevent you from making this mistake (Item 40). This
equals method won’t compile, and the error message will tell you
exactly what is wrong:
Click here to view code image
// Still broken, but won’t compile
@Override public boolean equals(MyClass o) {
...
}
Writing and testing equals (and hashCode) methods is tedious, and the
resulting code is mundane. An excellent alternative to writing and testing
these methods manually is to use Google’s open source AutoValue
framework, which automatically generates these methods for you, triggered
by a single annotation on the class . In most cases, the methods generated by
AutoValue are essentially identical to those you’d write yourself.
IDEs, too, have facilities to generate equals and hashCode methods,
but the resulting source code is more verbose and less readable than code that
uses AutoValue, does not track changes in the class automatically, and
therefore requires testing. That said, having IDEs generate equals (and
71
hashCode) methods is generally preferable to implementing them manually
because IDEs do not make careless mistakes, and humans do.
In summary, don’t override the equals method unless you have to: in
many cases, the implementation inherited from Object does exactly what
you want. If you do override equals, make sure to compare all of the class’s
significant fields and to compare them in a manner that preserves all five
provisions of the equals contract.
Item 11: Always override hashCode when you override
equals
You must override hashCode in every class that overrides equals. If
you fail to do so, your class will violate the general contract for hashCode,
which will prevent it from functioning properly in collections such as
HashMap and HashSet. Here is the contract, adapted from the Object
specification :
• When the hashCode method is invoked on an object repeatedly during
an execution of an application, it must consistently return the same
value, provided no information used in equals comparisons is
modified. This value need not remain consistent from one execution of
an application to another.
• If two objects are equal according to the equals(Object) method,
then calling hashCode on the two objects must produce the same
integer result.
• If two objects are unequal according to the equals(Object) method,
it is not required that calling hashCode on each of the objects must
produce distinct results. However, the programmer should be aware that
producing distinct results for unequal objects may improve the
performance of hash tables.
The key provision that is violated when you fail to override hashCode
is the second one: equal objects must have equal hash codes. Two distinct
instances may be logically equal according to a class’s equals method, but
to Object’s hashCode method, they’re just two objects with nothing much
in common. Therefore, Object’s hashCode method returns two seemingly
random numbers instead of two equal numbers as required by the contract.
For example, suppose you attempt to use instances of the PhoneNumber
class from Item 10 as keys in a HashMap:
Click here to view code image
72
Map<PhoneNumber, String> m = new HashMap<>();
m.put(new PhoneNumber(707, 867, 5309), "Jenny");
At this point, you might expect m.get(new PhoneNumber(707, 867,
5309)) to return "Jenny", but instead, it returns null. Notice that two
PhoneNumber instances are involved: one is used for insertion into the
HashMap, and a second, equal instance is used for (attempted) retrieval. The
PhoneNumber class’s failure to override hashCode causes the two equal
instances to have unequal hash codes, in violation of the hashCode contract.
Therefore, the get method is likely to look for the phone number in a
different hash bucket from the one in which it was stored by the put method.
Even if the two instances happen to hash to the same bucket, the get method
will almost certainly return null, because HashMap has an optimization
that caches the hash code associated with each entry and doesn’t bother
checking for object equality if the hash codes don’t match.
Fixing this problem is as simple as writing a proper hashCode method for
PhoneNumber. So what should a hashCode method look like? It’s trivial
to write a bad one. This one, for example, is always legal but should never be
used:
Click here to view code image
// The worst possible legal hashCode implementation -
never use!
@Override public int hashCode() { return 42; }
It’s legal because it ensures that equal objects have the same hash code. It’s
atrocious because it ensures that every object has the same hash code.
Therefore, every object hashes to the same bucket, and hash tables degenerate
to linked lists. Programs that should run in linear time instead run in quadratic
time. For large hash tables, this is the difference between working and not
working.
A good hash function tends to produce unequal hash codes for unequal
instances. This is exactly what is meant by the third part of the hashCode
contract. Ideally, a hash function should distribute any reasonable collection
of unequal instances uniformly across all int values. Achieving this ideal
can be difficult. Luckily it’s not too hard to achieve a fair approximation.
Here is a simple recipe:
1. Declare an int variable named result, and initialize it to the hash code
c for the first significant field in your object, as computed in step 2.a.
(Recall from Item 10 that a significant field is a field that affects equals
comparisons.)
2. For every remaining significant field f in your object, do the following:
73
a. Compute an int hash code c for the field:
i. If the field is of a primitive type, compute Type.hashCode(f),
where Type is the boxed primitive class corresponding to f’s type.
ii. If the field is an object reference and this class’s equals method
compares the field by recursively invoking equals, recursively
invoke hashCode on the field. If a more complex comparison is
required, compute a “canonical representation” for this field and
invoke hashCode on the canonical representation. If the value of the
field is null, use 0 (or some other constant, but 0 is traditional).
iii. If the field is an array, treat it as if each significant element were a
separate field. That is, compute a hash code for each significant
element by applying these rules recursively, and combine the values
per step 2.b. If the array has no significant elements, use a constant,
preferably not 0. If all elements are significant, use
Arrays.hashCode.
b. Combine the hash code c computed in step 2.a into result as follows:
result = 31 * result + c;
3. Return result.
When you are finished writing the hashCode method, ask yourself
whether equal instances have equal hash codes. Write unit tests to verify your
intuition (unless you used AutoValue to generate your equals and
hashCode methods, in which case you can safely omit these tests). If equal
instances have unequal hash codes, figure out why and fix the problem.
You may exclude derived fields from the hash code computation. In other
words, you may ignore any field whose value can be computed from fields
included in the computation. You must exclude any fields that are not used in
equals comparisons, or you risk violating the second provision of the
hashCode contract.
The multiplication in step 2.b makes the result depend on the order of the
fields, yielding a much better hash function if the class has multiple similar
fields. For example, if the multiplication were omitted from a String hash
function, all anagrams would have identical hash codes. The value 31 was
chosen because it is an odd prime. If it were even and the multiplication
overflowed, information would be lost, because multiplication by 2 is
equivalent to shifting. The advantage of using a prime is less clear, but it is
traditional. A nice property of 31 is that the multiplication can be replaced by
a shift and a subtraction for better performance on some architectures: 31 *
i == (i << 5) - i. Modern VMs do this sort of optimization
automatically.
74
Let’s apply the previous recipe to the PhoneNumber class:
Click here to view code image
// Typical hashCode method
@Override public int hashCode() {
int result = Short.hashCode(areaCode);
result = 31 * result + Short.hashCode(prefix);
result = 31 * result + Short.hashCode(lineNum);
return result;
}
Because this method returns the result of a simple deterministic
computation whose only inputs are the three significant fields in a
PhoneNumber instance, it is clear that equal PhoneNumber instances
have equal hash codes. This method is, in fact, a perfectly good hashCode
implementation for PhoneNumber, on par with those in the Java platform
libraries. It is simple, is reasonably fast, and does a reasonable job of
dispersing unequal phone numbers into different hash buckets.
While the recipe in this item yields reasonably good hash functions, they
are not state-of-the-art. They are comparable in quality to the hash functions
found in the Java platform libraries’ value types and are adequate for most
uses. If you have a bona fide need for hash functions less likely to produce
collisions, see Guava’s com.google.common.hash.Hashing [Guava].
The Objects class has a static method that takes an arbitrary number of
objects and returns a hash code for them. This method, named hash, lets you
write one-line hashCode methods whose quality is comparable to those
written according to the recipe in this item. Unfortunately, they run more
slowly because they entail array creation to pass a variable number of
arguments, as well as boxing and unboxing if any of the arguments are of
primitive type. This style of hash function is recommended for use only in
situations where performance is not critical. Here is a hash function for
PhoneNumber written using this technique:
Click here to view code image
// One-line hashCode method - mediocre performance
@Override public int hashCode() {
return Objects.hash(lineNum, prefix, areaCode);
}
If a class is immutable and the cost of computing the hash code is
significant, you might consider caching the hash code in the object rather than
recalculating it each time it is requested. If you believe that most objects of
this type will be used as hash keys, then you should calculate the hash code
when the instance is created. Otherwise, you might choose to lazily initialize
75
the hash code the first time hash-Code is invoked. Some care is required to
ensure that the class remains thread-safe in the presence of a lazily initialized
field (Item 83). Our PhoneNumber class does not merit this treatment, but
just to show you how it’s done, here it is. Note that the initial value for the
hashCode field (in this case, 0) should not be the hash code of a commonly
created instance:
Click here to view code image
// hashCode method with lazily initialized cached hash
code
private int hashCode; // Automatically initialized to 0
@Override public int hashCode() {
int result = hashCode;
if (result == 0) {
result = Short.hashCode(areaCode);
result = 31 * result + Short.hashCode(prefix);
result = 31 * result + Short.hashCode(lineNum);
hashCode = result;
}
return result;
}
Do not be tempted to exclude significant fields from the hash code
computation to improve performance. While the resulting hash function
may run faster, its poor quality may degrade hash tables’ performance to the
point where they become unusable. In particular, the hash function may be
confronted with a large collection of instances that differ mainly in regions
you’ve chosen to ignore. If this happens, the hash function will map all these
instances to a few hash codes, and programs that should run in linear time will
instead run in quadratic time.
This is not just a theoretical problem. Prior to Java 2, the String hash
function used at most sixteen characters evenly spaced throughout the string,
starting with the first character. For large collections of hierarchical names,
such as URLs, this function displayed exactly the pathological behavior
described earlier.
Don’t provide a detailed specification for the value returned by
hashCode, so clients can’t reasonably depend on it; this gives you the
flexibility to change it. Many classes in the Java libraries, such as String
and Integer, specify the exact value returned by their hashCode method
as a function of the instance value. This is not a good idea but a mistake that
we’re forced to live with: It impedes the ability to improve the hash function
in future releases. If you leave the details unspecified and a flaw is found in
the hash function or a better hash function is discovered, you can change it in
76
a subsequent release.
In summary, you must override hashCode every time you override
equals, or your program will not run correctly. Your hashCode method
must obey the general contract specified in Object and must do a
reasonable job assigning unequal hash codes to unequal instances. This is
easy to achieve, if slightly tedious, using the recipe on page 51. As mentioned
in Item 10, the AutoValue framework provides a fine alternative to writing
equals and hashCode methods manually, and IDEs also provide some of
this functionality.
Item 12: Always override toString
While Object provides an implementation of the toString method, the
string that it returns is generally not what the user of your class wants to see.
It consists of the class name followed by an “at” sign (@) and the unsigned
hexadecimal representation of the hash code, for example,
PhoneNumber@163b91. The general contract for toString says that the
returned string should be “a concise but informative representation that is
easy for a person to read.” While it could be argued that
PhoneNumber@163b91 is concise and easy to read, it isn’t very
informative when compared to 707-867-5309. The toString contract
goes on to say, “It is recommended that all subclasses override this method.”
Good advice, indeed!
While it isn’t as critical as obeying the equals and hashCode contracts
(Items 10 and 11), providing a good toString implementation makes
your class much more pleasant to use and makes systems using the class
easier to debug. The toString method is automatically invoked when an
object is passed to println, printf, the string concatenation operator, or
assert, or is printed by a debugger. Even if you never call toString on
an object, others may. For example, a component that has a reference to your
object may include the string representation of the object in a logged error
message. If you fail to override toString, the message may be all but
useless.
If you’ve provided a good toString method for PhoneNumber,
generating a useful diagnostic message is as easy as this:
Click here to view code image
System.out.println("Failed to connect to " +
phoneNumber);
Programmers will generate diagnostic messages in this fashion whether or
not you override toString, but the messages won’t be useful unless you
77
do. The benefits of providing a good toString method extend beyond
instances of the class to objects containing references to these instances,
especially collections. Which would you rather see when printing a map,
{Jenny=PhoneNumber@163b91} or {Jenny=707-867-5309}?
When practical, the toString method should return all of the
interesting information contained in the object, as shown in the phone
number example. It is impractical if the object is large or if it contains state
that is not conducive to string representation. Under these circumstances,
toString should return a summary such as Manhattan residential
phone directory (1487536 listings) or
Thread[main,5,main]. Ideally, the string should be self-explanatory.
(The Thread example flunks this test.) A particularly annoying penalty for
failing to include all of an object’s interesting information in its string
representation is test failure reports that look like this:
Click here to view code image
Assertion failure: expected {abc, 123}, but was {abc,
123}.
One important decision you’ll have to make when implementing a
toString method is whether to specify the format of the return value in the
documentation. It is recommended that you do this for value classes, such as
phone number or matrix. The advantage of specifying the format is that it
serves as a standard, unambiguous, human-readable representation of the
object. This representation can be used for input and output and in persistent
human-readable data objects, such as CSV files. If you specify the format, it’s
usually a good idea to provide a matching static factory or constructor so
programmers can easily translate back and forth between the object and its
string representation. This approach is taken by many value classes in the
Java platform libraries, including BigInteger, BigDecimal, and most of
the boxed primitive classes.
The disadvantage of specifying the format of the toString return value
is that once you’ve specified it, you’re stuck with it for life, assuming your
class is widely used. Programmers will write code to parse the representation,
to generate it, and to embed it into persistent data. If you change the
representation in a future release, you’ll break their code and data, and they
will yowl. By choosing not to specify a format, you preserve the flexibility to
add information or improve the format in a subsequent release.
Whether or not you decide to specify the format, you should clearly
document your intentions. If you specify the format, you should do so
precisely. For example, here’s a toString method to go with the
PhoneNumber class in Item 11:
78
Click here to view code image
/**
* Returns the string representation of this phone
number.
* The string consists of twelve characters whose format
is
* "XXX-YYY-ZZZZ", where XXX is the area code, YYY is
the
* prefix, and ZZZZ is the line number. Each of the
capital
* letters represents a single decimal digit.
*
* If any of the three parts of this phone number is too
small
* to fill up its field, the field is padded with
leading zeros.
* For example, if the value of the line number is 123,
the last
* four characters of the string representation will be
"0123".
*/
@Override public String toString() {
return String.format("%03d-%03d-%04d",
areaCode, prefix, lineNum);
}
If you decide not to specify a format, the documentation comment should
read something like this:
Click here to view code image
/**
* Returns a brief description of this potion. The exact
details
* of the representation are unspecified and subject to
change,
* but the following may be regarded as typical:
*
* "[Potion #9: type=love, smell=turpentine, look=india
ink]"
*/
@Override public String toString() { ... }
After reading this comment, programmers who produce code or persistent
data that depends on the details of the format will have no one but themselves
to blame when the format is changed.
Whether or not you specify the format, provide programmatic access to
the information contained in the value returned by toString. For
79
example, the PhoneNumber class should contain accessors for the area
code, prefix, and line number. If you fail to do this, you force programmers
who need this information to parse the string. Besides reducing performance
and making unnecessary work for programmers, this process is error-prone
and results in fragile systems that break if you change the format. By failing
to provide accessors, you turn the string format into a de facto API, even if
you’ve specified that it’s subject to change.
It makes no sense to write a toString method in a static utility class
(Item 4). Nor should you write a toString method in most enum types
(Item 34) because Java provides a perfectly good one for you. You should,
however, write a toString method in any abstract class whose subclasses
share a common string representation. For example, the toString methods
on most collection implementations are inherited from the abstract collection
classes.
Google’s open source AutoValue facility, discussed in Item 10, will
generate a toString method for you, as will most IDEs. These methods are
great for telling you the contents of each field but aren’t specialized to the
meaning of the class. So, for example, it would be inappropriate to use an
automatically generated toString method for our PhoneNumber class (as
phone numbers have a standard string representation), but it would be
perfectly acceptable for our Potion class. That said, an automatically
generated toString method is far preferable to the one inherited from
Object, which tells you nothing about an object’s value.
To recap, override Object’s toString implementation in every
instantiable class you write, unless a superclass has already done so. It makes
classes much more pleasant to use and aids in debugging. The toString
method should return a concise, useful description of the object, in an
aesthetically pleasing format.
Item 13: Override clone judiciously
The Cloneable interface was intended as a mixin interface (Item 20) for
classes to advertise that they permit cloning. Unfortunately, it fails to serve
this purpose. Its primary flaw is that it lacks a clone method, and Object’s
clone method is protected. You cannot, without resorting to reflection (Item
65), invoke clone on an object merely because it implements Cloneable.
Even a reflective invocation may fail, because there is no guarantee that the
object has an accessible clone method. Despite this flaw and many others,
the facility is in reasonably wide use, so it pays to understand it. This item
tells you how to implement a well-behaved clone method, discusses when it
is appropriate to do so, and presents alternatives.
80
So what does Cloneable do, given that it contains no methods? It
determines the behavior of Object’s protected clone implementation: if a
class implements Cloneable, Object’s clone method returns a field-byfield copy of the object; otherwise it throws
CloneNotSupportedException. This is a highly atypical use of
interfaces and not one to be emulated. Normally, implementing an interface
says something about what a class can do for its clients. In this case, it
modifies the behavior of a protected method on a superclass.
Though the specification doesn’t say it, in practice, a class implementing
Cloneable is expected to provide a properly functioning public clone
method. In order to achieve this, the class and all of its superclasses must
obey a complex, unenforceable, thinly documented protocol. The resulting
mechanism is fragile, dangerous, and extralinguistic: it creates objects
without calling a constructor.
The general contract for the clone method is weak. Here it is, copied
from the Object specification :
Creates and returns a copy of this object. The precise meaning of “copy”
may depend on the class of the object. The general intent is that, for any
object x, the expression
x.clone() != x
will be true, and the expression
Click here to view code image
x.clone().getClass() == x.getClass()
will be true, but these are not absolute requirements. While it is typically
the case that
x.clone().equals(x)
will be true, this is not an absolute requirement.
By convention, the object returned by this method should be obtained by
calling super.clone. If a class and all of its superclasses (except
Object) obey this convention, it will be the case that
Click here to view code image
x.clone().getClass() == x.getClass().
By convention, the returned object should be independent of the object
being cloned. To achieve this independence, it may be necessary to modify
one or more fields of the object returned by super.clone before
returning it.
81
This mechanism is vaguely similar to constructor chaining, except that it
isn’t enforced: if a class’s clone method returns an instance that is not
obtained by calling super.clone but by calling a constructor, the compiler
won’t complain, but if a subclass of that class calls super.clone, the
resulting object will have the wrong class, preventing the subclass from
clone method from working properly. If a class that overrides clone is
final, this convention may be safely ignored, as there are no subclasses to
worry about. But if a final class has a clone method that does not invoke
super.clone, there is no reason for the class to implement Cloneable,
as it doesn’t rely on the behavior of Object’s clone implementation.
Suppose you want to implement Cloneable in a class whose superclass
provides a well-behaved clone method. First call super.clone. The
object you get back will be a fully functional replica of the original. Any
fields declared in your class will have values identical to those of the original.
If every field contains a primitive value or a reference to an immutable object,
the returned object may be exactly what you need, in which case no further
processing is necessary. This is the case, for example, for the PhoneNumber
class in Item 11, but note that immutable classes should never provide a
clone method because it would merely encourage wasteful copying. With
that caveat, here’s how a clone method for PhoneNumber would look:
Click here to view code image
// Clone method for class with no references to mutable
state
@Override public PhoneNumber clone() {
try {
return (PhoneNumber) super.clone();
} catch (CloneNotSupportedException e) {
throw new AssertionError(); // Can't happen
}
}
In order for this method to work, the class declaration for PhoneNumber
would have to be modified to indicate that it implements Cloneable.
Though Object’s clone method returns Object, this clone method
returns PhoneNumber. It is legal and desirable to do this because Java
supports covariant return types. In other words, an overriding method’s return
type can be a subclass of the overridden method’s return type. This eliminates
the need for casting in the client. We must cast the result of super.clone
from Object to PhoneNumber before returning it, but the cast is
guaranteed to succeed.
The call to super.clone is contained in a try-catch block. This is
because Object declares its clone method to throw
82
CloneNotSupportedException, which is a checked exception.
Because PhoneNumber implements Cloneable, we know the call to
super.clone will succeed. The need for this boilerplate indicates that
CloneNotSupportedException should have been unchecked (Item
71).
If an object contains fields that refer to mutable objects, the simple clone
implementation shown earlier can be disastrous. For example, consider the
Stack class in Item 7:
Click here to view code image
public class Stack {
private Object[] elements;
private int size = 0;
private static final int DEFAULT_INITIAL_CAPACITY =
16;
public Stack() {
this.elements = new
Object[DEFAULT_INITIAL_CAPACITY];
}
public void push(Object e) {
ensureCapacity();
elements[size++] = e;
}
public Object pop() {
if (size == 0)
throw new EmptyStackException();
Object result = elements[--size];
elements[size] = null; // Eliminate obsolete
reference
return result;
}
// Ensure space for at least one more element.
private void ensureCapacity() {
if (elements.length == size)
elements = Arrays.copyOf(elements, 2 * size
+ 1);
}
}
Suppose you want to make this class cloneable. If the clone method
merely returns super.clone(), the resulting Stack instance will have
the correct value in its size field, but its elements field will refer to the
83
same array as the original Stack instance. Modifying the original will
destroy the invariants in the clone and vice versa. You will quickly find that
your program produces nonsensical results or throws a
NullPointerException.
This situation could never occur as a result of calling the sole constructor in
the Stack class. In effect, the clone method functions as a constructor;
you must ensure that it does no harm to the original object and that it
properly establishes invariants on the clone. In order for the clone
method on Stack to work properly, it must copy the internals of the stack.
The easiest way to do this is to call clone recursively on the elements
array:
Click here to view code image
// Clone method for class with references to mutable
state
@Override public Stack clone() {
try {
Stack result = (Stack) super.clone();
result.elements = elements.clone();
return result;
} catch (CloneNotSupportedException e) {
throw new AssertionError();
}
}
Note that we do not have to cast the result of elements.clone to
Object[]. Calling clone on an array returns an array whose runtime and
compile-time types are identical to those of the array being cloned. This is the
preferred idiom to duplicate an array. In fact, arrays are the sole compelling
use of the clone facility.
Note also that the earlier solution would not work if the elements field
were final because clone would be prohibited from assigning a new value to
the field. This is a fundamental problem: like serialization, the Cloneable
architecture is incompatible with normal use of final fields referring to
mutable objects, except in cases where the mutable objects may be safely
shared between an object and its clone. In order to make a class cloneable, it
may be necessary to remove final modifiers from some fields.
It is not always sufficient merely to call clone recursively. For example,
suppose you are writing a clone method for a hash table whose internals
consist of an array of buckets, each of which references the first entry in a
linked list of key-value pairs. For performance, the class implements its own
lightweight singly linked list instead of using java.util.LinkedList
internally:
84
Click here to view code image
public class HashTable implements Cloneable {
private Entry[] buckets = ...;
private static class Entry {
final Object key;
Object value;
Entry next;
Entry(Object key, Object value, Entry next) {
this.key = key;
this.value = value;
this.next = next;
}
}
... // Remainder omitted
}
Suppose you merely clone the bucket array recursively, as we did for
Stack:
Click here to view code image
// Broken clone method - results in shared mutable
state!
@Override public HashTable clone() {
try {
HashTable result = (HashTable) super.clone();
result.buckets = buckets.clone();
return result;
} catch (CloneNotSupportedException e) {
throw new AssertionError();
}
}
Though the clone has its own bucket array, this array references the same
linked lists as the original, which can easily cause nondeterministic behavior
in both the clone and the original. To fix this problem, you’ll have to copy the
linked list that comprises each bucket. Here is one common approach:
Click here to view code image
// Recursive clone method for class with complex mutable
state
public class HashTable implements Cloneable {
private Entry[] buckets = ...;
private static class Entry {
final Object key;
Object value;
85
Entry next;
Entry(Object key, Object value, Entry next) {
this.key = key;
this.value = value;
this.next = next;
}
// Recursively copy the linked list headed by
this Entry
Entry deepCopy() {
return new Entry(key, value,
next == null ? null : next.deepCopy());
}
}
@Override public HashTable clone() {
try {
HashTable result = (HashTable)
super.clone();
result.buckets = new Entry[buckets.length];
for (int i = 0; i < buckets.length; i++)
if (buckets[i] != null)
result.buckets[i] =
buckets[i].deepCopy();
return result;
} catch (CloneNotSupportedException e) {
throw new AssertionError();
}
}
... // Remainder omitted
}
The private class HashTable.Entry has been augmented to support a
“deep copy” method. The clone method on HashTable allocates a new
buckets array of the proper size and iterates over the original buckets
array, deep-copying each nonempty bucket. The deepCopy method on
Entry invokes itself recursively to copy the entire linked list headed by the
entry. While this technique is cute and works fine if the buckets aren’t too
long, it is not a good way to clone a linked list because it consumes one stack
frame for each element in the list. If the list is long, this could easily cause a
stack overflow. To prevent this from happening, you can replace the recursion
in deepCopy with iteration:
Click here to view code image
86
// Iteratively copy the linked list headed by this Entry
Entry deepCopy() {
Entry result = new Entry(key, value, next);
for (Entry p = result; p.next != null; p = p.next)
p.next = new Entry(p.next.key, p.next.value,
p.next.next);
return result;
}
A final approach to cloning complex mutable objects is to call
super.clone, set all of the fields in the resulting object to their initial
state, and then call higher-level methods to regenerate the state of the original
object. In the case of our HashTable example, the buckets field would
be initialized to a new bucket array, and the put(key, value) method
(not shown) would be invoked for each key-value mapping in the hash table
being cloned. This approach typically yields a simple, reasonably elegant
clone method that does not run as quickly as one that directly manipulates
the innards of the clone. While this approach is clean, it is antithetical to the
whole Cloneable architecture because it blindly overwrites the field-byfield object copy that forms the basis of the architecture.
Like a constructor, a clone method must never invoke an overridable
method on the clone under construction (Item 19). If clone invokes a
method that is overridden in a subclass, this method will execute before the
subclass has had a chance to fix its state in the clone, quite possibly leading to
corruption in the clone and the original. Therefore, the put(key, value)
method discussed in the previous paragraph should be either final or private.
(If it is private, it is presumably the “helper method” for a nonfinal public
method.)
Object’s clone method is declared to throw
CloneNotSupportedException, but overriding methods need not.
Public clone methods should omit the throws clause, as methods that
don’t throw checked exceptions are easier to use (Item 71).
You have two choices when designing a class for inheritance (Item 19), but
whichever one you choose, the class should not implement Cloneable.
You may choose to mimic the behavior of Object by implementing a
properly functioning protected clone method that is declared to throw
CloneNotSupportedException. This gives subclasses the freedom to
implement Cloneable or not, just as if they extended Object directly.
Alternatively, you may choose not to implement a working clone method,
and to prevent subclasses from implementing one, by providing the following
degenerate clone implementation:
Click here to view code image
87
// clone method for extendable class not supporting
Cloneable
@Override
protected final Object clone() throws
CloneNotSupportedException {
throw new CloneNotSupportedException();
}
There is one more detail that bears noting. If you write a thread-safe class
that implements Cloneable, remember that its clone method must be
properly synchronized, just like any other method (Item 78). Object’s
clone method is not synchronized, so even if its implementation is
otherwise satisfactory, you may have to write a synchronized clone method
that returns super.clone().
To recap, all classes that implement Cloneable should override clone
with a public method whose return type is the class itself. This method should
first call super.clone, then fix any fields that need fixing. Typically, this
means copying any mutable objects that comprise the internal “deep
structure” of the object and replacing the clone’s references to these objects
with references to their copies. While these internal copies can usually be
made by calling clone recursively, this is not always the best approach. If
the class contains only primitive fields or references to immutable objects,
then it is likely the case that no fields need to be fixed. There are exceptions
to this rule. For example, a field representing a serial number or other unique
ID will need to be fixed even if it is primitive or immutable.
Is all this complexity really necessary? Rarely. If you extend a class that
already implements Cloneable, you have little choice but to implement a
well-behaved clone method. Otherwise, you are usually better off providing
an alternative means of object copying. A better approach to object copying
is to provide a copy constructor or copy factory. A copy constructor is
simply a constructor that takes a single argument whose type is the class
containing the constructor, for example,
Click here to view code image
// Copy constructor
public Yum(Yum yum) { ... };
A copy factory is the static factory (Item 1) analogue of a copy constructor:
Click here to view code image
// Copy factory
public static Yum newInstance(Yum yum) { ... };
The copy constructor approach and its static factory variant have many
88
advantages over Cloneable/clone: they don’t rely on a risk-prone
extralinguistic object creation mechanism; they don’t demand unenforceable
adherence to thinly documented conventions; they don’t conflict with the
proper use of final fields; they don’t throw unnecessary checked exceptions;
and they don’t require casts.
Furthermore, a copy constructor or factory can take an argument whose
type is an interface implemented by the class. For example, by convention all
general-purpose collection implementations provide a constructor whose
argument is of type Collection or Map. Interface-based copy constructors
and factories, more properly known as conversion constructors and
conversion factories, allow the client to choose the implementation type of
the copy rather than forcing the client to accept the implementation type of
the original. For example, suppose you have a HashSet, s, and you want to
copy it as a TreeSet. The clone method can’t offer this functionality, but
it’s easy with a conversion constructor: new TreeSet<>(s).
Given all the problems associated with Cloneable, new interfaces
should not extend it, and new extendable classes should not implement it.
While it’s less harmful for final classes to implement Cloneable, this
should be viewed as a performance optimization, reserved for the rare cases
where it is justified (Item 67). As a rule, copy functionality is best provided
by constructors or factories. A notable exception to this rule is arrays, which
are best copied with the clone method.
Item 14: Consider implementing Comparable
Unlike the other methods discussed in this chapter, the compareTo method
is not declared in Object. Rather, it is the sole method in the Comparable
interface. It is similar in character to Object’s equals method, except that
it permits order comparisons in addition to simple equality comparisons, and
it is generic. By implementing Comparable, a class indicates that its
instances have a natural ordering. Sorting an array of objects that implement
Comparable is as simple as this:
Arrays.sort(a);
It is similarly easy to search, compute extreme values, and maintain
automatically sorted collections of Comparable objects. For example, the
following program, which relies on the fact that String implements
Comparable, prints an alphabetized list of its command-line arguments
with duplicates eliminated:
Click here to view code image
89
public class WordList {
public static void main(String[] args) {
Set<String> s = new TreeSet<>();
Collections.addAll(s, args);
System.out.println(s);
}
}
By implementing Comparable, you allow your class to interoperate with
all of the many generic algorithms and collection implementations that
depend on this interface. You gain a tremendous amount of power for a small
amount of effort. Virtually all of the value classes in the Java platform
libraries, as well as all enum types (Item 34), implement Comparable. If
you are writing a value class with an obvious natural ordering, such as
alphabetical order, numerical order, or chronological order, you should
implement the Comparable interface:
Click here to view code image
public interface Comparable<T> {
int compareTo(T t);
}
The general contract of the compareTo method is similar to that of
equals:
Compares this object with the specified object for order. Returns a negative
integer, zero, or a positive integer as this object is less than, equal to, or
greater than the specified object. Throws ClassCastException if the
specified object’s type prevents it from being compared to this object.
In the following description, the notation sgn(expression) designates the
mathematical signum function, which is defined to return -1, 0, or 1,
according to whether the value of expression is negative, zero, or positive.
• The implementor must ensure that sgn(x.compareTo(y)) == -
sgn(y. compareTo(x)) for all x and y. (This implies that
x.compareTo(y) must throw an exception if and only if
y.compareTo(x) throws an exception.)
• The implementor must also ensure that the relation is transitive: (x.
compareTo(y) > 0 && y.compareTo(z) > 0) implies
x.compareTo(z) > 0.
• Finally, the implementor must ensure that x.compareTo(y) == 0
implies that sgn(x.compareTo(z)) ==
sgn(y.compareTo(z)), for all z.
90
• It is strongly recommended, but not required, that (x.compareTo(y)
== 0) == (x.equals(y)). Generally speaking, any class that
implements the Comparable interface and violates this condition
should clearly indicate this fact. The recommended language is “Note:
This class has a natural ordering that is inconsistent with equals.”
Don’t be put off by the mathematical nature of this contract. Like the
equals contract (Item 10), this contract isn’t as complicated as it looks.
Unlike the equals method, which imposes a global equivalence relation on
all objects, compareTo doesn’t have to work across objects of different
types: when confronted with objects of different types, compareTo is
permitted to throw ClassCastException. Usually, that is exactly what it
does. The contract does permit intertype comparisons, which are typically
defined in an interface implemented by the objects being compared.
Just as a class that violates the hashCode contract can break other classes
that depend on hashing, a class that violates the compareTo contract can
break other classes that depend on comparison. Classes that depend on
comparison include the sorted collections TreeSet and TreeMap and the
utility classes Collections and Arrays, which contain searching and
sorting algorithms.
Let’s go over the provisions of the compareTo contract. The first
provision says that if you reverse the direction of a comparison between two
object references, the expected thing happens: if the first object is less than
the second, then the second must be greater than the first; if the first object is
equal to the second, then the second must be equal to the first; and if the first
object is greater than the second, then the second must be less than the first.
The second provision says that if one object is greater than a second and the
second is greater than a third, then the first must be greater than the third. The
final provision says that all objects that compare as equal must yield the same
results when compared to any other object.
One consequence of these three provisions is that the equality test imposed
by a compareTo method must obey the same restrictions imposed by the
equals con-tract: reflexivity, symmetry, and transitivity. Therefore, the
same caveat applies: there is no way to extend an instantiable class with a
new value component while preserving the compareTo contract, unless you
are willing to forgo the benefits of object-oriented abstraction (Item 10). The
same workaround applies, too. If you want to add a value component to a
class that implements Comparable, don’t extend it; write an unrelated class
containing an instance of the first class. Then provide a “view” method that
returns the contained instance. This frees you to implement whatever
compareTo method you like on the containing class, while allowing its
91
client to view an instance of the containing class as an instance of the
contained class when needed.
The final paragraph of the compareTo contract, which is a strong
suggestion rather than a true requirement, simply states that the equality test
imposed by the compareTo method should generally return the same results
as the equals method. If this provision is obeyed, the ordering imposed by
the compareTo method is said to be consistent with equals. If it’s
violated, the ordering is said to be inconsistent with equals. A class whose
compareTo method imposes an order that is inconsistent with equals will
still work, but sorted collections containing elements of the class may not
obey the general contract of the appropriate collection interfaces
(Collection, Set, or Map). This is because the general contracts for these
interfaces are defined in terms of the equals method, but sorted collections
use the equality test imposed by compareTo in place of equals. It is not a
catastrophe if this happens, but it’s something to be aware of.
For example, consider the BigDecimal class, whose compareTo
method is inconsistent with equals. If you create an empty HashSet
instance and then add new BigDecimal("1.0") and new
BigDecimal("1.00"), the set will contain two elements because the two
BigDecimal instances added to the set are unequal when compared using
the equals method. If, however, you perform the same procedure using a
TreeSet instead of a HashSet, the set will contain only one element
because the two BigDecimal instances are equal when compared using the
compareTo method. (See the BigDecimal documentation for details.)
Writing a compareTo method is similar to writing an equals method,
but there are a few key differences. Because the Comparable interface is
parameterized, the compareTo method is statically typed, so you don’t need
to type check or cast its argument. If the argument is of the wrong type, the
invocation won’t even compile. If the argument is null, the invocation
should throw a NullPointer-Exception, and it will, as soon as the
method attempts to access its members.
In a compareTo method, fields are compared for order rather than
equality. To compare object reference fields, invoke the compareTo method
recursively. If a field does not implement Comparable or you need a
nonstandard ordering, use a Comparator instead. You can write your own
comparator or use an existing one, as in this compareTo method for
CaseInsensitiveString in Item 10:
Click here to view code image
// Single-field Comparable with object reference field
public final class CaseInsensitiveString
92
implements Comparable<CaseInsensitiveString> {
public int compareTo(CaseInsensitiveString cis) {
return String.CASE_INSENSITIVE_ORDER.compare(s,
cis.s);
}
... // Remainder omitted
}
Note that CaseInsensitiveString implements
Comparable<CaseInsensitiveString>. This means that a
CaseInsensitiveString reference can be compared only to another
CaseInsensitiveString reference. This is the normal pattern to follow
when declaring a class to implement Comparable.
Prior editions of this book recommended that compareTo methods
compare integral primitive fields using the relational operators < and >, and
floating point primitive fields using the static methods Double.compare
and Float.compare. In Java 7, static compare methods were added to
all of Java’s boxed primitive classes. Use of the relational operators < and
> in compareTo methods is verbose and error-prone and no longer
recommended.
If a class has multiple significant fields, the order in which you compare
them is critical. Start with the most significant field and work your way down.
If a comparison results in anything other than zero (which represents
equality), you’re done; just return the result. If the most significant field is
equal, compare the next-most-significant field, and so on, until you find an
unequal field or compare the least significant field. Here is a compareTo
method for the PhoneNumber class in Item 11 demonstrating this
technique:
Click here to view code image
// Multiple-field Comparable with primitive fields
public int compareTo(PhoneNumber pn) {
int result = Short.compare(areaCode, pn.areaCode);
if (result == 0) {
result = Short.compare(prefix, pn.prefix);
if (result == 0)
result = Short.compare(lineNum, pn.lineNum);
}
return result;
}
In Java 8, the Comparator interface was outfitted with a set of
comparator construction methods, which enable fluent construction of
comparators. These comparators can then be used to implement a
93
compareTo method, as required by the Comparable interface. Many
programmers prefer the conciseness of this approach, though it does come at a
modest performance cost: sorting arrays of PhoneNumber instances is about
10% slower on my machine. When using this approach, consider using Java’s
static import facility so you can refer to static comparator construction
methods by their simple names for clarity and brevity. Here’s how the
compareTo method for PhoneNumber looks using this approach:
Click here to view code image
// Comparable with comparator construction methods
private static final Comparator<PhoneNumber> COMPARATOR
=
comparingInt((PhoneNumber pn) -> pn.areaCode)
.thenComparingInt(pn -> pn.prefix)
.thenComparingInt(pn -> pn.lineNum);
public int compareTo(PhoneNumber pn) {
return COMPARATOR.compare(this, pn);
}
This implementation builds a comparator at class initialization time, using
two comparator construction methods. The first is comparingInt. It is a
static method that takes a key extractor function that maps an object reference
to a key of type int and returns a comparator that orders instances according
to that key. In the previous example, comparingInt takes a lambda () that
extracts the area code from a PhoneNumber and returns a
Comparator<PhoneNumber> that orders phone numbers according to
their area codes. Note that the lambda explicitly specifies the type of its input
parameter (PhoneNumber pn). It turns out that in this situation, Java’s type
inference isn’t powerful enough to figure the type out for itself, so we’re
forced to help it in order to make the program compile.
If two phone numbers have the same area code, we need to further refine
the comparison, and that’s exactly what the second comparator construction
method, thenComparingInt, does. It is an instance method on
Comparator that takes an int key extractor function, and returns a
comparator that first applies the original comparator and then uses the
extracted key to break ties. You can stack up as many calls to
thenComparingInt as you like, resulting in a lexicographic ordering. In
the example above, we stack up two calls to thenComparingInt,
resulting in an ordering whose secondary key is the prefix and whose tertiary
key is the line number. Note that we did not have to specify the parameter
type of the key extractor function passed to either of the calls to
thenComparingInt: Java’s type inference was smart enough to figure
94
this one out for itself.
The Comparator class has a full complement of construction methods.
There are analogues to comparingInt and thenComparingInt for the
primitive types long and double. The int versions can also be used for
narrower integral types, such as short, as in our PhoneNumber example.
The double versions can also be used for float. This provides coverage of
all of Java’s numerical primitive types.
There are also comparator construction methods for object reference types.
The static method, named comparing, has two overloadings. One takes a
key extractor and uses the keys’ natural order. The second takes both a key
extractor and a comparator to be used on the extracted keys. There are three
overloadings of the instance method, which is named thenComparing.
One overloading takes only a comparator and uses it to provide a secondary
order. A second overloading takes only a key extractor and uses the key’s
natural order as a secondary order. The final overloading takes both a key
extractor and a comparator to be used on the extracted keys.
Occasionally you may see compareTo or compare methods that rely on
the fact that the difference between two values is negative if the first value is
less than the second, zero if the two values are equal, and positive if the first
value is greater. Here is an example:
Click here to view code image
// BROKEN difference-based comparator - violates
transitivity!
static Comparator<Object> hashCodeOrder = new
Comparator<>() {
public int compare(Object o1, Object o2) {
return o1.hashCode() - o2.hashCode();
}
};
Do not use this technique. It is fraught with danger from integer overflow and
IEEE 754 floating point arithmetic artifacts [JLS 15.20.1, 15.21.1].
Furthermore, the resulting methods are unlikely to be significantly faster than
those written using the techniques described in this item. Use either a static
compare method:
Click here to view code image
// Comparator based on static compare method
static Comparator<Object> hashCodeOrder = new
Comparator<>() {
public int compare(Object o1, Object o2) {
return Integer.compare(o1.hashCode(),
o2.hashCode());
95
}
};
or a comparator construction method:
Click here to view code image
// Comparator based on Comparator construction method
static Comparator<Object> hashCodeOrder =
Comparator.comparingInt(o -> o.hashCode());
In summary, whenever you implement a value class that has a sensible
ordering, you should have the class implement the Comparable interface so
that its instances can be easily sorted, searched, and used in comparison-based
collections. When comparing field values in the implementations of the
compareTo methods, avoid the use of the < and > operators. Instead, use
the static compare methods in the boxed primitive classes or the comparator
construction methods in the Comparator interface.
96
Chapter 4. Classes and Interfaces
CLASSES and interfaces lie at the heart of the Java programming language.
They are its basic units of abstraction. The language provides many powerful
elements that you can use to design classes and interfaces. This chapter
contains guidelines to help you make the best use of these elements so that
your classes and interfaces are usable, robust, and flexible.
Item 15: Minimize the accessibility of classes and
members
The single most important factor that distinguishes a well-designed
component from a poorly designed one is the degree to which the component
hides its internal data and other implementation details from other
components. A well-designed component hides all its implementation details,
cleanly separating its API from its implementation. Components then
communicate only through their APIs and are oblivious to each others’ inner
workings. This concept, known as information hiding or encapsulation, is a
fundamental tenet of software design [Parnas72].
Information hiding is important for many reasons, most of which stem from
the fact that it decouples the components that comprise a system, allowing
them to be developed, tested, optimized, used, understood, and modified in
isolation. This speeds up system development because components can be
developed in parallel. It eases the burden of maintenance because components
can be understood more quickly and debugged or replaced with little fear of
harming other components. While information hiding does not, in and of
itself, cause good performance, it enables effective performance tuning: once
a system is complete and profiling has determined which components are
causing performance problems (Item 67), those components can be optimized
without affecting the correctness of others. Information hiding increases
software reuse because components that aren’t tightly coupled often prove
useful in other contexts besides the ones for which they were developed.
Finally, information hiding decreases the risk in building large systems
because individual components may prove successful even if the system does
not.
Java has many facilities to aid in information hiding. The access control
mechanism [JLS, 6.6] specifies the accessibility of classes, interfaces, and
members. The accessibility of an entity is determined by the location of its
declaration and by which, if any, of the access modifiers (private,
97
protected, and public) is present on the declaration. Proper use of these
modifiers is essential to information hiding.
The rule of thumb is simple: make each class or member as inaccessible
as possible. In other words, use the lowest possible access level consistent
with the proper functioning of the software that you are writing.
For top-level (non-nested) classes and interfaces, there are only two
possible access levels: package-private and public. If you declare a top-level
class or interface with the public modifier, it will be public; otherwise, it
will be package-private. If a top-level class or interface can be made packageprivate, it should be. By making it package-private, you make it part of the
implementation rather than the exported API, and you can modify it, replace
it, or eliminate it in a subsequent release without fear of harming existing
clients. If you make it public, you are obligated to support it forever to
maintain compatibility.
If a package-private top-level class or interface is used by only one class,
consider making the top-level class a private static nested class of the sole
class that uses it (Item 24). This reduces its accessibility from all the classes
in its package to the one class that uses it. But it is far more important to
reduce the accessibility of a gratuitously public class than of a packageprivate top-level class: the public class is part of the package’s API, while the
package-private top-level class is already part of its implementation.
For members (fields, methods, nested classes, and nested interfaces), there
are four possible access levels, listed here in order of increasing accessibility:
• private—The member is accessible only from the top-level class where it
is declared.
• package-private—The member is accessible from any class in the
package where it is declared. Technically known as default access, this is
the access level you get if no access modifier is specified (except for
interface members, which are public by default).
• protected—The member is accessible from subclasses of the class where it
is declared (subject to a few restrictions [JLS, 6.6.2]) and from any class in
the package where it is declared.
• public—The member is accessible from anywhere.
After carefully designing your class’s public API, your reflex should be to
make all other members private. Only if another class in the same package
really needs to access a member should you remove the private modifier,
making the member package-private. If you find yourself doing this often,
you should reexamine the design of your system to see if another
decomposition might yield classes that are better decoupled from one another.
That said, both private and package-private members are part of a class’s
98
implementation and do not normally impact its exported API. These fields
can, however, “leak” into the exported API if the class implements
Serializable (Items 86 and 87).
For members of public classes, a huge increase in accessibility occurs when
the access level goes from package-private to protected. A protected member
is part of the class’s exported API and must be supported forever. Also, a
protected member of an exported class represents a public commitment to an
implementation detail (Item 19). The need for protected members should be
relatively rare.
There is a key rule that restricts your ability to reduce the accessibility of
methods. If a method overrides a superclass method, it cannot have a more
restrictive access level in the subclass than in the superclass [JLS, 8.4.8.3].
This is necessary to ensure that an instance of the subclass is usable anywhere
that an instance of the superclass is usable (the Liskov substitution principle,
see Item 15). If you violate this rule, the compiler will generate an error
message when you try to compile the subclass. A special case of this rule is
that if a class implements an interface, all of the class methods that are in the
interface must be declared public in the class.
To facilitate testing your code, you may be tempted to make a class,
interface, or member more accessible than otherwise necessary. This is fine
up to a point. It is acceptable to make a private member of a public class
package-private in order to test it, but it is not acceptable to raise the
accessibility any higher. In other words, it is not acceptable to make a class,
interface, or member a part of a pack-age’s exported API to facilitate testing.
Luckily, it isn’t necessary either because tests can be made to run as part of
the package being tested, thus gaining access to its package-private elements.
Instance fields of public classes should rarely be public (Item 16). If an
instance field is nonfinal or is a reference to a mutable object, then by making
it public, you give up the ability to limit the values that can be stored in the
field. This means you give up the ability to enforce invariants involving the
field. Also, you give up the ability to take any action when the field is
modified, so classes with public mutable fields are not generally threadsafe. Even if a field is final and refers to an immutable object, by making it
public you give up the flexibility to switch to a new internal data
representation in which the field does not exist.
The same advice applies to static fields, with one exception. You can
expose constants via public static final fields, assuming the constants form an
integral part of the abstraction provided by the class. By convention, such
fields have names consisting of capital letters, with words separated by
underscores (Item 68). It is critical that these fields contain either primitive
values or references to immutable objects (Item 17). a field containing a
reference to a mutable object has all the disadvantages of a nonfinal field.
99
While the reference cannot be modified, the referenced object can be
modified—with disastrous results.
Note that a nonzero-length array is always mutable, so it is wrong for a
class to have a public static final array field, or an accessor that returns
such a field. If a class has such a field or accessor, clients will be able to
modify the contents of the array. This is a frequent source of security holes:
Click here to view code image
// Potential security hole!
public static final Thing[] VALUES = { ... };
Beware of the fact that some IDEs generate accessors that return references to
private array fields, resulting in exactly this problem. There are two ways to
fix the problem. You can make the public array private and add a public
immutable list:
Click here to view code image
private static final Thing[] PRIVATE_VALUES = { ... };
public static final List<Thing> VALUES =
Collections.unmodifiableList(Arrays.asList(PRIVATE_VALUES));
Alternatively, you can make the array private and add a public method that
returns a copy of a private array:
Click here to view code image
private static final Thing[] PRIVATE_VALUES = { ... };
public static final Thing[] values() {
return PRIVATE_VALUES.clone();
}
To choose between these alternatives, think about what the client is likely to
do with the result. Which return type will be more convenient? Which will
give better performance?
As of Java 9, there are two additional, implicit access levels introduced as
part of the module system. A module is a grouping of packages, like a
package is a grouping of classes. A module may explicitly export some of its
packages via export declarations in its module declaration (which is by
convention contained in a source file named module-info.java). Public
and protected members of unexported packages in a module are inaccessible
outside the module; within the module, accessibility is unaffected by export
declarations. Using the module system allows you to share classes among
packages within a module without making them visible to the entire world.
Public and protected members of public classes in unexported packages give
rise to the two implicit access levels, which are intramodular analogues of the
normal public and protected levels. The need for this kind of sharing is
100
relatively rare and can often be eliminated by rearranging the classes within
your packages.
Unlike the four main access levels, the two module-based levels are largely
advisory. If you place a module’s JAR file on your application’s class path
instead of its module path, the packages in the module revert to their nonmodular behavior: all of the public and protected members of the packages’
public classes have their normal accessibility, regardless of whether the
packages are exported by the module [Reinhold, 1.2]. The one place where
the newly introduced access levels are strictly enforced is the JDK itself: the
unexported packages in the Java libraries are truly inaccessible outside of
their modules.
Not only is the access protection afforded by modules of limited utility to
the typical Java programmer, and largely advisory in nature; in order to take
advantage of it, you must group your packages into modules, make all of their
dependencies explicit in module declarations, rearrange your source tree, and
take special actions to accommodate any access to non-modularized packages
from within your modules [Reinhold, 3]. It is too early to say whether
modules will achieve widespread use outside of the JDK itself. In the
meantime, it seems best to avoid them unless you have a compelling need.
To summarize, you should reduce accessibility of program elements as
much as possible (within reason). After carefully designing a minimal public
API, you should prevent any stray classes, interfaces, or members from
becoming part of the API. With the exception of public static final fields,
which serve as constants, public classes should have no public fields. Ensure
that objects referenced by public static final fields are immutable.
Item 16: In public classes, use accessor methods, not
public fields
Occasionally, you may be tempted to write degenerate classes that serve no
purpose other than to group instance fields:
Click here to view code image
// Degenerate classes like this should not be public!
class Point {
public double x;
public double y;
}
Because the data fields of such classes are accessed directly, these classes
do not offer the benefits of encapsulation (Item 15). You can’t change the
representation without changing the API, you can’t enforce invariants, and
you can’t take auxiliary action when a field is accessed. Hard-line object101
oriented programmers feel that such classes are anathema and should always
be replaced by classes with private fields and public accessor methods
(getters) and, for mutable classes, mutators (setters):
Click here to view code image
// Encapsulation of data by accessor methods and
mutators
class Point {
private double x;
private double y;
public Point(double x, double y) {
this.x = x;
this.y = y;
}
public double getX() { return x; }
public double getY() { return y; }
public void setX(double x) { this.x = x; }
public void setY(double y) { this.y = y; }
}
Certainly, the hard-liners are correct when it comes to public classes: if a
class is accessible outside its package, provide accessor methods to
preserve the flexibility to change the class’s internal representation. If a
public class exposes its data fields, all hope of changing its representation is
lost because client code can be distributed far and wide.
However, if a class is package-private or is a private nested class, there
is nothing inherently wrong with exposing its data fields—assuming they
do an adequate job of describing the abstraction provided by the class. This
approach generates less visual clutter than the accessor-method approach,
both in the class definition and in the client code that uses it. While the client
code is tied to the class’s internal representation, this code is confined to the
package containing the class. If a change in representation becomes desirable,
you can make the change without touching any code outside the package. In
the case of a private nested class, the scope of the change is further restricted
to the enclosing class.
Several classes in the Java platform libraries violate the advice that public
classes should not expose fields directly. Prominent examples include the
Point and Dimension classes in the java.awt package. Rather than
examples to be emulated, these classes should be regarded as cautionary tales.
As described in Item 67, the decision to expose the internals of the
Dimension class resulted in a serious performance problem that is still with
102
us today.
While it’s never a good idea for a public class to expose fields directly, it is
less harmful if the fields are immutable. You can’t change the representation
of such a class without changing its API, and you can’t take auxiliary actions
when a field is read, but you can enforce invariants. For example, this class
guarantees that each instance represents a valid time:
Click here to view code image
// Public class with exposed immutable fields -
questionable
public final class Time {
private static final int HOURS_PER_DAY = 24;
private static final int MINUTES_PER_HOUR = 60;
public final int hour;
public final int minute;
public Time(int hour, int minute) {
if (hour < 0 || hour >= HOURS_PER_DAY)
throw new IllegalArgumentException("Hour: " +
hour);
if (minute < 0 || minute >= MINUTES_PER_HOUR)
throw new IllegalArgumentException("Min: " +
minute);
this.hour = hour;
this.minute = minute;
}
... // Remainder omitted
}
In summary, public classes should never expose mutable fields. It is less
harmful, though still questionable, for public classes to expose immutable
fields. It is, however, sometimes desirable for package-private or private
nested classes to expose fields, whether mutable or immutable.
Item 17: Minimize mutability
An immutable class is simply a class whose instances cannot be modified. All
of the information contained in each instance is fixed for the lifetime of the
object, so no changes can ever be observed. The Java platform libraries
contain many immutable classes, including String, the boxed primitive
classes, and BigInteger and BigDecimal. There are many good reasons
for this: Immutable classes are easier to design, implement, and use than
mutable classes. They are less prone to error and are more secure.
To make a class immutable, follow these five rules:
103
1. Don’t provide methods that modify the object’s state (known as
mutators).
2. Ensure that the class can’t be extended. This prevents careless or
malicious subclasses from compromising the immutable behavior of the
class by behaving as if the object’s state has changed. Preventing
subclassing is generally accomplished by making the class final, but there
is an alternative that we’ll discuss later.
3. Make all fields final. This clearly expresses your intent in a manner that
is enforced by the system. Also, it is necessary to ensure correct behavior
if a reference to a newly created instance is passed from one thread to
another without synchronization, as spelled out in the memory model
[JLS, 17.5; Goetz06, 16].
4. Make all fields private. This prevents clients from obtaining access to
mutable objects referred to by fields and modifying these objects directly.
While it is technically permissible for immutable classes to have public
final fields containing primitive values or references to immutable
objects, it is not recommended because it precludes changing the internal
representation in a later release (Items 15 and 16).
5. Ensure exclusive access to any mutable components. If your class has
any fields that refer to mutable objects, ensure that clients of the class
cannot obtain references to these objects. Never initialize such a field to a
client-provided object reference or return the field from an accessor.
Make defensive copies (Item 50) in constructors, accessors, and
readObject methods (Item 88).
Many of the example classes in previous items are immutable. One such
class is PhoneNumber in Item 11, which has accessors for each attribute but
no corresponding mutators. Here is a slightly more complex example:
Click here to view code image
// Immutable complex number class
public final class Complex {
private final double re;
private final double im;
public Complex(double re, double im) {
this.re = re;
this.im = im;
}
public double realPart() { return re; }
104
public double imaginaryPart() { return im; }
public Complex plus(Complex c) {
return new Complex(re + c.re, im + c.im);
}
public Complex minus(Complex c) {
return new Complex(re - c.re, im - c.im);
}
public Complex times(Complex c) {
return new Complex(re * c.re - im * c.im,
re * c.im + im * c.re);
}
public Complex dividedBy(Complex c) {
double tmp = c.re * c.re + c.im * c.im;
return new Complex((re * c.re + im * c.im) /
tmp,
(im * c.re - re * c.im) /
tmp);
}
@Override public boolean equals(Object o) {
if (o == this)
return true;
if (!(o instanceof Complex))
return false;
Complex c = (Complex) o;
// See page 47 to find out why we use compare
instead of ==
return Double.compare(c.re, re) == 0
&& Double.compare(c.im, im) == 0;
}
@Override public int hashCode() {
return 31 * Double.hashCode(re) +
Double.hashCode(im);
}
@Override public String toString() {
return "(" + re + " + " + im + "i)";
}
}
This class represents a complex number (a number with both real and
imaginary parts). In addition to the standard Object methods, it provides
accessors for the real and imaginary parts and provides the four basic
105
arithmetic operations: addition, subtraction, multiplication, and division.
Notice how the arithmetic operations create and return a new Complex
instance rather than modifying this instance. This pattern is known as the
functional approach because methods return the result of applying a function
to their operand, without modifying it. Contrast it to the procedural or
imperative approach in which methods apply a procedure to their operand,
causing its state to change. Note that the method names are prepositions (such
as plus) rather than verbs (such as add). This emphasizes the fact that
methods don’t change the values of the objects. The BigInteger and
BigDecimal classes did not obey this naming convention, and it led to
many usage errors.
The functional approach may appear unnatural if you’re not familiar with
it, but it enables immutability, which has many advantages. Immutable
objects are simple. An immutable object can be in exactly one state, the state
in which it was created. If you make sure that all constructors establish class
invariants, then it is guaranteed that these invariants will remain true for all
time, with no further effort on your part or on the part of the programmer who
uses the class. Mutable objects, on the other hand, can have arbitrarily
complex state spaces. If the documentation does not provide a precise
description of the state transitions performed by mutator methods, it can be
difficult or impossible to use a mutable class reliably.
Immutable objects are inherently thread-safe; they require no
synchronization. They cannot be corrupted by multiple threads accessing
them concurrently. This is far and away the easiest approach to achieve thread
safety. Since no thread can ever observe any effect of another thread on an
immutable object, immutable objects can be shared freely. Immutable
classes should therefore encourage clients to reuse existing instances
wherever possible. One easy way to do this is to provide public static final
constants for commonly used values. For example, the Complex class might
provide these constants:
Click here to view code image
public static final Complex ZERO = new Complex(0, 0);
public static final Complex ONE = new Complex(1, 0);
public static final Complex I = new Complex(0, 1);
This approach can be taken one step further. An immutable class can
provide static factories (Item 1) that cache frequently requested instances to
avoid creating new instances when existing ones would do. All the boxed
primitive classes and BigInteger do this. Using such static factories
causes clients to share instances instead of creating new ones, reducing
memory footprint and garbage collection costs. Opting for static factories in
106
place of public constructors when designing a new class gives you the
flexibility to add caching later, without modifying clients.
A consequence of the fact that immutable objects can be shared freely is
that you never have to make defensive copies of them (Item 50). In fact, you
never have to make any copies at all because the copies would be forever
equivalent to the originals. Therefore, you need not and should not provide a
clone method or copy constructor (Item 13) on an immutable class. This
was not well understood in the early days of the Java platform, so the
String class does have a copy constructor, but it should rarely, if ever, be
used (Item 6).
Not only can you share immutable objects, but they can share their
internals. For example, the BigInteger class uses a sign-magnitude
representation internally. The sign is represented by an int, and the
magnitude is represented by an int array. The negate method produces a
new BigInteger of like magnitude and opposite sign. It does not need to
copy the array even though it is mutable; the newly created BigInteger
points to the same internal array as the original.
Immutable objects make great building blocks for other objects,
whether mutable or immutable. It’s much easier to maintain the invariants of
a complex object if you know that its component objects will not change
underneath it. A special case of this principle is that immutable objects make
great map keys and set elements: you don’t have to worry about their values
changing once they’re in the map or set, which would destroy the map or set’s
invariants.
Immutable objects provide failure atomicity for free (Item 76). Their
state never changes, so there is no possibility of a temporary inconsistency.
The major disadvantage of immutable classes is that they require a
separate object for each distinct value. Creating these objects can be costly,
especially if they are large. For example, suppose that you have a million-bit
BigInteger and you want to change its low-order bit:
BigInteger moby = ...;
moby = moby.flipBit(0);
The flipBit method creates a new BigInteger instance, also a million
bits long, that differs from the original in only one bit. The operation requires
time and space proportional to the size of the BigInteger. Contrast this to
java.util.BitSet. Like BigInteger, BitSet represents an
arbitrarily long sequence of bits, but unlike BigInteger, BitSet is
mutable. The BitSet class provides a method that allows you to change the
state of a single bit of a million-bit instance in constant time:
BitSet moby = ...;
107
moby.flip(0);
The performance problem is magnified if you perform a multistep
operation that generates a new object at every step, eventually discarding all
objects except the final result. There are two approaches to coping with this
problem. The first is to guess which multistep operations will be commonly
required and to provide them as primitives. If a multistep operation is
provided as a primitive, the immutable class does not have to create a separate
object at each step. Internally, the immutable class can be arbitrarily clever.
For example, BigInteger has a package-private mutable “companion
class” that it uses to speed up multistep operations such as modular
exponentiation. It is much harder to use the mutable companion class than to
use BigInteger, for all of the reasons outlined earlier. Luckily, you don’t
have to use it: the implementors of BigInteger did the hard work for you.
The package-private mutable companion class approach works fine if you
can accurately predict which complex operations clients will want to perform
on your immutable class. If not, then your best bet is to provide a public
mutable companion class. The main example of this approach in the Java
platform libraries is the String class, whose mutable companion is
StringBuilder (and its obsolete predecessor, StringBuffer).
Now that you know how to make an immutable class and you understand
the pros and cons of immutability, let’s discuss a few design alternatives.
Recall that to guarantee immutability, a class must not permit itself to be
subclassed. This can be done by making the class final, but there is another,
more flexible alternative. Instead of making an immutable class final, you can
make all of its constructors private or package-private and add public static
factories in place of the public constructors (Item 1). To make this concrete,
here’s how Complex would look if you took this approach:
Click here to view code image
// Immutable class with static factories instead of
constructors
public class Complex {
private final double re;
private final double im;
private Complex(double re, double im) {
this.re = re;
this.im = im;
}
public static Complex valueOf(double re, double im)
{
return new Complex(re, im);
108
}
... // Remainder unchanged
}
This approach is often the best alternative. It is the most flexible because it
allows the use of multiple package-private implementation classes. To its
clients that reside outside its package, the immutable class is effectively final
because it is impossible to extend a class that comes from another package
and that lacks a public or protected constructor. Besides allowing the
flexibility of multiple implementation classes, this approach makes it possible
to tune the performance of the class in subsequent releases by improving the
object-caching capabilities of the static factories.
It was not widely understood that immutable classes had to be effectively
final when BigInteger and BigDecimal were written, so all of their
methods may be overridden. Unfortunately, this could not be corrected after
the fact while preserving backward compatibility. If you write a class whose
security depends on the immutability of a BigInteger or BigDecimal
argument from an untrusted client, you must check to see that the argument is
a “real” BigInteger or BigDecimal, rather than an instance of an
untrusted subclass. If it is the latter, you must defensively copy it under the
assumption that it might be mutable (Item 50):
Click here to view code image
public static BigInteger safeInstance(BigInteger val) {
return val.getClass() == BigInteger.class ?
val : new BigInteger(val.toByteArray());
}
The list of rules for immutable classes at the beginning of this item says
that no methods may modify the object and that all its fields must be final. In
fact these rules are a bit stronger than necessary and can be relaxed to
improve performance. In truth, no method may produce an externally visible
change in the object’s state. However, some immutable classes have one or
more nonfinal fields in which they cache the results of expensive
computations the first time they are needed. If the same value is requested
again, the cached value is returned, saving the cost of recalculation. This trick
works precisely because the object is immutable, which guarantees that the
computation would yield the same result if it were repeated.
For example, PhoneNumber’s hashCode method (Item 11, page 53)
computes the hash code the first time it’s invoked and caches it in case it’s
invoked again. This technique, an example of lazy initialization (Item 83), is
also used by String.
One caveat should be added concerning serializability. If you choose to
109
have your immutable class implement Serializable and it contains one
or more fields that refer to mutable objects, you must provide an explicit
readObject or readResolve method, or use the
ObjectOutputStream.writeUnshared and
ObjectInputStream.readUnshared methods, even if the default
serialized form is acceptable. Otherwise an attacker could create a mutable
instance of your class. This topic is covered in detail in Item 88.
To summarize, resist the urge to write a setter for every getter. Classes
should be immutable unless there’s a very good reason to make them
mutable. Immutable classes provide many advantages, and their only
disadvantage is the potential for performance problems under certain
circumstances. You should always make small value objects, such as
PhoneNumber and Complex, immutable. (There are several classes in the
Java platform libraries, such as java.util.Date and
java.awt.Point, that should have been immutable but aren’t.) You
should seriously consider making larger value objects, such as String and
BigInteger, immutable as well. You should provide a public mutable
companion class for your immutable class only once you’ve confirmed that
it’s necessary to achieve satisfactory performance (Item 67).
There are some classes for which immutability is impractical. If a class
cannot be made immutable, limit its mutability as much as possible.
Reducing the number of states in which an object can exist makes it easier to
reason about the object and reduces the likelihood of errors. Therefore, make
every field final unless there is a compelling reason to make it nonfinal.
Combining the advice of this item with that of Item 15, your natural
inclination should be to declare every field private final unless
there’s a good reason to do otherwise.
Constructors should create fully initialized objects with all of their
invariants established. Don’t provide a public initialization method separate
from the constructor or static factory unless there is a compelling reason to do
so. Similarly, don’t provide a “reinitialize” method that enables an object to
be reused as if it had been constructed with a different initial state. Such
methods generally provide little if any performance benefit at the expense of
increased complexity.
The CountDownLatch class exemplifies these principles. It is mutable,
but its state space is kept intentionally small. You create an instance, use it
once, and it’s done: once the countdown latch’s count has reached zero, you
may not reuse it.
A final note should be added concerning the Complex class in this item.
This example was meant only to illustrate immutability. It is not an industrialstrength complex number implementation. It uses the standard formulas for
110
complex multiplication and division, which are not correctly rounded and
provide poor semantics for complex NaNs and infinities [Kahan91, Smith62,
Thomas94].
Item 18: Favor composition over inheritance
Inheritance is a powerful way to achieve code reuse, but it is not always the
best tool for the job. Used inappropriately, it leads to fragile software. It is
safe to use inheritance within a package, where the subclass and the
superclass implementations are under the control of the same programmers. It
is also safe to use inheritance when extending classes specifically designed
and documented for extension (Item 19). Inheriting from ordinary concrete
classes across package boundaries, however, is dangerous. As a reminder, this
book uses the word “inheritance” to mean implementation inheritance (when
one class extends another). The problems discussed in this item do not apply
to interface inheritance (when a class implements an interface or when one
interface extends another).
Unlike method invocation, inheritance violates encapsulation
[Snyder86]. In other words, a subclass depends on the implementation details
of its superclass for its proper function. The superclass’s implementation may
change from release to release, and if it does, the subclass may break, even
though its code has not been touched. As a consequence, a subclass must
evolve in tandem with its superclass, unless the superclass’s authors have
designed and documented it specifically for the purpose of being extended.
To make this concrete, let’s suppose we have a program that uses a
HashSet. To tune the performance of our program, we need to query the
HashSet as to how many elements have been added since it was created
(not to be confused with its current size, which goes down when an element is
removed). To provide this functionality, we write a HashSet variant that
keeps count of the number of attempted element insertions and exports an
accessor for this count. The HashSet class contains two methods capable of
adding elements, add and addAll, so we override both of these methods:
Click here to view code image
// Broken - Inappropriate use of inheritance!
public class InstrumentedHashSet<E> extends HashSet<E> {
// The number of attempted element insertions
private int addCount = 0;
public InstrumentedHashSet() {
}
public InstrumentedHashSet(int initCap, float
111
loadFactor) {
super(initCap, loadFactor);
}
@Override public boolean add(E e) {
addCount++;
return super.add(e);
}
@Override public boolean addAll(Collection<? extends
E> c) {
addCount += c.size();
return super.addAll(c);
}
public int getAddCount() {
return addCount;
}
}
This class looks reasonable, but it doesn’t work. Suppose we create an
instance and add three elements using the addAll method. Incidentally, note
that we create a list using the static factory method List.of, which was
added in Java 9; if you’re using an earlier release, use Arrays.asList
instead:
Click here to view code image
InstrumentedHashSet<String> s = new
InstrumentedHashSet<>();
s.addAll(List.of("Snap", "Crackle", "Pop"));
We would expect the getAddCount method to return three at this point,
but it returns six. What went wrong? Internally, HashSet’s addAll method
is implemented on top of its add method, although HashSet, quite
reasonably, does not document this implementation detail. The addAll
method in Instrumented-HashSet added three to addCount and then
invoked HashSet’s addAll implementation using super.addAll. This
in turn invoked the add method, as overridden in
InstrumentedHashSet, once for each element. Each of these three
invocations added one more to addCount, for a total increase of six: each
element added with the addAll method is double-counted.
We could “fix” the subclass by eliminating its override of the addAll
method. While the resulting class would work, it would depend for its proper
function on the fact that HashSet’s addAll method is implemented on top
of its add method. This “self-use” is an implementation detail, not
guaranteed to hold in all implementations of the Java platform and subject to
change from release to release. Therefore, the resulting
112
InstrumentedHashSet class would be fragile.
It would be slightly better to override the addAll method to iterate over
the specified collection, calling the add method once for each element. This
would guarantee the correct result whether or not HashSet’s addAll
method were implemented atop its add method because HashSet’s
addAll implementation would no longer be invoked. This technique,
however, does not solve all our problems. It amounts to reimplementing
superclass methods that may or may not result in self-use, which is difficult,
time-consuming, error-prone, and may reduce performance. Additionally, it
isn’t always possible because some methods cannot be implemented without
access to private fields inaccessible to the subclass.
A related cause of fragility in subclasses is that their superclass can acquire
new methods in subsequent releases. Suppose a program depends for its
security on the fact that all elements inserted into some collection satisfy
some predicate. This can be guaranteed by subclassing the collection and
overriding each method capable of adding an element to ensure that the
predicate is satisfied before adding the element. This works fine until a new
method capable of inserting an element is added to the superclass in a
subsequent release. Once this happens, it becomes possible to add an “illegal”
element merely by invoking the new method, which is not overridden in the
subclass. This is not a purely theoretical problem. Several security holes of
this nature had to be fixed when Hashtable and Vector were retrofitted
to participate in the Collections Framework.
Both of these problems stem from overriding methods. You might think
that it is safe to extend a class if you merely add new methods and refrain
from overriding existing methods. While this sort of extension is much safer,
it is not without risk. If the superclass acquires a new method in a subsequent
release and you have the bad luck to have given the subclass a method with
the same signature and a different return type, your subclass will no longer
compile [JLS, 8.4.8.3]. If you’ve given the subclass a method with the same
signature and return type as the new superclass method, then you’re now
overriding it, so you’re subject to the problems described earlier.
Furthermore, it is doubtful that your method will fulfill the contract of the
new superclass method, because that contract had not yet been written when
you wrote the subclass method.
Luckily, there is a way to avoid all of the problems described above.
Instead of extending an existing class, give your new class a private field that
references an instance of the existing class. This design is called composition
because the existing class becomes a component of the new one. Each
instance method in the new class invokes the corresponding method on the
contained instance of the existing class and returns the results. This is known
113
as forwarding, and the methods in the new class are known as forwarding
methods. The resulting class will be rock solid, with no dependencies on the
implementation details of the existing class. Even adding new methods to the
existing class will have no impact on the new class. To make this concrete,
here’s a replacement for InstrumentedHashSet that uses the
composition-and-forwarding approach. Note that the implementation is
broken into two pieces, the class itself and a reusable forwarding class, which
contains all of the forwarding methods and nothing else:
Click here to view code image
// Wrapper class - uses composition in place of
inheritance
public class InstrumentedSet<E> extends ForwardingSet<E>
{
private int addCount = 0;
public InstrumentedSet(Set<E> s) {
super(s);
}
@Override public boolean add(E e) {
addCount++;
return super.add(e);
}
@Override public boolean addAll(Collection<?
extends E> c) {
addCount += c.size();
return super.addAll(c);
}
public int getAddCount() {
return addCount;
}
}
// Reusable forwarding class
public class ForwardingSet<E> implements Set<E> {
private final Set<E> s;
public ForwardingSet(Set<E> s) { this.s = s; }
public void clear() {
s.clear(); }
public boolean contains(Object o) { return
s.contains(o); }
public boolean isEmpty() { return
s.isEmpty(); }
public int size() { return
s.size(); }
114
public Iterator<E> iterator() { return
s.iterator(); }
public boolean add(E e) { return
s.add(e); }
public boolean remove(Object o) { return
s.remove(o); }
public boolean containsAll(Collection<?> c)
{ return
s.containsAll(c); }
public boolean addAll(Collection<? extends E> c)
{ return
s.addAll(c); }
public boolean removeAll(Collection<?> c)
{ return
s.removeAll(c); }
public boolean retainAll(Collection<?> c)
{ return
s.retainAll(c); }
public Object[] toArray() { return
s.toArray(); }
public <T> T[] toArray(T[] a) { return
s.toArray(a); }
@Override public boolean equals(Object o)
{ return
s.equals(o); }
@Override public int hashCode() { return
s.hashCode(); }
@Override public String toString() { return
s.toString(); }
}
The design of the InstrumentedSet class is enabled by the existence
of the Set interface, which captures the functionality of the HashSet class.
Besides being robust, this design is extremely flexible. The
InstrumentedSet class implements the Set interface and has a single
constructor whose argument is also of type Set. In essence, the class
transforms one Set into another, adding the instrumentation functionality.
Unlike the inheritance-based approach, which works only for a single
concrete class and requires a separate constructor for each supported
constructor in the superclass, the wrapper class can be used to instrument any
Set implementation and will work in conjunction with any preexisting
constructor:
Click here to view code image
Set<Instant> times = new InstrumentedSet<>(new TreeSet<>
(cmp));
115
Set<E> s = new InstrumentedSet<>(new HashSet<>
(INIT_CAPACITY));
The InstrumentedSet class can even be used to temporarily
instrument a set instance that has already been used without instrumentation:
Click here to view code image
static void walk(Set<Dog> dogs) {
InstrumentedSet<Dog> iDogs = new InstrumentedSet<>
(dogs);
... // Within this method use iDogs instead of dogs
}
The InstrumentedSet class is known as a wrapper class because each
InstrumentedSet instance contains (“wraps”) another Set instance.
This is also known as the Decorator pattern [Gamma95] because the
InstrumentedSet class “decorates” a set by adding instrumentation.
Sometimes the combination of composition and forwarding is loosely referred
to as delegation. Technically it’s not delegation unless the wrapper object
passes itself to the wrapped object [Lieberman86; Gamma95].
The disadvantages of wrapper classes are few. One caveat is that wrapper
classes are not suited for use in callback frameworks, wherein objects pass
self-references to other objects for subsequent invocations (“callbacks”).
Because a wrapped object doesn’t know of its wrapper, it passes a reference
to itself (this) and callbacks elude the wrapper. This is known as the SELF
problem [Lieberman86]. Some people worry about the performance impact of
forwarding method invocations or the memory footprint impact of wrapper
objects. Neither turn out to have much impact in practice. It’s tedious to write
forwarding methods, but you have to write the reusable forwarding class for
each interface only once, and forwarding classes may be provided for you.
For example, Guava provides forwarding classes for all of the collection
interfaces [Guava].
Inheritance is appropriate only in circumstances where the subclass really
is a subtype of the superclass. In other words, a class B should extend a class
A only if an “is-a” relationship exists between the two classes. If you are
tempted to have a class B extend a class A, ask yourself the question: Is every
B really an A? If you cannot truthfully answer yes to this question, B should
not extend A. If the answer is no, it is often the case that B should contain a
private instance of A and expose a different API: A is not an essential part of
B, merely a detail of its implementation.
There are a number of obvious violations of this principle in the Java
platform libraries. For example, a stack is not a vector, so Stack should not
extend Vector. Similarly, a property list is not a hash table, so
116
Properties should not extend Hashtable. In both cases, composition
would have been preferable.
If you use inheritance where composition is appropriate, you needlessly
expose implementation details. The resulting API ties you to the original
implementation, forever limiting the performance of your class. More
seriously, by exposing the internals you let clients access them directly. At the
very least, it can lead to confusing semantics. For example, if p refers to a
Properties instance, then p.getProperty(key) may yield different
results from p.get(key): the former method takes defaults into account,
while the latter method, which is inherited from Hashtable, does not. Most
seriously, the client may be able to corrupt invariants of the subclass by
modifying the superclass directly. In the case of Properties, the designers
intended that only strings be allowed as keys and values, but direct access to
the underlying Hashtable allows this invariant to be violated. Once
violated, it is no longer possible to use other parts of the Properties API
(load and store). By the time this problem was discovered, it was too late
to correct it because clients depended on the use of non-string keys and
values.
There is one last set of questions you should ask yourself before deciding to
use inheritance in place of composition. Does the class that you contemplate
extending have any flaws in its API? If so, are you comfortable propagating
those flaws into your class’s API? Inheritance propagates any flaws in the
superclass’s API, while composition lets you design a new API that hides
these flaws.
To summarize, inheritance is powerful, but it is problematic because it
violates encapsulation. It is appropriate only when a genuine subtype
relationship exists between the subclass and the superclass. Even then,
inheritance may lead to fragility if the subclass is in a different package from
the superclass and the superclass is not designed for inheritance. To avoid this
fragility, use composition and forwarding instead of inheritance, especially if
an appropriate interface to implement a wrapper class exists. Not only are
wrapper classes more robust than subclasses, they are also more powerful.
Item 19: Design and document for inheritance or else
prohibit it
Item 18 alerted you to the dangers of subclassing a “foreign” class that was
not designed and documented for inheritance. So what does it mean for a
class to be designed and documented for inheritance?
First, the class must document precisely the effects of overriding any
method. In other words, the class must document its self-use of overridable
117
methods. For each public or protected method, the documentation must
indicate which overridable methods the method invokes, in what sequence,
and how the results of each invocation affect subsequent processing. (By
overridable, we mean nonfinal and either public or protected.) More
generally, a class must document any circumstances under which it might
invoke an overridable method. For example, invocations might come from
background threads or static initializers.
A method that invokes overridable methods contains a description of these
invocations at the end of its documentation comment. The description is in a
special section of the specification, labeled “Implementation Requirements,”
which is generated by the Javadoc tag @implSpec. This section describes
the inner workings of the method. Here’s an example, copied from the
specification for java.util.AbstractCollection:
Click here to view code image
public boolean remove(Object o)
Removes a single instance of the specified element from this collection,
if it is present (optional operation). More formally, removes an element e
such that Objects.equals(o, e), if this collection contains one or
more such elements. Returns true if this collection contained the
specified element (or equivalently, if this collection changed as a result
of the call).
Implementation Requirements: This implementation iterates over the
collection looking for the specified element. If it finds the element, it
removes the element from the collection using the iterator’s remove
method. Note that this implementation throws an
UnsupportedOperationException if the iterator returned by this
collection’s iterator method does not implement the remove
method and this collection contains the specified object.
This documentation leaves no doubt that overriding the iterator
method will affect the behavior of the remove method. It also describes
exactly how the behavior of the Iterator returned by the iterator
method will affect the behavior of the remove method. Contrast this to the
situation in Item 18, where the programmer subclassing HashSet simply
could not say whether overriding the add method would affect the behavior
of the addAll method.
But doesn’t this violate the dictum that good API documentation should
describe what a given method does and not how it does it? Yes, it does! This
is an unfortunate consequence of the fact that inheritance violates
encapsulation. To document a class so that it can be safely subclassed, you
118
must describe implementation details that should otherwise be left
unspecified.
The @implSpec tag was added in Java 8 and used heavily in Java 9. This
tag should be enabled by default, but as of Java 9, the Javadoc utility still
ignores it unless you pass the command line switch -tag
"apiNote:a:API Note:".
Designing for inheritance involves more than just documenting patterns of
self-use. To allow programmers to write efficient subclasses without undue
pain, a class may have to provide hooks into its internal workings in the
form of judiciously chosen protected methods or, in rare instances,
protected fields. For example, consider the removeRange method from
java.util.AbstractList:
Click here to view code image
protected void removeRange(int fromIndex, int toIndex)
Removes from this list all of the elements whose index is between
fromIndex, inclusive, and toIndex, exclusive. Shifts any succeeding
elements to the left (reduces their index). This call shortens the list by
(toIndex - fromIndex) elements. (If toIndex ==
fromIndex, this operation has no effect.)
This method is called by the clear operation on this list and its sublists.
Overriding this method to take advantage of the internals of the list
implementation can substantially improve the performance of the clear
operation on this list and its sublists.
Implementation Requirements: This implementation gets a list iterator
positioned before fromIndex and repeatedly calls
ListIterator.next followed by ListIterator.remove, until
the entire range has been removed. Note: If ListIterator.remove
requires linear time, this implementation requires quadratic time.
Parameters:
fromIndex index of first element to be removed.
toIndex index after last element to be removed.
This method is of no interest to end users of a List implementation. It is
provided solely to make it easy for subclasses to provide a fast clear
method on sublists. In the absence of the removeRange method, subclasses
would have to make do with quadratic performance when the clear method
was invoked on sublists or rewrite the entire subList mechanism from
scratch—not an easy task!
119
So how do you decide what protected members to expose when you design
a class for inheritance? Unfortunately, there is no magic bullet. The best you
can do is to think hard, take your best guess, and then test it by writing
subclasses. You should expose as few protected members as possible because
each one represents a commitment to an implementation detail. On the other
hand, you must not expose too few because a missing protected member can
render a class practically unusable for inheritance.
The only way to test a class designed for inheritance is to write
subclasses. If you omit a crucial protected member, trying to write a subclass
will make the omission painfully obvious. Conversely, if several subclasses
are written and none uses a protected member, you should probably make it
private. Experience shows that three subclasses are usually sufficient to test
an extendable class. One or more of these subclasses should be written by
someone other than the superclass author.
When you design for inheritance a class that is likely to achieve wide use,
realize that you are committing forever to the self-use patterns that you
document and to the implementation decisions implicit in its protected
methods and fields. These commitments can make it difficult or impossible to
improve the performance or functionality of the class in a subsequent release.
Therefore, you must test your class by writing subclasses before you
release it.
Also, note that the special documentation required for inheritance clutters
up normal documentation, which is designed for programmers who create
instances of your class and invoke methods on them. As of this writing, there
is little in the way of tools to separate ordinary API documentation from
information of interest only to programmers implementing subclasses.
There are a few more restrictions that a class must obey to allow
inheritance. Constructors must not invoke overridable methods, directly or
indirectly. If you violate this rule, program failure will result. The superclass
constructor runs before the subclass constructor, so the overriding method in
the subclass will get invoked before the subclass constructor has run. If the
overriding method depends on any initialization performed by the subclass
constructor, the method will not behave as expected. To make this concrete,
here’s a class that violates this rule:
Click here to view code image
public class Super {
// Broken - constructor invokes an overridable
method
public Super() {
overrideMe();
}
public void overrideMe() {
120
}
}
Here’s a subclass that overrides the overrideMe method, which is
erroneously invoked by Super’s sole constructor:
Click here to view code image
public final class Sub extends Super {
// Blank final, set by constructor
private final Instant instant;
Sub() {
instant = Instant.now();
}
// Overriding method invoked by superclass
constructor
@Override public void overrideMe() {
System.out.println(instant);
}
public static void main(String[] args) {
Sub sub = new Sub();
sub.overrideMe();
}
}
You might expect this program to print out the instant twice, but it prints out
null the first time because overrideMe is invoked by the Super
constructor before the Sub constructor has a chance to initialize the
instant field. Note that this program observes a final field in two different
states! Note also that if overrideMe had invoked any method on
instant, it would have thrown a NullPointerException when the
Super constructor invoked overrideMe. The only reason this program
doesn’t throw a NullPointerException as it stands is that the
println method tolerates null parameters.
Note that it is safe to invoke private methods, final methods, and static
methods, none of which are overridable, from a constructor.
The Cloneable and Serializable interfaces present special
difficulties when designing for inheritance. It is generally not a good idea for
a class designed for inheritance to implement either of these interfaces
because they place a substantial burden on programmers who extend the
class. There are, however, special actions that you can take to allow
subclasses to implement these interfaces without mandating that they do so.
These actions are described in Item 13 and Item 86.
121
If you do decide to implement either Cloneable or Serializable in
a class that is designed for inheritance, you should be aware that because the
clone and readObject methods behave a lot like constructors, a similar
restriction applies: neither clone nor readObject may invoke an
overridable method, directly or indirectly. In the case of readObject,
the overriding method will run before the subclass’s state has been
deserialized. In the case of clone, the overriding method will run before the
subclass’s clone method has a chance to fix the clone’s state. In either case,
a program failure is likely to follow. In the case of clone, the failure can
damage the original object as well as the clone. This can happen, for example,
if the overriding method assumes it is modifying the clone’s copy of the
object’s deep structure, but the copy hasn’t been made yet.
Finally, if you decide to implement Serializable in a class designed
for inheritance and the class has a readResolve or writeReplace
method, you must make the readResolve or writeReplace method
protected rather than private. If these methods are private, they will be silently
ignored by subclasses. This is one more case where an implementation detail
becomes part of a class’s API to permit inheritance.
By now it should be apparent that designing a class for inheritance
requires great effort and places substantial limitations on the class. This
is not a decision to be undertaken lightly. There are some situations where it
is clearly the right thing to do, such as abstract classes, including skeletal
implementations of interfaces (Item 20). There are other situations where it is
clearly the wrong thing to do, such as immutable classes (Item 17).
But what about ordinary concrete classes? Traditionally, they are neither
final nor designed and documented for subclassing, but this state of affairs is
dangerous. Each time a change is made in such a class, there is a chance that
subclasses extending the class will break. This is not just a theoretical
problem. It is not uncommon to receive subclassing-related bug reports after
modifying the internals of a nonfinal concrete class that was not designed and
documented for inheritance.
The best solution to this problem is to prohibit subclassing in classes
that are not designed and documented to be safely subclassed. There are
two ways to prohibit subclassing. The easier of the two is to declare the class
final. The alternative is to make all the constructors private or package-private
and to add public static factories in place of the constructors. This alternative,
which provides the flexibility to use subclasses internally, is discussed in Item
17. Either approach is acceptable.
This advice may be somewhat controversial because many programmers
have grown accustomed to subclassing ordinary concrete classes to add
facilities such as instrumentation, notification, and synchronization or to limit
122
functionality. If a class implements some interface that captures its essence,
such as Set, List, or Map, then you should feel no compunction about
prohibiting subclassing. The wrapper class pattern, described in Item 18,
provides a superior alternative to inheritance for augmenting the functionality.
If a concrete class does not implement a standard interface, then you may
inconvenience some programmers by prohibiting inheritance. If you feel that
you must allow inheritance from such a class, one reasonable approach is to
ensure that the class never invokes any of its overridable methods and to
document this fact. In other words, eliminate the class’s self-use of
overridable methods entirely. In doing so, you’ll create a class that is
reasonably safe to subclass. Overriding a method will never affect the
behavior of any other method.
You can eliminate a class’s self-use of overridable methods mechanically,
without changing its behavior. Move the body of each overridable method to
a private “helper method” and have each overridable method invoke its
private helper method. Then replace each self-use of an overridable method
with a direct invocation of the overridable method’s private helper method.
In summary, designing a class for inheritance is hard work. You must
document all of its self-use patterns, and once you’ve documented them, you
must commit to them for the life of the class. If you fail to do this, subclasses
may become dependent on implementation details of the superclass and may
break if the implementation of the superclass changes. To allow others to
write efficient subclasses, you may also have to export one or more protected
methods. Unless you know there is a real need for subclasses, you are
probably better off prohibiting inheritance by declaring your class final or
ensuring that there are no accessible constructors.
Item 20: Prefer interfaces to abstract classes
Java has two mechanisms to define a type that permits multiple
implementations: interfaces and abstract classes. Since the introduction of
default methods for interfaces in Java 8 [JLS 9.4.3], both mechanisms allow
you to provide implementations for some instance methods. A major
difference is that to implement the type defined by an abstract class, a class
must be a subclass of the abstract class. Because Java permits only single
inheritance, this restriction on abstract classes severely constrains their use as
type definitions. Any class that defines all the required methods and obeys the
general contract is permitted to implement an interface, regardless of where
the class resides in the class hierarchy.
Existing classes can easily be retrofitted to implement a new interface.
All you have to do is to add the required methods, if they don’t yet exist, and
to add an implements clause to the class declaration. For example, many
123
existing classes were retrofitted to implement the Comparable,
Iterable, and Autocloseable interfaces when they were added to the
platform. Existing classes cannot, in general, be retrofitted to extend a new
abstract class. If you want to have two classes extend the same abstract class,
you have to place it high up in the type hierarchy where it is an ancestor of
both classes. Unfortunately, this can cause great collateral damage to the type
hierarchy, forcing all descendants of the new abstract class to subclass it,
whether or not it is appropriate.
Interfaces are ideal for defining mixins. Loosely speaking, a mixin is a
type that a class can implement in addition to its “primary type,” to declare
that it provides some optional behavior. For example, Comparable is a
mixin interface that allows a class to declare that its instances are ordered
with respect to other mutually comparable objects. Such an interface is called
a mixin because it allows the optional functionality to be “mixed in” to the
type’s primary functionality. Abstract classes can’t be used to define mixins
for the same reason that they can’t be retrofitted onto existing classes: a class
cannot have more than one parent, and there is no reasonable place in the
class hierarchy to insert a mixin.
Interfaces allow for the construction of nonhierarchical type
frameworks. Type hierarchies are great for organizing some things, but other
things don’t fall neatly into a rigid hierarchy. For example, suppose we have
an interface representing a singer and another representing a songwriter:
Click here to view code image
public interface Singer {
AudioClip sing(Song s);
}
public interface Songwriter {
Song compose(int chartPosition);
}
In real life, some singers are also songwriters. Because we used interfaces
rather than abstract classes to define these types, it is perfectly permissible for
a single class to implement both Singer and Songwriter. In fact, we can
define a third interface that extends both Singer and Songwriter and
adds new methods that are appropriate to the combination:
Click here to view code image
public interface SingerSongwriter extends Singer,
Songwriter {
AudioClip strum();
void actSensitive();
}
124
You don’t always need this level of flexibility, but when you do, interfaces
are a lifesaver. The alternative is a bloated class hierarchy containing a
separate class for every supported combination of attributes. If there are n
attributes in the type system, there are 2
n possible combinations that you
might have to support. This is what’s known as a combinatorial explosion.
Bloated class hierarchies can lead to bloated classes with many methods that
differ only in the type of their arguments because there are no types in the
class hierarchy to capture common behaviors.
Interfaces enable safe, powerful functionality enhancements via the
wrapper class idiom (Item 18). If you use abstract classes to define types, you
leave the programmer who wants to add functionality with no alternative but
inheritance. The resulting classes are less powerful and more fragile than
wrapper classes.
When there is an obvious implementation of an interface method in terms
of other interface methods, consider providing implementation assistance to
programmers in the form of a default method. For an example of this
technique, see the removeIf method on page 104. If you provide default
methods, be sure to document them for inheritance using the @implSpec
Javadoc tag (Item 19).
There are limits on how much implementation assistance you can provide
with default methods. Although many interfaces specify the behavior of
Object methods such as equals and hashCode, you are not permitted to
provide default methods for them. Also, interfaces are not permitted to
contain instance fields or nonpublic static members (with the exception of
private static methods). Finally, you can’t add default methods to an interface
that you don’t control.
You can, however, combine the advantages of interfaces and abstract
classes by providing an abstract skeletal implementation class to go with an
interface. The interface defines the type, perhaps providing some default
methods, while the skeletal implementation class implements the remaining
non-primitive interface methods atop the primitive interface methods.
Extending a skeletal implementation takes most of the work out of
implementing an interface. This is the Template Method pattern [Gamma95].
By convention, skeletal implementation classes are called
AbstractInterface, where Interface is the name of the interface they
implement. For example, the Collections Framework provides a skeletal
implementation to go along with each main collection interface:
AbstractCollection, AbstractSet, AbstractList, and
AbstractMap. Arguably it would have made sense to call them
SkeletalCollection, SkeletalSet, SkeletalList, and
SkeletalMap, but the Abstract convention is now firmly established.
125
When properly designed, skeletal implementations (whether a separate
abstract class, or consisting solely of default methods on an interface) can
make it very easy for programmers to provide their own implementations of
an interface. For example, here’s a static factory method containing a
complete, fully functional List implementation atop AbstractList:
Click here to view code image
// Concrete implementation built atop skeletal
implementation
static List<Integer> intArrayAsList(int[] a) {
Objects.requireNonNull(a);
// The diamond operator is only legal here in Java 9
and later
// If you're using an earlier release, specify
<Integer>
return new AbstractList<>() {
@Override public Integer get(int i) {
return a[i]; // Autoboxing (Item 6)
}
@Override public Integer set(int i, Integer val)
{
int oldVal = a[i];
a[i] = val; // Auto-unboxing
return oldVal; // Autoboxing
}
@Override public int size() {
return a.length;
}
};
}
When you consider all that a List implementation does for you, this
example is an impressive demonstration of the power of skeletal
implementations. Incidentally, this example is an Adapter [Gamma95] that
allows an int array to be viewed as a list of Integer instances. Because of
all the translation back and forth between int values and Integer
instances (boxing and unboxing), its performance is not terribly good. Note
that the implementation takes the form of an anonymous class (Item 24).
The beauty of skeletal implementation classes is that they provide all of the
implementation assistance of abstract classes without imposing the severe
constraints that abstract classes impose when they serve as type definitions.
For most implementors of an interface with a skeletal implementation class,
extending this class is the obvious choice, but it is strictly optional. If a class
126
cannot be made to extend the skeletal implementation, the class can always
implement the interface directly. The class still benefits from any default
methods present on the interface itself. Furthermore, the skeletal
implementation can still aid the implementor’s task. The class implementing
the interface can forward invocations of interface methods to a contained
instance of a private inner class that extends the skeletal implementation. This
technique, known as simulated multiple inheritance, is closely related to the
wrapper class idiom discussed in Item 18. It provides many of the benefits of
multiple inheritance, while avoiding the pitfalls.
Writing a skeletal implementation is a relatively simple, if somewhat
tedious, process. First, study the interface and decide which methods are the
primitives in terms of which the others can be implemented. These primitives
will be the abstract methods in your skeletal implementation. Next, provide
default methods in the interface for all of the methods that can be
implemented directly atop the primitives, but recall that you may not provide
default methods for Object methods such as equals and hashCode. If
the primitives and default methods cover the interface, you’re done, and have
no need for a skeletal implementation class. Otherwise, write a class declared
to implement the interface, with implementations of all of the remaining
interface methods. The class may contain any nonpublic fields ands methods
appropriate to the task.
As a simple example, consider the Map.Entry interface. The obvious
primitives are getKey, getValue, and (optionally) setValue. The
interface specifies the behavior of equals and hashCode, and there is an
obvious implementation of toString in terms of the primitives. Since you
are not allowed to provide default implementations for the Object methods,
all implementations are placed in the skeletal implementation class:
Click here to view code image
// Skeletal implementation class
public abstract class AbstractMapEntry<K,V>
implements Map.Entry<K,V> {
// Entries in a modifiable map must override this
method
@Override public V setValue(V value) {
throw new UnsupportedOperationException();
}
// Implements the general contract of
Map.Entry.equals
@Override public boolean equals(Object o) {
if (o == this)
return true;
if (!(o instanceof Map.Entry))
return false;
127
Map.Entry<?,?> e = (Map.Entry) o;
return Objects.equals(e.getKey(), getKey())
&& Objects.equals(e.getValue(), getValue());
}
// Implements the general contract of
Map.Entry.hashCode
@Override public int hashCode() {
return Objects.hashCode(getKey())
^ Objects.hashCode(getValue());
}
@Override public String toString() {
return getKey() + "=" + getValue();
}
}
Note that this skeletal implementation could not be implemented in the
Map.Entry interface or as a subinterface because default methods are not
permitted to override Object methods such as equals, hashCode, and
toString.
Because skeletal implementations are designed for inheritance, you should
follow all of the design and documentation guidelines in Item 19. For
brevity’s sake, the documentation comments were omitted from the previous
example, but good documentation is absolutely essential in a skeletal
implementation, whether it consists of default methods on an interface or a
separate abstract class.
A minor variant on the skeletal implementation is the simple
implementation, exemplified by AbstractMap.SimpleEntry. A simple
implementation is like a skeletal implementation in that it implements an
interface and is designed for inheritance, but it differs in that it isn’t abstract:
it is the simplest possible working implementation. You can use it as it stands
or subclass it as circumstances warrant.
To summarize, an interface is generally the best way to define a type that
permits multiple implementations. If you export a nontrivial interface, you
should strongly consider providing a skeletal implementation to go with it. To
the extent possible, you should provide the skeletal implementation via
default methods on the interface so that all implementors of the interface can
make use of it. That said, restrictions on interfaces typically mandate that a
skeletal implementation take the form of an abstract class.
Item 21: Design interfaces for posterity
Prior to Java 8, it was impossible to add methods to interfaces without
128
breaking existing implementations. If you added a new method to an
interface, existing implementations would, in general, lack the method,
resulting in a compile-time error. In Java 8, the default method construct was
added [JLS 9.4], with the intent of allowing the addition of methods to
existing interfaces. But adding new methods to existing interfaces is fraught
with risk.
The declaration for a default method includes a default implementation that
is used by all classes that implement the interface but do not implement the
default method. While the addition of default methods to Java makes it
possible to add methods to an existing interface, there is no guarantee that
these methods will work in all preexisting implementations. Default methods
are “injected” into existing implementations without the knowledge or
consent of their implementors. Before Java 8, these implementations were
written with the tacit understanding that their interfaces would never acquire
any new methods.
Many new default methods were added to the core collection interfaces in
Java 8, primarily to facilitate the use of lambdas (Chapter 6). The Java
libraries’ default methods are high-quality general-purpose implementations,
and in most cases, they work fine. But it is not always possible to write a
default method that maintains all invariants of every conceivable
implementation.
For example, consider the removeIf method, which was added to the
Collection interface in Java 8. This method removes all elements for
which a given boolean function (or predicate) returns true. The default
implementation is specified to traverse the collection using its iterator,
invoking the predicate on each element, and using the iterator’s remove
method to remove the elements for which the predicate returns true.
Presumably the declaration looks something like this:
Click here to view code image
// Default method added to the Collection interface in
Java 8
default boolean removeIf(Predicate<? super E> filter) {
Objects.requireNonNull(filter);
boolean result = false;
for (Iterator<E> it = iterator(); it.hasNext(); ) {
if (filter.test(it.next())) {
it.remove();
result = true;
}
}
return result;
}
129
This is the best general-purpose implementation one could possibly write
for the removeIf method, but sadly, it fails on some real-world
Collection implementations. For example, consider
org.apache.commons.collections4.-
collection.SynchronizedCollection. This class, from the
Apache Commons library, is similar to the one returned by the static factory
Collections.-synchronizedCollection in java.util. The
Apache version additionally provides the ability to use a client-supplied
object for locking, in place of the collection. In other words, it is a wrapper
class (Item 18), all of whose methods synchronize on a locking object before
delegating to the wrapped collection.
The Apache SynchronizedCollection class is still being actively
maintained, but as of this writing, it does not override the removeIf
method. If this class is used in conjunction with Java 8, it will therefore
inherit the default implementation of removeIf, which does not, indeed
cannot, maintain the class’s fundamental promise: to automatically
synchronize around each method invocation. The default implementation
knows nothing about synchronization and has no access to the field that
contains the locking object. If a client calls the removeIf method on a
SynchronizedCollection instance in the presence of concurrent
modification of the collection by another thread, a
ConcurrentModificationException or other unspecified behavior
may result.
In order to prevent this from happening in similar Java platform libraries
implementations, such as the package-private class returned by
Collections.synchronizedCollection, the JDK maintainers had
to override the default removeIf implementation and other methods like it
to perform the necessary synchronization before invoking the default
implementation. Preexisting collection implementations that were not part of
the Java platform did not have the opportunity to make analogous changes in
lockstep with the interface change, and some have yet to do so.
In the presence of default methods, existing implementations of an
interface may compile without error or warning but fail at runtime.
While not terribly common, this problem is not an isolated incident either. A
handful of the methods added to the collections interfaces in Java 8 are known
to be susceptible, and a handful of existing implementations are known to be
affected.
Using default methods to add new methods to existing interfaces should be
avoided unless the need is critical, in which case you should think long and
hard about whether an existing interface implementation might be broken by
your default method implementation. Default methods are, however,
130
extremely useful for providing standard method implementations when an
interface is created, to ease the task of implementing the interface (Item 20).
It is also worth noting that default methods were not designed to support
removing methods from interfaces or changing the signatures of existing
methods. Neither of these interface changes is possible without breaking
existing clients.
The moral is clear. Even though default methods are now a part of the Java
platform, it is still of the utmost importance to design interfaces with great
care. While default methods make it possible to add methods to existing
interfaces, there is great risk in doing so. If an interface contains a minor flaw,
it may irritate its users forever; if an interface is severely deficient, it may
doom the API that contains it.
Therefore, it is critically important to test each new interface before you
release it. Multiple programmers should implement each interface in different
ways. At a minimum, you should aim for three diverse implementations.
Equally important is to write multiple client programs that use instances of
each new interface to perform various tasks. This will go a long way toward
ensuring that each interface satisfies all of its intended uses. These steps will
allow you to discover flaws in interfaces before they are released, when you
can still correct them easily. While it may be possible to correct some
interface flaws after an interface is released, you cannot count on it.
Item 22: Use interfaces only to define types
When a class implements an interface, the interface serves as a type that can
be used to refer to instances of the class. That a class implements an interface
should therefore say something about what a client can do with instances of
the class. It is inappropriate to define an interface for any other purpose.
One kind of interface that fails this test is the so-called constant interface.
Such an interface contains no methods; it consists solely of static final fields,
each exporting a constant. Classes using these constants implement the
interface to avoid the need to qualify constant names with a class name. Here
is an example:
Click here to view code image
// Constant interface antipattern - do not use!
public interface PhysicalConstants {
// Avogadro's number (1/mol)
static final double AVOGADROS_NUMBER =
6.022_140_857e23;
// Boltzmann constant (J/K)
static final double BOLTZMANN_CONSTANT =
131
1.380_648_52e-23;
// Mass of the electron (kg)
static final double ELECTRON_MASS =
9.109_383_56e-31;
}
The constant interface pattern is a poor use of interfaces. That a class
uses some constants internally is an implementation detail. Implementing a
constant interface causes this implementation detail to leak into the class’s
exported API. It is of no consequence to the users of a class that the class
implements a constant interface. In fact, it may even confuse them. Worse, it
represents a commitment: if in a future release the class is modified so that it
no longer needs to use the constants, it still must implement the interface to
ensure binary compatibility. If a nonfinal class implements a constant
interface, all of its subclasses will have their namespaces polluted by the
constants in the interface.
There are several constant interfaces in the Java platform libraries, such as
java.io.ObjectStreamConstants. These interfaces should be
regarded as anomalies and should not be emulated.
If you want to export constants, there are several reasonable choices. If the
constants are strongly tied to an existing class or interface, you should add
them to the class or interface. For example, all of the boxed numerical
primitive classes, such as Integer and Double, export MIN_VALUE and
MAX_VALUE constants. If the constants are best viewed as members of an
enumerated type, you should export them with an enum type (Item 34).
Otherwise, you should export the constants with a noninstantiable utility class
(Item 4). Here is a utility class version of the PhysicalConstants
example shown earlier:
Click here to view code image
// Constant utility class
package com.effectivejava.science;
public class PhysicalConstants {
private PhysicalConstants() { } // Prevents
instantiation
public static final double AVOGADROS_NUMBER =
6.022_140_857e23;
public static final double BOLTZMANN_CONST =
1.380_648_52e-23;
public static final double ELECTRON_MASS =
9.109_383_56e-31;
}
132
Incidentally, note the use of the underscore character (_) in the numeric
literals. Underscores, which have been legal since Java 7, have no effect on
the values of numeric literals, but can make them much easier to read if used
with discretion. Consider adding underscores to numeric literals, whether
fixed of floating point, if they contain five or more consecutive digits. For
base ten literals, whether integral or floating point, you should use
underscores to separate literals into groups of three digits indicating positive
and negative powers of one thousand.
Normally a utility class requires clients to qualify constant names with a
class name, for example, PhysicalConstants.AVOGADROS_NUMBER.
If you make heavy use of the constants exported by a utility class, you can
avoid the need for qualifying the constants with the class name by making use
of the static import facility:
Click here to view code image
// Use of static import to avoid qualifying constants
import static
com.effectivejava.science.PhysicalConstants.*;
public class Test {
double atoms(double mols) {
return AVOGADROS_NUMBER * mols;
}
...
// Many more uses of PhysicalConstants justify
static import
}
In summary, interfaces should be used only to define types. They should
not be used merely to export constants.
Item 23: Prefer class hierarchies to tagged classes
Occasionally you may run across a class whose instances come in two or
more flavors and contain a tag field indicating the flavor of the instance. For
example, consider this class, which is capable of representing a circle or a
rectangle:
Click here to view code image
// Tagged class - vastly inferior to a class hierarchy!
class Figure {
enum Shape { RECTANGLE, CIRCLE };
// Tag field - the shape of this figure
final Shape shape;
133
// These fields are used only if shape is RECTANGLE
double length;
double width;
// This field is used only if shape is CIRCLE
double radius;
// Constructor for circle
Figure(double radius) {
shape = Shape.CIRCLE;
this.radius = radius;
}
// Constructor for rectangle
Figure(double length, double width) {
shape = Shape.RECTANGLE;
this.length = length;
this.width = width;
}
double area() {
switch(shape) {
case RECTANGLE:
return length * width;
case CIRCLE:
return Math.PI * (radius * radius);
default:
throw new AssertionError(shape);
}
}
}
Such tagged classes have numerous shortcomings. They are cluttered with
boilerplate, including enum declarations, tag fields, and switch statements.
Readability is further harmed because multiple implementations are jumbled
together in a single class. Memory footprint is increased because instances are
burdened with irrelevant fields belonging to other flavors. Fields can’t be
made final unless constructors initialize irrelevant fields, resulting in more
boilerplate. Constructors must set the tag field and initialize the right data
fields with no help from the compiler: if you initialize the wrong fields, the
program will fail at runtime. You can’t add a flavor to a tagged class unless
you can modify its source file. If you do add a flavor, you must remember to
add a case to every switch statement, or the class will fail at runtime. Finally,
the data type of an instance gives no clue as to its flavor. In short, tagged
classes are verbose, error-prone, and inefficient.
134
Luckily, object-oriented languages such as Java offer a far better alternative
for defining a single data type capable of representing objects of multiple
flavors: subtyping. A tagged class is just a pallid imitation of a class
hierarchy.
To transform a tagged class into a class hierarchy, first define an abstract
class containing an abstract method for each method in the tagged class
whose behavior depends on the tag value. In the Figure class, there is only
one such method, which is area. This abstract class is the root of the class
hierarchy. If there are any methods whose behavior does not depend on the
value of the tag, put them in this class. Similarly, if there are any data fields
used by all the flavors, put them in this class. There are no such flavorindependent methods or fields in the Figure class.
Next, define a concrete subclass of the root class for each flavor of the
original tagged class. In our example, there are two: circle and rectangle.
Include in each subclass the data fields particular to its flavor. In our example,
radius is particular to circle, and length and width are particular to
rectangle. Also include in each subclass the appropriate implementation of
each abstract method in the root class. Here is the class hierarchy
corresponding to the original Figure class:
Click here to view code image
// Class hierarchy replacement for a tagged class
abstract class Figure {
abstract double area();
}
class Circle extends Figure {
final double radius;
Circle(double radius) { this.radius = radius; }
@Override double area() { return Math.PI * (radius *
radius); }
}
class Rectangle extends Figure {
final double length;
final double width;
Rectangle(double length, double width) {
this.length = length;
this.width = width;
}
@Override double area() { return length * width; }
}
135
This class hierarchy corrects every shortcoming of tagged classes noted
previously. The code is simple and clear, containing none of the boilerplate
found in the original. The implementation of each flavor is allotted its own
class, and none of these classes is encumbered by irrelevant data fields. All
fields are final. The compiler ensures that each class’s constructor initializes
its data fields and that each class has an implementation for every abstract
method declared in the root class. This eliminates the possibility of a runtime
failure due to a missing switch case. Multiple programmers can extend the
hierarchy independently and interoperably without access to the source for the
root class. There is a separate data type associated with each flavor, allowing
programmers to indicate the flavor of a variable and to restrict variables and
input parameters to a particular flavor.
Another advantage of class hierarchies is that they can be made to reflect
natural hierarchical relationships among types, allowing for increased
flexibility and better compile-time type checking. Suppose the tagged class in
the original example also allowed for squares. The class hierarchy could be
made to reflect the fact that a square is a special kind of rectangle (assuming
both are immutable):
Click here to view code image
class Square extends Rectangle {
Square(double side) {
super(side, side);
}
}
Note that the fields in the above hierarchy are accessed directly rather than
by accessor methods. This was done for brevity and would be a poor design if
the hierarchy were public (Item 16).
In summary, tagged classes are seldom appropriate. If you’re tempted to
write a class with an explicit tag field, think about whether the tag could be
eliminated and the class replaced by a hierarchy. When you encounter an
existing class with a tag field, consider refactoring it into a hierarchy.
Item 24: Favor static member classes over nonstatic
A nested class is a class defined within another class. A nested class should
exist only to serve its enclosing class. If a nested class would be useful in
some other context, then it should be a top-level class. There are four kinds of
nested classes: static member classes, nonstatic member classes, anonymous
classes, and local classes. All but the first kind are known as inner classes.
This item tells you when to use which kind of nested class and why.
A static member class is the simplest kind of nested class. It is best thought
136
of as an ordinary class that happens to be declared inside another class and
has access to all of the enclosing class’s members, even those declared
private. A static member class is a static member of its enclosing class and
obeys the same accessibility rules as other static members. If it is declared
private, it is accessible only within the enclosing class, and so forth.
One common use of a static member class is as a public helper class, useful
only in conjunction with its outer class. For example, consider an enum
describing the operations supported by a calculator (Item 34). The
Operation enum should be a public static member class of the
Calculator class. Clients of Calculator could then refer to operations
using names like Calculator.Operation.PLUS and
Calculator.Operation.MINUS.
Syntactically, the only difference between static and nonstatic member
classes is that static member classes have the modifier static in their
declarations. Despite the syntactic similarity, these two kinds of nested
classes are very different. Each instance of a nonstatic member class is
implicitly associated with an enclosing instance of its containing class. Within
instance methods of a nonstatic member class, you can invoke methods on the
enclosing instance or obtain a reference to the enclosing instance using the
qualified this construct [JLS, 15.8.4]. If an instance of a nested class can exist
in isolation from an instance of its enclosing class, then the nested class must
be a static member class: it is impossible to create an instance of a nonstatic
member class without an enclosing instance.
The association between a nonstatic member class instance and its
enclosing instance is established when the member class instance is created
and cannot be modified thereafter. Normally, the association is established
automatically by invoking a nonstatic member class constructor from within
an instance method of the enclosing class. It is possible, though rare, to
establish the association manually using the expression
enclosingInstance.new MemberClass(args). As you would
expect, the association takes up space in the nonstatic member class instance
and adds time to its construction.
One common use of a nonstatic member class is to define an Adapter
[Gamma95] that allows an instance of the outer class to be viewed as an
instance of some unrelated class. For example, implementations of the Map
interface typically use nonstatic member classes to implement their collection
views, which are returned by Map’s keySet, entrySet, and values
methods. Similarly, implementations of the collection interfaces, such as Set
and List, typically use nonstatic member classes to implement their
iterators:
Click here to view code image
137
// Typical use of a nonstatic member class
public class MySet<E> extends AbstractSet<E> {
... // Bulk of the class omitted
@Override public Iterator<E> iterator() {
return new MyIterator();
}
private class MyIterator implements Iterator<E> {
...
}
}
If you declare a member class that does not require access to an
enclosing instance, always put the static modifier in its declaration,
making it a static rather than a nonstatic member class. If you omit this
modifier, each instance will have a hidden extraneous reference to its
enclosing instance. As previously mentioned, storing this reference takes time
and space. More seriously, it can result in the enclosing instance being
retained when it would otherwise be eligible for garbage collection (Item 7).
The resulting memory leak can be catastrophic. It is often difficult to detect
because the reference is invisible.
A common use of private static member classes is to represent components
of the object represented by their enclosing class. For example, consider a
Map instance, which associates keys with values. Many Map implementations
have an internal Entry object for each key-value pair in the map. While each
entry is associated with a map, the methods on an entry (getKey,
getValue, and setValue) do not need access to the map. Therefore, it
would be wasteful to use a nonstatic member class to represent entries: a
private static member class is best. If you accidentally omit the static
modifier in the entry declaration, the map will still work, but each entry will
contain a superfluous reference to the map, which wastes space and time.
It is doubly important to choose correctly between a static and a nonstatic
member class if the class in question is a public or protected member of an
exported class. In this case, the member class is an exported API element and
cannot be changed from a nonstatic to a static member class in a subsequent
release without violating backward compatibility.
As you would expect, an anonymous class has no name. It is not a member
of its enclosing class. Rather than being declared along with other members, it
is simultaneously declared and instantiated at the point of use. Anonymous
classes are permitted at any point in the code where an expression is legal.
Anonymous classes have enclosing instances if and only if they occur in a
nonstatic context. But even if they occur in a static context, they cannot have
138
any static members other than constant variables, which are final primitive or
string fields initialized to constant expressions [JLS, 4.12.4].
There are many limitations on the applicability of anonymous classes. You
can’t instantiate them except at the point they’re declared. You can’t perform
instanceof tests or do anything else that requires you to name the class.
You can’t declare an anonymous class to implement multiple interfaces or to
extend a class and implement an interface at the same time. Clients of an
anonymous class can’t invoke any members except those it inherits from its
supertype. Because anonymous classes occur in the midst of expressions, they
must be kept short—about ten lines or fewer—or readability will suffer.
Before lambdas were added to Java (Chapter 6), anonymous classes were
the preferred means of creating small function objects and process objects on
the fly, but lambdas are now preferred (Item 42). Another common use of
anonymous classes is in the implementation of static factory methods (see
intArrayAsList in Item 20).
Local classes are the least frequently used of the four kinds of nested
classes. A local class can be declared practically anywhere a local variable
can be declared and obeys the same scoping rules. Local classes have
attributes in common with each of the other kinds of nested classes. Like
member classes, they have names and can be used repeatedly. Like
anonymous classes, they have enclosing instances only if they are defined in a
nonstatic context, and they cannot contain static members. And like
anonymous classes, they should be kept short so as not to harm readability.
To recap, there are four different kinds of nested classes, and each has its
place. If a nested class needs to be visible outside of a single method or is too
long to fit comfortably inside a method, use a member class. If each instance
of a member class needs a reference to its enclosing instance, make it
nonstatic; otherwise, make it static. Assuming the class belongs inside a
method, if you need to create instances from only one location and there is a
preexisting type that characterizes the class, make it an anonymous class;
otherwise, make it a local class.
Item 25: Limit source files to a single top-level class
While the Java compiler lets you define multiple top-level classes in a single
source file, there are no benefits associated with doing so, and there are
significant risks. The risks stem from the fact that defining multiple top-level
classes in a source file makes it possible to provide multiple definitions for a
class. Which definition gets used is affected by the order in which the source
files are passed to the compiler.
To make this concrete, consider this source file, which contains only a
Main class that refers to members of two other top-level classes (Utensil
139
and Dessert):
Click here to view code image
public class Main {
public static void main(String[] args) {
System.out.println(Utensil.NAME + Dessert.NAME);
}
}
Now suppose you define both Utensil and Dessert in a single source file
named Utensil.java:
Click here to view code image
// Two classes defined in one file. Don't ever do this!
class Utensil {
static final String NAME = "pan";
}
class Dessert {
static final String NAME = "cake";
}
Of course the main program prints pancake.
Now suppose you accidentally make another source file named
Dessert.java that defines the same two classes:
Click here to view code image
// Two classes defined in one file. Don't ever do this!
class Utensil {
static final String NAME = "pot";
}
class Dessert {
static final String NAME = "pie";
}
If you’re lucky enough to compile the program with the command javac
Main.java Dessert.java, the compilation will fail, and the compiler
will tell you that you’ve multiply defined the classes Utensil and
Dessert. This is so because the compiler will first compile Main.java,
and when it sees the reference to Utensil (which precedes the reference to
Dessert), it will look in Utensil.java for this class and find both
Utensil and Dessert. When the compiler encounters Dessert.java
on the command line, it will pull in that file too, causing it to encounter both
definitions of Utensil and Dessert.
If you compile the program with the command javac Main.java or
140
javac Main.java Utensil.java, it will behave as it did before you
wrote the Dessert.java file, printing pancake. But if you compile the
program with the command javac Dessert.java Main.java, it will
print potpie. The behavior of the program is thus affected by the order in
which the source files are passed to the compiler, which is clearly
unacceptable.
Fixing the problem is as simple as splitting the top-level classes (Utensil
and Dessert, in the case of our example) into separate source files. If you
are tempted to put multiple top-level classes into a single source file, consider
using static member classes (Item 24) as an alternative to splitting the classes
into separate source files. If the classes are subservient to another class,
making them into static member classes is generally the better alternative
because it enhances readability and makes it possible to reduce the
accessibility of the classes by declaring them private (Item 15). Here is how
our example looks with static member classes:
Click here to view code image
// Static member classes instead of multiple top-level
classes
public class Test {
public static void main(String[] args) {
System.out.println(Utensil.NAME + Dessert.NAME);
}
private static class Utensil {
static final String NAME = "pan";
}
private static class Dessert {
static final String NAME = "cake";
}
}
The lesson is clear: Never put multiple top-level classes or interfaces in
a single source file. Following this rule guarantees that you can’t have
multiple definitions for a single class at compile time. This in turn guarantees
that the class files generated by compilation, and the behavior of the resulting
program, are independent of the order in which the source files are passed to
the compiler.
141
Chapter 5. Generics
SINCE Java 5, generics have been a part of the language. Before generics, you
had to cast every object you read from a collection. If someone accidentally
inserted an object of the wrong type, casts could fail at runtime. With
generics, you tell the compiler what types of objects are permitted in each
collection. The compiler inserts casts for you automatically and tells you at
compile time if you try to insert an object of the wrong type. This results in
programs that are both safer and clearer, but these benefits, which are not
limited to collections, come at a price. This chapter tells you how to maximize
the benefits and minimize the complications.
Item 26: Don’t use raw types
First, a few terms. A class or interface whose declaration has one or more type
parameters is a generic class or interface [JLS, 8.1.2, 9.1.2]. For example, the
List interface has a single type parameter, E, representing its element type.
The full name of the interface is List<E> (read “list of E”), but people often
call it List for short. Generic classes and interfaces are collectively known
as generic types.
Each generic type defines a set of parameterized types, which consist of the
class or interface name followed by an angle-bracketed list of actual type
parameters corresponding to the generic type’s formal type parameters [JLS,
4.4, 4.5]. For example, List<String> (read “list of string”) is a
parameterized type representing a list whose elements are of type String.
(String is the actual type parameter corresponding to the formal type
parameter E.)
Finally, each generic type defines a raw type, which is the name of the
generic type used without any accompanying type parameters [JLS, 4.8]. For
example, the raw type corresponding to List<E> is List. Raw types
behave as if all of the generic type information were erased from the type
declaration. They exist primarily for compatibility with pre-generics code.
Before generics were added to Java, this would have been an exemplary
collection declaration. As of Java 9, it is still legal, but far from exemplary:
Click here to view code image
// Raw collection type - don't do this!
// My stamp collection. Contains only Stamp instances.
142
private final Collection stamps = ... ;
If you use this declaration today and then accidentally put a coin into your
stamp collection, the erroneous insertion compiles and runs without error
(though the compiler does emit a vague warning):
Click here to view code image
// Erroneous insertion of coin into stamp collection
stamps.add(new Coin( ... )); // Emits "unchecked call"
warning
You don’t get an error until you try to retrieve the coin from the stamp
collection:
Click here to view code image
// Raw iterator type - don't do this!
for (Iterator i = stamps.iterator(); i.hasNext(); )
Stamp stamp = (Stamp) i.next(); // Throws
ClassCastException
stamp.cancel();
As mentioned throughout this book, it pays to discover errors as soon as
possible after they are made, ideally at compile time. In this case, you don’t
discover the error until runtime, long after it has happened, and in code that
may be distant from the code containing the error. Once you see the
ClassCastException, you have to search through the codebase looking
for the method invocation that put the coin into the stamp collection. The
compiler can’t help you, because it can’t understand the comment that says,
“Contains only Stamp instances.”
With generics, the type declaration contains the information, not the
comment:
Click here to view code image
// Parameterized collection type - typesafe
private final Collection<Stamp> stamps = ... ;
From this declaration, the compiler knows that stamps should contain only
Stamp instances and guarantees it to be true, assuming your entire codebase
compiles without emitting (or suppressing; see Item 27) any warnings. When
stamps is declared with a parameterized type declaration, the erroneous
insertion generates a compile-time error message that tells you exactly what is
wrong:
Click here to view code image
Test.java:9: error: incompatible types: Coin cannot be
converted
143
to Stamp
c.add(new Coin());
^
The compiler inserts invisible casts for you when retrieving elements from
collections and guarantees that they won’t fail (assuming, again, that all of
your code did not generate or suppress any compiler warnings). While the
prospect of accidentally inserting a coin into a stamp collection may appear
far-fetched, the problem is real. For example, it is easy to imagine putting a
BigInteger into a collection that is supposed to contain only
BigDecimal instances.
As noted earlier, it is legal to use raw types (generic types without their
type parameters), but you should never do it. If you use raw types, you lose
all the safety and expressiveness benefits of generics. Given that you
shouldn’t use them, why did the language designers permit raw types in the
first place? For compatibility. Java was about to enter its second decade when
generics were added, and there was an enormous amount of code in existence
that did not use generics. It was deemed critical that all of this code remain
legal and interoperate with newer code that does use generics. It had to be
legal to pass instances of parameterized types to methods that were designed
for use with raw types, and vice versa. This requirement, known as migration
compatibility, drove the decisions to support raw types and to implement
generics using erasure (Item 28).
While you shouldn’t use raw types such as List, it is fine to use types that
are parameterized to allow insertion of arbitrary objects, such as
List<Object>. Just what is the difference between the raw type List and
the parameterized type List<Object>? Loosely speaking, the former has
opted out of the generic type system, while the latter has explicitly told the
compiler that it is capable of holding objects of any type. While you can pass
a List<String> to a parameter of type List, you can’t pass it to a
parameter of type List<Object>. There are sub-typing rules for generics,
and List<String> is a subtype of the raw type List, but not of the
parameterized type List<Object> (Item 28). As a consequence, you lose
type safety if you use a raw type such as List, but not if you use a
parameterized type such as List<Object>.
To make this concrete, consider the following program:
Click here to view code image
// Fails at runtime - unsafeAdd method uses a raw type
(List)!
public static void main(String[] args) {
List<String> strings = new ArrayList<>();
unsafeAdd(strings, Integer.valueOf(42));
144
String s = strings.get(0); // Has compiler-generated
cast
}
private static void unsafeAdd(List list, Object o) {
list.add(o);
}
This program compiles, but because it uses the raw type List, you get a
warning:
Click here to view code image
Test.java:10: warning: [unchecked] unchecked call to
add(E) as a
member of the raw type List
list.add(o);
^
And indeed, if you run the program, you get a ClassCastException
when the program tries to cast the result of the invocation
strings.get(0), which is an Integer, to a String. This is a
compiler-generated cast, so it’s normally guaranteed to succeed, but in this
case we ignored a compiler warning and paid the price.
If you replace the raw type List with the parameterized type
List<Object> in the unsafeAdd declaration and try to recompile the
program, you’ll find that it no longer compiles but emits the error message:
Click here to view code image
Test.java:5: error: incompatible types: List<String>
cannot be
converted to List<Object>
unsafeAdd(strings, Integer.valueOf(42));
^
You might be tempted to use a raw type for a collection whose element
type is unknown and doesn’t matter. For example, suppose you want to write
a method that takes two sets and returns the number of elements they have in
common. Here’s how you might write such a method if you were new to
generics:
Click here to view code image
// Use of raw type for unknown element type - don't do
this!
static int numElementsInCommon(Set s1, Set s2) {
int result = 0;
for (Object o1 : s1)
145
if (s2.contains(o1))
result++;
return result;
}
This method works but it uses raw types, which are dangerous. The safe
alternative is to use unbounded wildcard types. If you want to use a generic
type but you don’t know or care what the actual type parameter is, you can
use a question mark instead. For example, the unbounded wildcard type for
the generic type Set<E> is Set<?> (read “set of some type”). It is the most
general parameterized Set type, capable of holding any set. Here is how the
numElementsInCommon declaration looks with unbounded wildcard
types:
Click here to view code image
// Uses unbounded wildcard type - typesafe and flexible
static int numElementsInCommon(Set<?> s1, Set<?> s2) {
... }
What is the difference between the unbounded wildcard type Set<?> and
the raw type Set? Does the question mark really buy you anything? Not to
belabor the point, but the wildcard type is safe and the raw type isn’t. You can
put any element into a collection with a raw type, easily corrupting the
collection’s type invariant (as demonstrated by the unsafeAdd method on
page 119); you can’t put any element (other than null) into a
Collection<?>. Attempting to do so will generate a compile-time error
message like this:
Click here to view code image
WildCard.java:13: error: incompatible types: String
cannot be
converted to CAP#1
c.add("verboten");
^
where CAP#1 is a fresh type-variable:
CAP#1 extends Object from capture of ?
Admittedly this error message leaves something to be desired, but the
compiler has done its job, preventing you from corrupting the collection’s
type invariant, whatever its element type may be. Not only can’t you put any
element (other than null) into a Collection<?>, but you can’t assume
anything about the type of the objects that you get out. If these restrictions are
unacceptable, you can use generic methods (Item 30) or bounded wildcard
types (Item 31).
There are a few minor exceptions to the rule that you should not use raw
146
types. You must use raw types in class literals. The specification does not
permit the use of parameterized types (though it does permit array types and
primitive types) [JLS, 15.8.2]. In other words, List.class,
String[].class, and int.class are all legal, but
List<String>.class and List<?>.class are not.
A second exception to the rule concerns the instanceof operator.
Because generic type information is erased at runtime, it is illegal to use the
instanceof operator on parameterized types other than unbounded
wildcard types. The use of unbounded wildcard types in place of raw types
does not affect the behavior of the instanceof operator in any way. In this
case, the angle brackets and question marks are just noise. This is the
preferred way to use the instanceof operator with generic types:
Click here to view code image
// Legitimate use of raw type - instanceof operator
if (o instanceof Set) { // Raw type
Set<?> s = (Set<?>) o; // Wildcard type
...
}
Note that once you’ve determined that o is a Set, you must cast it to the
wildcard type Set<?>, not the raw type Set. This is a checked cast, so it
will not cause a compiler warning.
In summary, using raw types can lead to exceptions at runtime, so don’t use
them. They are provided only for compatibility and interoperability with
legacy code that predates the introduction of generics. As a quick review,
Set<Object> is a parameterized type representing a set that can contain
objects of any type, Set<?> is a wildcard type representing a set that can
contain only objects of some unknown type, and Set is a raw type, which
opts out of the generic type system. The first two are safe, and the last is not.
For quick reference, the terms introduced in this item (and a few introduced
later in this chapter) are summarized in the following table:
Term Example Item
Parameterized type List<String> Item 26
Actual type parameter String Item 26
Generic type List<E> Items 26,
29
Formal type
parameter
E Item 26
147
Unbounded wildcard
type
List<?> Item 26
Raw type List Item 26
Bounded type
parameter
<E extends Number> Item 29
Recursive type bound <T extends Comparable<T>> Item 30
Bounded wildcard
type
List<? extends Number> Item 31
Generic method static <E> List<E>
asList(E[] a)
Item 30
Type token String.class Item 33
Item 27: Eliminate unchecked warnings
When you program with generics, you will see many compiler warnings:
unchecked cast warnings, unchecked method invocation warnings, unchecked
parameterized vararg type warnings, and unchecked conversion warnings.
The more experience you acquire with generics, the fewer warnings you’ll
get, but don’t expect newly written code to compile cleanly.
Many unchecked warnings are easy to eliminate. For example, suppose you
accidentally write this declaration:
Click here to view code image
Set<Lark> exaltation = new HashSet();
The compiler will gently remind you what you did wrong:
Click here to view code image
Venery.java:4: warning: [unchecked] unchecked conversion
Set<Lark> exaltation = new HashSet();
^
required: Set<Lark>
found: HashSet
You can then make the indicated correction, causing the warning to disappear.
Note that you don’t actually have to specify the type parameter, merely to
indicate that it’s present with the diamond operator (<>), introduced in Java
7. The compiler will then infer the correct actual type parameter (in this case,
Lark):
148
Click here to view code image
Set<Lark> exaltation = new HashSet<>();
Some warnings will be much more difficult to eliminate. This chapter is
filled with examples of such warnings. When you get warnings that require
some thought, persevere! Eliminate every unchecked warning that you
can. If you eliminate all warnings, you are assured that your code is typesafe,
which is a very good thing. It means that you won’t get a
ClassCastException at runtime, and it increases your confidence that
your program will behave as you intended.
If you can’t eliminate a warning, but you can prove that the code that
provoked the warning is typesafe, then (and only then) suppress the
warning with an @SuppressWarnings("unchecked") annotation. If
you suppress warnings without first proving that the code is typesafe, you are
giving yourself a false sense of security. The code may compile without
emitting any warnings, but it can still throw a ClassCastException at
runtime. If, however, you ignore unchecked warnings that you know to be
safe (instead of suppressing them), you won’t notice when a new warning
crops up that represents a real problem. The new warning will get lost amidst
all the false alarms that you didn’t silence.
The SuppressWarnings annotation can be used on any declaration,
from an individual local variable declaration to an entire class. Always use
the SuppressWarnings annotation on the smallest scope possible.
Typically this will be a variable declaration or a very short method or
constructor. Never use SuppressWarnings on an entire class. Doing so
could mask critical warnings.
If you find yourself using the SuppressWarnings annotation on a
method or constructor that’s more than one line long, you may be able to
move it onto a local variable declaration. You may have to declare a new
local variable, but it’s worth it. For example, consider this toArray method,
which comes from ArrayList:
Click here to view code image
public <T> T[] toArray(T[] a) {
if (a.length < size)
return (T[]) Arrays.copyOf(elements, size,
a.getClass());
System.arraycopy(elements, 0, a, 0, size);
if (a.length > size)
a[size] = null;
return a;
}
149
If you compile ArrayList, the method generates this warning:
Click here to view code image
ArrayList.java:305: warning: [unchecked] unchecked cast
return (T[]) Arrays.copyOf(elements, size,
a.getClass());
^
required: T[]
found: Object[]
It is illegal to put a SuppressWarnings annotation on the return
statement, because it isn’t a declaration [JLS, 9.7]. You might be tempted to
put the annotation on the entire method, but don’t. Instead, declare a local
variable to hold the return value and annotate its declaration, like so:
Click here to view code image
// Adding local variable to reduce scope of
@SuppressWarnings
public <T> T[] toArray(T[] a) {
if (a.length < size) {
// This cast is correct because the array we're
creating
// is of the same type as the one passed in,
which is T[].
@SuppressWarnings("unchecked") T[] result =
(T[]) Arrays.copyOf(elements, size,
a.getClass());
return result;
}
System.arraycopy(elements, 0, a, 0, size);
if (a.length > size)
a[size] = null;
return a;
}
The resulting method compiles cleanly and minimizes the scope in which
unchecked warnings are suppressed.
Every time you use a @SuppressWarnings("unchecked")
annotation, add a comment saying why it is safe to do so. This will help
others understand the code, and more importantly, it will decrease the odds
that someone will modify the code so as to make the computation unsafe. If
you find it hard to write such a comment, keep thinking. You may end up
figuring out that the unchecked operation isn’t safe after all.
In summary, unchecked warnings are important. Don’t ignore them. Every
unchecked warning represents the potential for a ClassCastException
at runtime. Do your best to eliminate these warnings. If you can’t eliminate an
150
unchecked warning and you can prove that the code that provoked it is
typesafe, suppress the warning with a
@SuppressWarnings("unchecked") annotation in the narrowest
possible scope. Record the rationale for your decision to suppress the warning
in a comment.
Item 28: Prefer lists to arrays
Arrays differ from generic types in two important ways. First, arrays are
covariant. This scary-sounding word means simply that if Sub is a subtype of
Super, then the array type Sub[] is a subtype of the array type Super[].
Generics, by contrast, are invariant: for any two distinct types Type1 and
Type2, List<Type1> is neither a subtype nor a supertype of
List<Type2> [JLS, 4.10; Naftalin07, 2.5]. You might think this means that
generics are deficient, but arguably it is arrays that are deficient. This code
fragment is legal:
Click here to view code image
// Fails at runtime!
Object[] objectArray = new Long[1];
objectArray[0] = "I don't fit in"; // Throws
ArrayStoreException
but this one is not:
Click here to view code image
// Won't compile!
List<Object> ol = new ArrayList<Long>(); // Incompatible
types
ol.add("I don't fit in");
Either way you can’t put a String into a Long container, but with an array
you find out that you’ve made a mistake at runtime; with a list, you find out at
compile time. Of course, you’d rather find out at compile time.
The second major difference between arrays and generics is that arrays are
reified [JLS, 4.7]. This means that arrays know and enforce their element type
at runtime. As noted earlier, if you try to put a String into an array of
Long, you’ll get an ArrayStoreException. Generics, by contrast, are
implemented by erasure [JLS, 4.6]. This means that they enforce their type
constraints only at compile time and discard (or erase) their element type
information at runtime. Erasure is what allowed generic types to interoperate
freely with legacy code that didn’t use generics (Item 26), ensuring a smooth
transition to generics in Java 5.
151
Because of these fundamental differences, arrays and generics do not mix
well. For example, it is illegal to create an array of a generic type, a
parameterized type, or a type parameter. Therefore, none of these array
creation expressions are legal: new List<E>[], new List<String>
[], new E[]. All will result in generic array creation errors at compile
time.
Why is it illegal to create a generic array? Because it isn’t typesafe. If it
were legal, casts generated by the compiler in an otherwise correct program
could fail at runtime with a ClassCastException. This would violate
the fundamental guarantee provided by the generic type system.
To make this more concrete, consider the following code fragment:
Click here to view code image
// Why generic array creation is illegal - won't
compile!
List<String>[] stringLists = new List<String>[1]; //
(1)
List<Integer> intList = List.of(42); //
(2)
Object[] objects = stringLists; //
(3)
objects[0] = intList; //
(4)
String s = stringLists[0].get(0); //
(5)
Let’s pretend that line 1, which creates a generic array, is legal. Line 2 creates
and initializes a List<Integer> containing a single element. Line 3 stores
the List<String> array into an Object array variable, which is legal
because arrays are covariant. Line 4 stores the List<Integer> into the
sole element of the Object array, which succeeds because generics are
implemented by erasure: the runtime type of a List<Integer> instance is
simply List, and the runtime type of a List<String>[] instance is
List[], so this assignment doesn’t generate an
ArrayStoreException. Now we’re in trouble. We’ve stored a
List<Integer> instance into an array that is declared to hold only
List<String> instances. In line 5, we retrieve the sole element from the
sole list in this array. The compiler automatically casts the retrieved element
to String, but it’s an Integer, so we get a ClassCastException at
runtime. In order to prevent this from happening, line 1 (which creates a
generic array) must generate a compile-time error.
Types such as E, List<E>, and List<String> are technically known
as nonreifiable types [JLS, 4.7]. Intuitively speaking, a non-reifiable type is
152
one whose runtime representation contains less information than its compiletime representation. Because of erasure, the only parameterized types that are
reifiable are unbounded wildcard types such as List<?> and Map<?,?>
(Item 26). It is legal, though rarely useful, to create arrays of unbounded
wildcard types.
The prohibition on generic array creation can be annoying. It means, for
example, that it’s not generally possible for a generic collection to return an
array of its element type (but see Item 33 for a partial solution). It also means
that you get confusing warnings when using varargs methods (Item 53) in
combination with generic types. This is because every time you invoke a
varargs method, an array is created to hold the varargs parameters. If the
element type of this array is not reifiable, you get a warning. The
SafeVarargs annotation can be used to address this issue (Item 32).
When you get a generic array creation error or an unchecked cast warning
on a cast to an array type, the best solution is often to use the collection type
List<E> in preference to the array type E[]. You might sacrifice some
conciseness or performance, but in exchange you get better type safety and
interoperability.
For example, suppose you want to write a Chooser class with a
constructor that takes a collection, and a single method that returns an element
of the collection chosen at random. Depending on what collection you pass to
the constructor, you could use a chooser as a game die, a magic 8-ball, or a
data source for a Monte Carlo simulation. Here’s a simplistic implementation
without generics:
Click here to view code image
// Chooser - a class badly in need of generics!
public class Chooser {
private final Object[] choiceArray;
public Chooser(Collection choices) {
choiceArray = choices.toArray();
}
public Object choose() {
Random rnd = ThreadLocalRandom.current();
return
choiceArray[rnd.nextInt(choiceArray.length)];
}
}
To use this class, you have to cast the choose method’s return value from
Object to the desired type every time you use invoke the method, and the
cast will fail at runtime if you get the type wrong. Taking the advice of Item
153
29 to heart, we attempt to modify Chooser to make it generic. Changes are
shown in boldface:
Click here to view code image
// A first cut at making Chooser generic - won't compile
public class Chooser<T> {
private final T[] choiceArray;
public Chooser(Collection<T> choices) {
choiceArray = choices.toArray();
}
// choose method unchanged
}
If you try to compile this class, you’ll get this error message:
Click here to view code image
Chooser.java:9: error: incompatible types: Object[]
cannot be
converted to T[]
choiceArray = choices.toArray();
^
where T is a type-variable:
T extends Object declared in class Chooser
No big deal, you say, I’ll cast the Object array to a T array:
Click here to view code image
choiceArray = (T[]) choices.toArray();
This gets rid of the error, but instead you get a warning:
Click here to view code image
Chooser.java:9: warning: [unchecked] unchecked cast
choiceArray = (T[]) choices.toArray();
^
required: T[], found: Object[]
where T is a type-variable:
T extends Object declared in class Chooser
The compiler is telling you that it can’t vouch for the safety of the cast at
runtime because the program won’t know what type T represents—remember,
element type information is erased from generics at runtime. Will the program
work? Yes, but the compiler can’t prove it. You could prove it to yourself, put
the proof in a comment and suppress the warning with an annotation, but
you’re better off eliminating the cause of warning (Item 27).
154
To eliminate the unchecked cast warning, use a list instead of an array.
Here is a version of the Chooser class that compiles without error or
warning:
Click here to view code image
// List-based Chooser - typesafe
public class Chooser<T> {
private final List<T> choiceList;
public Chooser(Collection<T> choices) {
choiceList = new ArrayList<>(choices);
}
public T choose() {
Random rnd = ThreadLocalRandom.current();
return
choiceList.get(rnd.nextInt(choiceList.size()));
}
}
This version is a tad more verbose, and perhaps a tad slower, but it’s worth it
for the peace of mind that you won’t get a ClassCastException at
runtime.
In summary, arrays and generics have very different type rules. Arrays are
covariant and reified; generics are invariant and erased. As a consequence,
arrays provide runtime type safety but not compile-time type safety, and vice
versa for generics. As a rule, arrays and generics don’t mix well. If you find
yourself mixing them and getting compile-time errors or warnings, your first
impulse should be to replace the arrays with lists.
Item 29: Favor generic types
It is generally not too difficult to parameterize your declarations and make use
of the generic types and methods provided by the JDK. Writing your own
generic types is a bit more difficult, but it’s worth the effort to learn how.
Consider the simple (toy) stack implementation from Item 7:
Click here to view code image
// Object-based collection - a prime candidate for
generics
public class Stack {
private Object[] elements;
private int size = 0;
private static final int DEFAULT_INITIAL_CAPACITY =
16;
155
public Stack() {
elements = new Object[DEFAULT_INITIAL_CAPACITY];
}
public void push(Object e) {
ensureCapacity();
elements[size++] = e;
}
public Object pop() {
if (size == 0)
throw new EmptyStackException();
Object result = elements[--size];
elements[size] = null; // Eliminate obsolete
reference
return result;
}
public boolean isEmpty() {
return size == 0;
}
private void ensureCapacity() {
if (elements.length == size)
elements = Arrays.copyOf(elements, 2 * size
+ 1);
}
}
This class should have been parameterized to begin with, but since it wasn’t,
we can generify it after the fact. In other words, we can parameterize it
without harming clients of the original non-parameterized version. As it
stands, the client has to cast objects that are popped off the stack, and those
casts might fail at runtime. The first step in generifying a class is to add one
or more type parameters to its declaration. In this case there is one type
parameter, representing the element type of the stack, and the conventional
name for this type parameter is E (Item 68).
The next step is to replace all the uses of the type Object with the
appropriate type parameter and then try to compile the resulting program:
Click here to view code image
// Initial attempt to generify Stack - won't compile!
public class Stack<E> {
private E[] elements;
private int size = 0;
private static final int DEFAULT_INITIAL_CAPACITY =
156
16;
public Stack() {
elements = new E[DEFAULT_INITIAL_CAPACITY];
}
public void push(E e) {
ensureCapacity();
elements[size++] = e;
}
public E pop() {
if (size == 0)
throw new EmptyStackException();
E result = elements[--size];
elements[size] = null; // Eliminate obsolete
reference
return result;
}
... // no changes in isEmpty or ensureCapacity
}
You’ll generally get at least one error or warning, and this class is no
exception. Luckily, this class generates only one error:
Click here to view code image
Stack.java:8: generic array creation
elements = new E[DEFAULT_INITIAL_CAPACITY];
^
As explained in Item 28, you can’t create an array of a non-reifiable type,
such as E. This problem arises every time you write a generic type that is
backed by an array. There are two reasonable ways to solve it. The first
solution directly circumvents the prohibition on generic array creation: create
an array of Object and cast it to the generic array type. Now in place of an
error, the compiler will emit a warning. This usage is legal, but it’s not (in
general) typesafe:
Click here to view code image
Stack.java:8: warning: [unchecked] unchecked cast
found: Object[], required: E[]
elements = (E[]) new
Object[DEFAULT_INITIAL_CAPACITY];
^
The compiler may not be able to prove that your program is typesafe, but
you can. You must convince yourself that the unchecked cast will not
157
compromise the type safety of the program. The array in question
(elements) is stored in a private field and never returned to the client or
passed to any other method. The only elements stored in the array are those
passed to the push method, which are of type E, so the unchecked cast can
do no harm.
Once you’ve proved that an unchecked cast is safe, suppress the warning in
as narrow a scope as possible (Item 27). In this case, the constructor contains
only the unchecked array creation, so it’s appropriate to suppress the warning
in the entire constructor. With the addition of an annotation to do this, Stack
compiles cleanly, and you can use it without explicit casts or fear of a
ClassCastException:
Click here to view code image
// The elements array will contain only E instances from
push(E).
// This is sufficient to ensure type safety, but the
runtime
// type of the array won't be E[]; it will always be
Object[]!
@SuppressWarnings("unchecked")
public Stack() {
elements = (E[]) new
Object[DEFAULT_INITIAL_CAPACITY];
}
The second way to eliminate the generic array creation error in Stack is to
change the type of the field elements from E[] to Object[]. If you do
this, you’ll get a different error:
Click here to view code image
Stack.java:19: incompatible types
found: Object, required: E
E result = elements[--size];
^
You can change this error into a warning by casting the element retrieved
from the array to E, but you will get a warning:
Click here to view code image
Stack.java:19: warning: [unchecked] unchecked cast
found: Object, required: E
E result = (E) elements[--size];
^
Because E is a non-reifiable type, there’s no way the compiler can check
the cast at runtime. Again, you can easily prove to yourself that the unchecked
158
cast is safe, so it’s appropriate to suppress the warning. In line with the advice
of Item 27, we suppress the warning only on the assignment that contains the
unchecked cast, not on the entire pop method:
Click here to view code image
// Appropriate suppression of unchecked warning
public E pop() {
if (size == 0)
throw new EmptyStackException();
// push requires elements to be of type E, so cast
is correct
@SuppressWarnings("unchecked") E result =
(E) elements[--size];
elements[size] = null; // Eliminate obsolete
reference
return result;
}
Both techniques for eliminating the generic array creation have their
adherents. The first is more readable: the array is declared to be of type E[],
clearly indicating that it contains only E instances. It is also more concise: in a
typical generic class, you read from the array at many points in the code; the
first technique requires only a single cast (where the array is created), while
the second requires a separate cast each time an array element is read. Thus,
the first technique is preferable and more commonly used in practice. It does,
however, cause heap pollution (Item 32): the runtime type of the array does
not match its compile-time type (unless E happens to be Object). This
makes some programmers sufficiently queasy that they opt for the second
technique, though the heap pollution is harmless in this situation.
The following program demonstrates the use of our generic Stack class.
The program prints its command line arguments in reverse order and
converted to uppercase. No explicit cast is necessary to invoke String’s
toUpperCase method on the elements popped from the stack, and the
automatically generated cast is guaranteed to succeed:
Click here to view code image
// Little program to exercise our generic Stack
public static void main(String[] args) {
Stack<String> stack = new Stack<>();
for (String arg : args)
stack.push(arg);
while (!stack.isEmpty())
System.out.println(stack.pop().toUpperCase());
159
}
The foregoing example may appear to contradict Item 28, which
encourages the use of lists in preference to arrays. It is not always possible or
desirable to use lists inside your generic types. Java doesn’t support lists
natively, so some generic types, such as ArrayList, must be implemented
atop arrays. Other generic types, such as HashMap, are implemented atop
arrays for performance.
The great majority of generic types are like our Stack example in that
their type parameters have no restrictions: you can create a
Stack<Object>, Stack<int[]>, Stack<List<String>>, or
Stack of any other object reference type. Note that you can’t create a
Stack of a primitive type: trying to create a Stack<int> or
Stack<double> will result in a compile-time error. This is a fundamental
limitation of Java’s generic type system. You can work around this restriction
by using boxed primitive types (Item 61).
There are some generic types that restrict the permissible values of their
type parameters. For example, consider
java.util.concurrent.DelayQueue, whose declaration looks like
this:
Click here to view code image
class DelayQueue<E extends Delayed> implements
BlockingQueue<E>
The type parameter list (<E extends Delayed>) requires that the actual
type parameter E be a subtype of java.util.concurrent.Delayed.
This allows the DelayQueue implementation and its clients to take
advantage of Delayed methods on the elements of a DelayQueue, without
the need for explicit casting or the risk of a ClassCastException. The
type parameter E is known as a bounded type parameter. Note that the
subtype relation is defined so that every type is a subtype of itself [JLS, 4.10],
so it is legal to create a DelayQueue<Delayed>.
In summary, generic types are safer and easier to use than types that require
casts in client code. When you design new types, make sure that they can be
used without such casts. This will often mean making the types generic. If
you have any existing types that should be generic but aren’t, generify them.
This will make life easier for new users of these types without breaking
existing clients (Item 26).
Item 30: Favor generic methods
160
Just as classes can be generic, so can methods. Static utility methods that
operate on parameterized types are usually generic. All of the “algorithm”
methods in Collections (such as binarySearch and sort) are
generic.
Writing generic methods is similar to writing generic types. Consider this
deficient method, which returns the union of two sets:
Click here to view code image
// Uses raw types - unacceptable! (Item 26)
public static Set union(Set s1, Set s2) {
Set result = new HashSet(s1);
result.addAll(s2);
return result;
}
This method compiles but with two warnings:
Click here to view code image
Union.java:5: warning: [unchecked] unchecked call to
HashSet(Collection<? extends E>) as a member of raw type
HashSet
Set result = new HashSet(s1);
^
Union.java:6: warning: [unchecked] unchecked call to
addAll(Collection<? extends E>) as a member of raw type
Set
result.addAll(s2);
^
To fix these warnings and make the method typesafe, modify its
declaration to declare a type parameter representing the element type for the
three sets (the two arguments and the return value) and use this type
parameter throughout the method. The type parameter list, which declares
the type parameters, goes between a method’s modifiers and its return
type. In this example, the type parameter list is <E>, and the return type is
Set<E>. The naming conventions for type parameters are the same for
generic methods and generic types (Items 29, 68):
Click here to view code image
// Generic method
public static <E> Set<E> union(Set<E> s1, Set<E> s2) {
Set<E> result = new HashSet<>(s1);
result.addAll(s2);
return result;
}
161
At least for simple generic methods, that’s all there is to it. This method
compiles without generating any warnings and provides type safety as well as
ease of use. Here’s a simple program to exercise the method. This program
contains no casts and compiles without errors or warnings:
Click here to view code image
// Simple program to exercise generic method
public static void main(String[] args) {
Set<String> guys = Set.of("Tom", "Dick", "Harry");
Set<String> stooges = Set.of("Larry", "Moe",
"Curly");
Set<String> aflCio = union(guys, stooges);
System.out.println(aflCio);
}
When you run the program, it prints [Moe, Tom, Harry, Larry,
Curly, Dick]. (The order of the elements in the output is
implementation-dependent.)
A limitation of the union method is that the types of all three sets (both
input parameters and the return value) have to be exactly the same. You can
make the method more flexible by using bounded wildcard types (Item 31).
On occasion, you will need to create an object that is immutable but
applicable to many different types. Because generics are implemented by
erasure (Item 28), you can use a single object for all required type
parameterizations, but you need to write a static factory method to repeatedly
dole out the object for each requested type parameterization. This pattern,
called the generic singleton factory, is used for function objects (Item 42)
such as Collections.reverseOrder, and occasionally for collections
such as Collections.emptySet.
Suppose that you want to write an identity function dispenser. The libraries
provide Function.identity, so there’s no reason to write your own
(Item 59), but it is instructive. It would be wasteful to create a new identity
function object time one is requested, because it’s stateless. If Java’s generics
were reified, you would need one identity function per type, but since they’re
erased a generic singleton will suffice. Here’s how it looks:
Click here to view code image
// Generic singleton factory pattern
private static UnaryOperator<Object> IDENTITY_FN = (t) -
> t;
@SuppressWarnings("unchecked")
public static <T> UnaryOperator<T> identityFunction() {
return (UnaryOperator<T>) IDENTITY_FN;
162
}
The cast of IDENTITY_FN to (UnaryFunction<T>) generates an
unchecked cast warning, as UnaryOperator<Object> is not a
UnaryOperator<T> for every T. But the identity function is special: it
returns its argument unmodified, so we know that it is typesafe to use it as a
UnaryFunction<T>, whatever the value of T. Therefore, we can
confidently suppress the unchecked cast warning generated by this cast. Once
we’ve done this, the code compiles without error or warning.
Here is a sample program that uses our generic singleton as a
UnaryOperator<String> and a UnaryOperator<Number>. As
usual, it contains no casts and compiles without errors or warnings:
Click here to view code image
// Sample program to exercise generic singleton
public static void main(String[] args) {
String[] strings = { "jute", "hemp", "nylon" };
UnaryOperator<String> sameString =
identityFunction();
for (String s : strings)
System.out.println(sameString.apply(s));
Number[] numbers = { 1, 2.0, 3L };
UnaryOperator<Number> sameNumber =
identityFunction();
for (Number n : numbers)
System.out.println(sameNumber.apply(n));
}
It is permissible, though relatively rare, for a type parameter to be bounded
by some expression involving that type parameter itself. This is what’s known
as a recursive type bound. A common use of recursive type bounds is in
connection with the Comparable interface, which defines a type’s natural
ordering (Item 14). This interface is shown here:
Click here to view code image
public interface Comparable<T> {
int compareTo(T o);
}
The type parameter T defines the type to which elements of the type
implementing Comparable<T> can be compared. In practice, nearly all
types can be compared only to elements of their own type. So, for example,
String implements Comparable<String>, Integer implements
Comparable<Integer>, and so on.
163
Many methods take a collection of elements implementing Comparable
to sort it, search within it, calculate its minimum or maximum, and the like.
To do these things, it is required that every element in the collection be
comparable to every other element in it, in other words, that the elements of
the list be mutually comparable. Here is how to express that constraint:
Click here to view code image
// Using a recursive type bound to express mutual
comparability
public static <E extends Comparable<E>> E
max(Collection<E> c);
The type bound <E extends Comparable<E>> may be read as “any
type E that can be compared to itself,” which corresponds more or less
precisely to the notion of mutual comparability.
Here is a method to go with the previous declaration. It calculates the
maximum value in a collection according to its elements’ natural order, and it
compiles without errors or warnings:
Click here to view code image
// Returns max value in a collection - uses recursive
type bound
public static <E extends Comparable<E>> E
max(Collection<E> c) {
if (c.isEmpty())
throw new IllegalArgumentException("Empty
collection");
E result = null;
for (E e : c)
if (result == null || e.compareTo(result) > 0)
result = Objects.requireNonNull(e);
return result;
}
Note that this method throws IllegalArgumentException if the list is
empty. A better alternative would be to return an Optional<E> (Item 55).
Recursive type bounds can get much more complex, but luckily they rarely
do. If you understand this idiom, its wildcard variant (Item 31), and the
simulated self-type idiom (Item 2), you’ll be able to deal with most of the
recursive type bounds you encounter in practice.
In summary, generic methods, like generic types, are safer and easier to use
than methods requiring their clients to put explicit casts on input parameters
and return values. Like types, you should make sure that your methods can be
164
used without casts, which often means making them generic. And like types,
you should generify existing methods whose use requires casts. This makes
life easier for new users without breaking existing clients (Item 26).
Item 31: Use bounded wildcards to increase API
flexibility
As noted in Item 28, parameterized types are invariant. In other words, for
any two distinct types Type1 and Type2, List<Type1> is neither a
subtype nor a supertype of List<Type2>. Although it is counterintuitive
that List<String> is not a subtype of List<Object>, it really does
make sense. You can put any object into a List<Object>, but you can put
only strings into a List<String>. Since a List<String> can’t do
everything a List<Object> can, it isn’t a subtype (by the Liskov
substitution principal, Item 10).
Sometimes you need more flexibility than invariant typing can provide.
Consider the Stack class from Item 29. To refresh your memory, here is its
public API:
Click here to view code image
public class Stack<E> {
public Stack();
public void push(E e);
public E pop();
public boolean isEmpty();
}
Suppose we want to add a method that takes a sequence of elements and
pushes them all onto the stack. Here’s a first attempt:
Click here to view code image
// pushAll method without wildcard type - deficient!
public void pushAll(Iterable<E> src) {
for (E e : src)
push(e);
}
This method compiles cleanly, but it isn’t entirely satisfactory. If the element
type of the Iterable src exactly matches that of the stack, it works fine.
But suppose you have a Stack<Number> and you invoke
push(intVal), where intVal is of type Integer. This works because
Integer is a subtype of Number. So logically, it seems that this should
work, too:
165
Click here to view code image
Stack<Number> numberStack = new Stack<>();
Iterable<Integer> integers = ... ;
numberStack.pushAll(integers);
If you try it, however, you’ll get this error message because parameterized
types are invariant:
Click here to view code image
StackTest.java:7: error: incompatible types:
Iterable<Integer>
cannot be converted to Iterable<Number>
numberStack.pushAll(integers);
^
Luckily, there’s a way out. The language provides a special kind of
parameterized type call a bounded wildcard type to deal with situations like
this. The type of the input parameter to pushAll should not be “Iterable
of E” but “Iterable of some subtype of E,” and there is a wildcard type
that means precisely that: Iterable<? extends E>. (The use of the
keyword extends is slightly misleading: recall from Item 29 that subtype is
defined so that every type is a subtype of itself, even though it does not
extend itself.) Let’s modify pushAll to use this type:
Click here to view code image
// Wildcard type for a parameter that serves as an E
producer
public void pushAll(Iterable<? extends E> src) {
for (E e : src)
push(e);
}
With this change, not only does Stack compile cleanly, but so does the
client code that wouldn’t compile with the original pushAll declaration.
Because Stack and its client compile cleanly, you know that everything is
typesafe.
Now suppose you want to write a popAll method to go with pushAll.
The popAll method pops each element off the stack and adds the elements
to the given collection. Here’s how a first attempt at writing the popAll
method might look:
Click here to view code image
// popAll method without wildcard type - deficient!
public void popAll(Collection<E> dst) {
while (!isEmpty())
166
dst.add(pop());
}
Again, this compiles cleanly and works fine if the element type of the
destination collection exactly matches that of the stack. But again, it isn’t
entirely satisfactory. Suppose you have a Stack<Number> and variable of
type Object. If you pop an element from the stack and store it in the
variable, it compiles and runs without error. So shouldn’t you be able to do
this, too?
Click here to view code image
Stack<Number> numberStack = new Stack<Number>();
Collection<Object> objects = ... ;
numberStack.popAll(objects);
If you try to compile this client code against the version of popAll shown
earlier, you’ll get an error very similar to the one that we got with our first
version of pushAll: Collection<Object> is not a subtype of
Collection<Number>. Once again, wildcard types provide a way out.
The type of the input parameter to popAll should not be “collection of E”
but “collection of some supertype of E” (where supertype is defined such that
E is a supertype of itself [JLS, 4.10]). Again, there is a wildcard type that
means precisely that: Collection<? super E>. Let’s modify popAll
to use it:
Click here to view code image
// Wildcard type for parameter that serves as an E
consumer
public void popAll(Collection<? super E> dst) {
while (!isEmpty())
dst.add(pop());
}
With this change, both Stack and the client code compile cleanly.
The lesson is clear. For maximum flexibility, use wildcard types on
input parameters that represent producers or consumers. If an input
parameter is both a producer and a consumer, then wildcard types will do you
no good: you need an exact type match, which is what you get without any
wildcards.
Here is a mnemonic to help you remember which wildcard type to use:
PECS stands for producer-extends, consumer-super.
In other words, if a parameterized type represents a T producer, use <?
extends T>; if it represents a T consumer, use <? super T>. In our
167
Stack example, pushAll’s src parameter produces E instances for use by
the Stack, so the appropriate type for src is Iterable<? extends
E>; popAll’s dst parameter consumes E instances from the Stack, so the
appropriate type for dst is Collection<? super E>. The PECS
mnemonic captures the fundamental principle that guides the use of wild-card
types. Naftalin and Wadler call it the Get and Put Principle [Naftalin07, 2.4].
With this mnemonic in mind, let’s take a look at some method and
constructor declarations from previous items in this chapter. The Chooser
constructor in Item 28 has this declaration:
Click here to view code image
public Chooser(Collection<T> choices)
This constructor uses the collection choices only to produce values of type
T (and stores them for later use), so its declaration should use a wildcard type
that extends T. Here’s the resulting constructor declaration:
Click here to view code image
// Wildcard type for parameter that serves as an T
producer
public Chooser(Collection<? extends T> choices)
And would this change make any difference in practice? Yes, it would.
Suppose you have a List<Integer>, and you want to pass it in to the
constructor for a Chooser<Number>. This would not compile with the
original declaration, but it does once you add the bounded wildcard type to
the declaration.
Now let’s look at the union method from Item 30. Here is the declaration:
Click here to view code image
public static <E> Set<E> union(Set<E> s1, Set<E> s2)
Both parameters, s1 and s2, are E producers, so the PECS mnemonic tells us
that the declaration should be as follows:
Click here to view code image
public static <E> Set<E> union(Set<? extends E> s1,
Set<? extends E> s2)
Note that the return type is still Set<E>. Do not use bounded wildcard
types as return types. Rather than providing additional flexibility for your
users, it would force them to use wildcard types in client code. With the
revised declaration, this code will compile cleanly:
Click here to view code image
168
Set<Integer> integers = Set.of(1, 3, 5);
Set<Double> doubles = Set.of(2.0, 4.0, 6.0);
Set<Number> numbers = union(integers, doubles);
Properly used, wildcard types are nearly invisible to the users of a class.
They cause methods to accept the parameters they should accept and reject
those they should reject. If the user of a class has to think about wildcard
types, there is probably something wrong with its API.
Prior to Java 8, the type inference rules were not clever enough to handle
the previous code fragment, which requires the compiler to use the
contextually specified return type (or target type) to infer the type of E. The
target type of the union invocation shown earlier is Set<Number>. If you
try to compile the fragment in an earlier version of Java (with an appropriate
replacement for the Set.of factory), you’ll get a long, convoluted error
message like this:
Click here to view code image
Union.java:14: error: incompatible types
Set<Number> numbers = union(integers, doubles);
^
required: Set<Number>
found: Set<INT#1>
where INT#1,INT#2 are intersection types:
INT#1 extends Number,Comparable<? extends INT#2>
INT#2 extends Number,Comparable<?>
Luckily there is a way to deal with this sort of error. If the compiler doesn’t
infer the correct type, you can always tell it what type to use with an explicit
type argument [JLS, 15.12]. Even prior to the introduction of target typing in
Java 8, this isn’t something that you had to do often, which is good because
explicit type arguments aren’t very pretty. With the addition of an explicit
type argument, as shown here, the code fragment compiles cleanly in versions
prior to Java 8:
Click here to view code image
// Explicit type parameter - required prior to Java 8
Set<Number> numbers = Union.<Number>union(integers,
doubles);
Next let’s turn our attention to the max method in Item 30. Here is the
original declaration:
Click here to view code image
public static <T extends Comparable<T>> T max(List<T>
list)
169
Here is a revised declaration that uses wildcard types:
Click here to view code image
public static <T extends Comparable<? super T>> T max(
List<? extends T> list)
To get the revised declaration from the original, we applied the PECS
heuristic twice. The straightforward application is to the parameter list. It
produces T instances, so we change the type from List<T> to List<?
extends T>. The tricky application is to the type parameter T. This is the
first time we’ve seen a wildcard applied to a type parameter. Originally, T
was specified to extend Comparable<T>, but a comparable of T consumes
T instances (and produces integers indicating order relations). Therefore, the
parameterized type Comparable<T> is replaced by the bounded wildcard
type Comparable<? super T>. Comparables are always consumers, so
you should generally use Comparable<? super T> in preference to
Comparable<T>. The same is true of comparators; therefore, you should
generally use Comparator<? super T> in preference to
Comparator<T>.
The revised max declaration is probably the most complex method
declaration in this book. Does the added complexity really buy you anything?
Again, it does. Here is a simple example of a list that would be excluded by
the original declaration but is permitted by the revised one:
Click here to view code image
List<ScheduledFuture<?>> scheduledFutures = ... ;
The reason that you can’t apply the original method declaration to this list
is that ScheduledFuture does not implement
Comparable<ScheduledFuture>. Instead, it is a subinterface of
Delayed, which extends Comparable<Delayed>. In other words, a
ScheduledFuture instance isn’t merely comparable to other
ScheduledFuture instances; it is comparable to any Delayed instance,
and that’s enough to cause the original declaration to reject it. More generally,
the wildcard is required to support types that do not implement
Comparable (or Comparator) directly but extend a type that does.
There is one more wildcard-related topic that bears discussing. There is a
duality between type parameters and wildcards, and many methods can be
declared using one or the other. For example, here are two possible
declarations for a static method to swap two indexed items in a list. The first
uses an unbounded type parameter (Item 30) and the second an unbounded
wildcard:
170
Click here to view code image
// Two possible declarations for the swap method
public static <E> void swap(List<E> list, int i, int j);
public static void swap(List<?> list, int i, int j);
Which of these two declarations is preferable, and why? In a public API,
the second is better because it’s simpler. You pass in a list—any list—and the
method swaps the indexed elements. There is no type parameter to worry
about. As a rule, if a type parameter appears only once in a method
declaration, replace it with a wildcard. If it’s an unbounded type parameter,
replace it with an unbounded wildcard; if it’s a bounded type parameter,
replace it with a bounded wildcard.
There’s one problem with the second declaration for swap. The
straightforward implementation won’t compile:
Click here to view code image
public static void swap(List<?> list, int i, int j) {
list.set(i, list.set(j, list.get(i)));
}
Trying to compile it produces this less-than-helpful error message:
Click here to view code image
Swap.java:5: error: incompatible types: Object cannot be
converted to CAP#1
list.set(i, list.set(j, list.get(i)));
^
where CAP#1 is a fresh type-variable:
CAP#1 extends Object from capture of ?
It doesn’t seem right that we can’t put an element back into the list that we
just took it out of. The problem is that the type of list is List<?>, and
you can’t put any value except null into a List<?>. Fortunately, there is a
way to implement this method without resorting to an unsafe cast or a raw
type. The idea is to write a private helper method to capture the wildcard
type. The helper method must be a generic method in order to capture the
type. Here’s how it looks:
Click here to view code image
public static void swap(List<?> list, int i, int j) {
swapHelper(list, i, j);
}
// Private helper method for wildcard capture
private static <E> void swapHelper(List<E> list, int i,
171
int j) {
list.set(i, list.set(j, list.get(i)));
}
The swapHelper method knows that list is a List<E>. Therefore, it
knows that any value it gets out of this list is of type E and that it’s safe to put
any value of type E into the list. This slightly convoluted implementation of
swap compiles cleanly. It allows us to export the nice wildcard-based
declaration, while taking advantage of the more complex generic method
internally. Clients of the swap method don’t have to confront the more
complex swapHelper declaration, but they do benefit from it. It is worth
noting that the helper method has precisely the signature that we dismissed as
too complex for the public method.
In summary, using wildcard types in your APIs, while tricky, makes the
APIs far more flexible. If you write a library that will be widely used, the
proper use of wildcard types should be considered mandatory. Remember the
basic rule: producer-extends, consumer-super (PECS). Also remember
that all comparables and comparators are consumers.
Item 32: Combine generics and varargs judiciously
Varargs methods (Item 53) and generics were both added to the platform in
Java 5, so you might expect them to interact gracefully; sadly, they do not.
The purpose of varargs is to allow clients to pass a variable number of
arguments to a method, but it is a leaky abstraction: when you invoke a
varargs method, an array is created to hold the varargs parameters; that array,
which should be an implementation detail, is visible. As a consequence, you
get confusing compiler warnings when varargs parameters have generic or
parameterized types.
Recall from Item 28 that a non-reifiable type is one whose runtime
representation has less information than its compile-time representation, and
that nearly all generic and parameterized types are non-reifiable. If a method
declares its varargs parameter to be of a non-reifiable type, the compiler
generates a warning on the declaration. If the method is invoked on varargs
parameters whose inferred type is non-reifiable, the compiler generates a
warning on the invocation too. The warnings look something like this:
Click here to view code image
warning: [unchecked] Possible heap pollution from
parameterized vararg type List<String>
Heap pollution occurs when a variable of a parameterized type refers to an
object that is not of that type [JLS, 4.12.2]. It can cause the compiler’s
172
automatically generated casts to fail, violating the fundamental guarantee of
the generic type system.
For example, consider this method, which is a thinly disguised variant of
the code fragment on page 127:
Click here to view code image
// Mixing generics and varargs can violate type safety!
static void dangerous(List<String>... stringLists) {
List<Integer> intList = List.of(42);
Object[] objects = stringLists;
objects[0] = intList; // Heap pollution
String s = stringLists[0].get(0); //
ClassCastException
}
This method has no visible casts yet throws a ClassCastException
when invoked with one or more arguments. Its last line has an invisible cast
that is generated by the compiler. This cast fails, demonstrating that type
safety has been compromised, and it is unsafe to store a value in a generic
varargs array parameter.
This example raises an interesting question: Why is it even legal to declare
a method with a generic varargs parameter, when it is illegal to create a
generic array explicitly? In other words, why does the method shown
previously generate only a warning, while the code fragment on page 127
generates an error? The answer is that methods with varargs parameters of
generic or parameterized types can be very useful in practice, so the language
designers opted to live with this inconsistency. In fact, the Java libraries
export several such methods, including Arrays.asList(T... a),
Collections.addAll(Collection<? super T> c, T...
elements), and EnumSet.of(E first, E... rest). Unlike the
dangerous method shown earlier, these library methods are typesafe.
Prior to Java 7, there was nothing the author of a method with a generic
varargs parameter could do about the warnings at the call sites. This made
these APIs unpleasant to use. Users had to put up with the warnings or,
preferably, to eliminate them with
@SuppressWarnings("unchecked") annotations at every call site
(Item 27). This was tedious, harmed readability, and hid warnings that
flagged real issues.
In Java 7, the SafeVarargs annotation was added to the platform, to
allow the author of a method with a generic varargs parameter to suppress
client warnings automatically. In essence, the SafeVarargs annotation
constitutes a promise by the author of a method that it is typesafe. In
exchange for this promise, the compiler agrees not to warn the users of the
173
method that calls may be unsafe.
It is critical that you do not annotate a method with @SafeVarargs
unless it actually is safe. So what does it take to ensure this? Recall that a
generic array is created when the method is invoked, to hold the varargs
parameters. If the method doesn’t store anything into the array (which would
overwrite the parameters) and doesn’t allow a reference to the array to escape
(which would enable untrusted code to access the array), then it’s safe. In
other words, if the varargs parameter array is used only to transmit a variable
number of arguments from the caller to the method—which is, after all, the
purpose of varargs—then the method is safe.
It is worth noting that you can violate type safety without ever storing
anything in the varargs parameter array. Consider the following generic
varargs method, which returns an array containing its parameters. At first
glance, it may look like a handy little utility:
Click here to view code image
// UNSAFE - Exposes a reference to its generic parameter
array!
static <T> T[] toArray(T... args) {
return args;
}
This method simply returns its varargs parameter array. The method may not
look dangerous, but it is! The type of this array is determined by the compiletime types of the arguments passed in to the method, and the compiler may
not have enough information to make an accurate determination. Because this
method returns its varargs parameter array, it can propagate heap pollution up
the call stack.
To make this concrete, consider the following generic method, which takes
three arguments of type T and returns an array containing two of the
arguments, chosen at random:
Click here to view code image
static <T> T[] pickTwo(T a, T b, T c) {
switch(ThreadLocalRandom.current().nextInt(3)) {
case 0: return toArray(a, b);
case 1: return toArray(a, c);
case 2: return toArray(b, c);
}
throw new AssertionError(); // Can't get here
}
This method is not, in and of itself, dangerous and would not generate a
warning except that it invokes the toArray method, which has a generic
174
varargs parameter.
When compiling this method, the compiler generates code to create a
varargs parameter array in which to pass two T instances to toArray. This
code allocates an array of type Object[], which is the most specific type
that is guaranteed to hold these instances, no matter what types of objects are
passed to pickTwo at the call site. The toArray method simply returns
this array to pickTwo, which in turn returns it to its caller, so pickTwo will
always return an array of type Object[].
Now consider this main method, which exercises pickTwo:
Click here to view code image
public static void main(String[] args) {
String[] attributes = pickTwo("Good", "Fast",
"Cheap");
}
There is nothing at all wrong with this method, so it compiles without
generating any warnings. But when you run it, it throws a
ClassCastException, though it contains no visible casts. What you
don’t see is that the compiler has generated a hidden cast to String[] on
the value returned by pickTwo so that it can be stored in attributes.
The cast fails, because Object[] is not a subtype of String[]. This
failure is quite disconcerting because it is two levels removed from the
method that actually causes the heap pollution (toArray), and the varargs
parameter array is not modified after the actual parameters are stored in it.
This example is meant to drive home the point that it is unsafe to give
another method access to a generic varargs parameter array, with two
exceptions: it is safe to pass the array to another varargs method that is
correctly annotated with @SafeVarargs, and it is safe to pass the array to a
non-varargs method that merely computes some function of the contents of
the array.
Here is a typical example of a safe use of a generic varargs parameter. This
method takes an arbitrary number of lists as arguments and returns a single
list containing the elements of all of the input lists in sequence. Because the
method is annotated with @SafeVarargs, it doesn’t generate any warnings,
on the declaration or at its call sites:
Click here to view code image
// Safe method with a generic varargs parameter
@SafeVarargs
static <T> List<T> flatten(List<? extends T>... lists) {
List<T> result = new ArrayList<>();
for (List<? extends T> list : lists)
175
result.addAll(list);
return result;
}
The rule for deciding when to use the SafeVarargs annotation is
simple: Use @SafeVarargs on every method with a varargs parameter
of a generic or parameterized type, so its users won’t be burdened by
needless and confusing compiler warnings. This implies that you should never
write unsafe varargs methods like dangerous or toArray. Every time the
compiler warns you of possible heap pollution from a generic varargs
parameter in a method you control, check that the method is safe. As a
reminder, a generic varargs methods is safe if:
1. it doesn’t store anything in the varargs parameter array, and
2. it doesn’t make the array (or a clone) visible to untrusted code. If either
of these prohibitions is violated, fix it.
Note that the SafeVarargs annotation is legal only on methods that
can’t be overridden, because it is impossible to guarantee that every possible
overriding method will be safe. In Java 8, the annotation was legal only on
static methods and final instance methods; in Java 9, it became legal on
private instance methods as well.
An alternative to using the SafeVarargs annotation is to take the advice
of Item 28 and replace the varargs parameter (which is an array in disguise)
with a List parameter. Here’s how this approach looks when applied to our
flatten method. Note that only the parameter declaration has changed:
Click here to view code image
// List as a typesafe alternative to a generic varargs
parameter
static <T> List<T> flatten(List<List<? extends T>>
lists) {
List<T> result = new ArrayList<>();
for (List<? extends T> list : lists)
result.addAll(list);
return result;
}
This method can then be used in conjunction with the static factory method
List.of to allow for a variable number of arguments. Note that this
approach relies on the fact that the List.of declaration is annotated with
@SafeVarargs:
Click here to view code image
audience = flatten(List.of(friends, romans,
countrymen));
176
The advantage of this approach is that the compiler can prove that the
method is typesafe. You don’t have to vouch for its safety with a
SafeVarargs annotation, and you don’t have worry that you might have
erred in determining that it was safe. The main disadvantage is that the client
code is a bit more verbose and may be a bit slower.
This trick can also be used in situations where it is impossible to write a
safe varargs method, as is the case with the toArray method on page 147.
Its List analogue is the List.of method, so we don’t even have to write
it; the Java libraries authors have done the work for us. The pickTwo
method then becomes this:
Click here to view code image
static <T> List<T> pickTwo(T a, T b, T c) {
switch(rnd.nextInt(3)) {
case 0: return List.of(a, b);
case 1: return List.of(a, c);
case 2: return List.of(b, c);
}
throw new AssertionError();
}
and the main method becomes this:
Click here to view code image
public static void main(String[] args) {
List<String> attributes = pickTwo("Good", "Fast",
"Cheap");
}
The resulting code is typesafe because it uses only generics, and not arrays.
In summary, varargs and generics do not interact well because the varargs
facility is a leaky abstraction built atop arrays, and arrays have different type
rules from generics. Though generic varargs parameters are not typesafe, they
are legal. If you choose to write a method with a generic (or parameterized)
varargs parameter, first ensure that the method is typesafe, and then annotate
it with @SafeVarargs so it is not unpleasant to use.
Item 33: Consider typesafe heterogeneous containers
Common uses of generics include collections, such as Set<E> and
Map<K,V>, and single-element containers, such as ThreadLocal<T> and
AtomicReference<T>. In all of these uses, it is the container that is
parameterized. This limits you to a fixed number of type parameters per
container. Normally that is exactly what you want. A Set has a single type
177
parameter, representing its element type; a Map has two, representing its key
and value types; and so forth.
Sometimes, however, you need more flexibility. For example, a database
row can have arbitrarily many columns, and it would be nice to be able to
access all of them in a typesafe manner. Luckily, there is an easy way to
achieve this effect. The idea is to parameterize the key instead of the
container. Then present the parameterized key to the container to insert or
retrieve a value. The generic type system is used to guarantee that the type of
the value agrees with its key.
As a simple example of this approach, consider a Favorites class that
allows its clients to store and retrieve a favorite instance of arbitrarily many
types. The Class object for the type will play the part of the parameterized
key. The reason this works is that class Class is generic. The type of a class
literal is not simply Class, but Class<T>. For example, String.class
is of type Class<String>, and Integer.class is of type
Class<Integer>. When a class literal is passed among methods to
communicate both compile-time and runtime type information, it is called a
type token [Bracha04].
The API for the Favorites class is simple. It looks just like a simple
map, except that the key is parameterized instead of the map. The client
presents a Class object when setting and getting favorites. Here is the API:
Click here to view code image
// Typesafe heterogeneous container pattern - API
public class Favorites {
public <T> void putFavorite(Class<T> type, T
instance);
public <T> T getFavorite(Class<T> type);
}
Here is a sample program that exercises the Favorites class, storing,
retrieving, and printing a favorite String, Integer, and Class instance:
Click here to view code image
// Typesafe heterogeneous container pattern - client
public static void main(String[] args) {
Favorites f = new Favorites();
f.putFavorite(String.class, "Java");
f.putFavorite(Integer.class, 0xcafebabe);
f.putFavorite(Class.class, Favorites.class);
String favoriteString = f.getFavorite(String.class);
int favoriteInteger = f.getFavorite(Integer.class);
Class<?> favoriteClass = f.getFavorite(Class.class);
System.out.printf("%s %x %s%n", favoriteString,
178
favoriteInteger, favoriteClass.getName());
}
As you would expect, this program prints Java cafebabe Favorites.
Note, incidentally, that Java’s printf method differs from C’s in that you
should use %n where you’d use \n in C. The %n generates the applicable
platform-specific line separator, which is \n on many but not all platforms.
A Favorites instance is typesafe: it will never return an Integer
when you ask it for a String. It is also heterogeneous: unlike an ordinary
map, all the keys are of different types. Therefore, we call Favorites a
typesafe heterogeneous container.
The implementation of Favorites is surprisingly tiny. Here it is, in its
entirety:
Click here to view code image
// Typesafe heterogeneous container pattern -
implementation
public class Favorites {
private Map<Class<?>, Object> favorites = new
HashMap<>();
public <T> void putFavorite(Class<T> type, T
instance) {
favorites.put(Objects.requireNonNull(type),
instance);
}
public <T> T getFavorite(Class<T> type) {
return type.cast(favorites.get(type));
}
}
There are a few subtle things going on here. Each Favorites instance is
backed by a private Map<Class<?>, Object> called favorites. You
might think that you couldn’t put anything into this Map because of the
unbounded wildcard type, but the truth is quite the opposite. The thing to
notice is that the wildcard type is nested: it’s not the type of the map that’s a
wildcard type but the type of its key. This means that every key can have a
different parameterized type: one can be Class<String>, the next
Class<Integer>, and so on. That’s where the heterogeneity comes from.
The next thing to notice is that the value type of the favorites Map is
simply Object. In other words, the Map does not guarantee the type
relationship between keys and values, which is that every value is of the type
represented by its key. In fact, Java’s type system is not powerful enough to
179
express this. But we know that it’s true, and we take advantage of it when the
time comes to retrieve a favorite.
The putFavorite implementation is trivial: it simply puts into
favorites a mapping from the given Class object to the given favorite
instance. As noted, this discards the “type linkage” between the key and the
value; it loses the knowledge that the value is an instance of the key. But
that’s OK, because the getFavorites method can and does reestablish
this linkage.
The implementation of getFavorite is trickier than that of
putFavorite. First, it gets from the favorites map the value
corresponding to the given Class object. This is the correct object reference
to return, but it has the wrong compile-time type: it is Object (the value
type of the favorites map) and we need to return a T. So, the
getFavorite implementation dynamically casts the object reference to the
type represented by the Class object, using Class’s cast method.
The cast method is the dynamic analogue of Java’s cast operator. It
simply checks that its argument is an instance of the type represented by the
Class object. If so, it returns the argument; otherwise it throws a
ClassCastException. We know that the cast invocation in
getFavorite won’t throw ClassCastException, assuming the client
code compiled cleanly. That is to say, we know that the values in the
favorites map always match the types of their keys.
So what does the cast method do for us, given that it simply returns its
argument? The signature of the cast method takes full advantage of the fact
that class Class is generic. Its return type is the type parameter of the
Class object:
Click here to view code image
public class Class<T> {
T cast(Object obj);
}
This is precisely what’s needed by the getFavorite method. It is what
allows us to make Favorites typesafe without resorting to an unchecked
cast to T.
There are two limitations to the Favorites class that are worth noting.
First, a malicious client could easily corrupt the type safety of a Favorites
instance, by using a Class object in its raw form. But the resulting client
code would generate an unchecked warning when it was compiled. This is no
different from a normal collection implementations such as HashSet and
HashMap. You can easily put a String into a HashSet<Integer> by
180
using the raw type HashSet (Item 26). That said, you can have runtime type
safety if you’re willing to pay for it. The way to ensure that Favorites
never violates its type invariant is to have the putFavorite method check
that instance is actually an instance of the type represented by type, and
we already know how to do this. Just use a dynamic cast:
Click here to view code image
// Achieving runtime type safety with a dynamic cast
public <T> void putFavorite(Class<T> type, T instance) {
favorites.put(type, type.cast(instance));
}
There are collection wrappers in java.util.Collections that play
the same trick. They are called checkedSet, checkedList,
checkedMap, and so forth. Their static factories take a Class object (or
two) in addition to a collection (or map). The static factories are generic
methods, ensuring that the compile-time types of the Class object and the
collection match. The wrappers add reification to the collections they wrap.
For example, the wrapper throws a ClassCastException at runtime if
someone tries to put a Coin into your Collection<Stamp>. These
wrappers are useful for tracking down client code that adds an incorrectly
typed element to a collection, in an application that mixes generic and raw
types.
The second limitation of the Favorites class is that it cannot be used on
a non-reifiable type (Item 28). In other words, you can store your favorite
String or String[], but not your favorite List<String>. If you try to
store your favorite List<String>, your program won’t compile. The
reason is that you can’t get a Class object for List<String>. The class
literal List<String>.class is a syntax error, and it’s a good thing, too.
List<String> and List<Integer> share a single Class object,
which is List.class. It would wreak havoc with the internals of a
Favorites object if the “type literals” List<String>.class and
List<Integer>.class were legal and returned the same object
reference. There is no entirely satisfactory workaround for this limitation.
The type tokens used by Favorites are unbounded: getFavorite
and put-Favorite accept any Class object. Sometimes you may need to
limit the types that can be passed to a method. This can be achieved with a
bounded type token, which is simply a type token that places a bound on what
type can be represented, using a bounded type parameter (Item 30) or a
bounded wildcard (Item 31).
The annotations API (Item 39) makes extensive use of bounded type
tokens. For example, here is the method to read an annotation at runtime. This
181
method comes from the AnnotatedElement interface, which is
implemented by the reflective types that represent classes, methods, fields,
and other program elements:
Click here to view code image
public <T extends Annotation>
T getAnnotation(Class<T> annotationType);
The argument, annotationType, is a bounded type token representing an
annotation type. The method returns the element’s annotation of that type, if it
has one, or null, if it doesn’t. In essence, an annotated element is a typesafe
heterogeneous container whose keys are annotation types.
Suppose you have an object of type Class<?> and you want to pass it to
a method that requires a bounded type token, such as getAnnotation.
You could cast the object to Class<? extends Annotation>, but this
cast is unchecked, so it would generate a compile-time warning (Item 27).
Luckily, class Class provides an instance method that performs this sort of
cast safely (and dynamically). The method is called asSubclass, and it
casts the Class object on which it is called to represent a subclass of the
class represented by its argument. If the cast succeeds, the method returns its
argument; if it fails, it throws a ClassCastException.
Here’s how you use the asSubclass method to read an annotation
whose type is unknown at compile time. This method compiles without error
or warning:
Click here to view code image
// Use of asSubclass to safely cast to a bounded type
token
static Annotation getAnnotation(AnnotatedElement
element,
String
annotationTypeName) {
Class<?> annotationType = null; // Unbounded type
token
try {
annotationType =
Class.forName(annotationTypeName);
} catch (Exception ex) {
throw new IllegalArgumentException(ex);
}
return element.getAnnotation(
annotationType.asSubclass(Annotation.class));
}
In summary, the normal use of generics, exemplified by the collections
182
APIs, restricts you to a fixed number of type parameters per container. You
can get around this restriction by placing the type parameter on the key rather
than the container. You can use Class objects as keys for such typesafe
heterogeneous containers. A Class object used in this fashion is called a
type token. You can also use a custom key type. For example, you could have
a DatabaseRow type representing a database row (the container), and a
generic type Column<T> as its key.
183
Chapter 6. Enums and Annotations
JAVA supports two special-purpose families of reference types: a kind of class
called an enum type, and a kind of interface called an annotation type. This
chapter discusses best practices for using these type families.
Item 34: Use enums instead of int constants
An enumerated type is a type whose legal values consist of a fixed set of
constants, such as the seasons of the year, the planets in the solar system, or
the suits in a deck of playing cards. Before enum types were added to the
language, a common pattern for representing enumerated types was to declare
a group of named int constants, one for each member of the type:
Click here to view code image
// The int enum pattern - severely deficient!
public static final int APPLE_FUJI = 0;
public static final int APPLE_PIPPIN = 1;
public static final int APPLE_GRANNY_SMITH = 2;
public static final int ORANGE_NAVEL = 0;
public static final int ORANGE_TEMPLE = 1;
public static final int ORANGE_BLOOD = 2;
This technique, known as the int enum pattern, has many shortcomings. It
provides nothing in the way of type safety and little in the way of expressive
power. The compiler won’t complain if you pass an apple to a method that
expects an orange, compare apples to oranges with the == operator, or worse:
Click here to view code image
// Tasty citrus flavored applesauce!
int i = (APPLE_FUJI - ORANGE_TEMPLE) / APPLE_PIPPIN;
Note that the name of each apple constant is prefixed with APPLE_ and the
name of each orange constant is prefixed with ORANGE_. This is because
Java doesn’t provide namespaces for int enum groups. Prefixes prevent
name clashes when two int enum groups have identically named constants,
for example between ELEMENT_MERCURY and PLANET_MERCURY.
Programs that use int enums are brittle. Because int enums are constant
variables [JLS, 4.12.4], their int values are compiled into the clients that use
them [JLS, 13.1]. If the value associated with an int enum is changed, its
clients must be recompiled. If not, the clients will still run, but their behavior
184
will be incorrect.
There is no easy way to translate int enum constants into printable
strings. If you print such a constant or display it from a debugger, all you see
is a number, which isn’t very helpful. There is no reliable way to iterate over
all the int enum constants in a group, or even to obtain the size of an int
enum group.
You may encounter a variant of this pattern in which String constants
are used in place of int constants. This variant, known as the String enum
pattern, is even less desirable. While it does provide printable strings for its
constants, it can lead naive users to hard-code string constants into client code
instead of using field names. If such a hard-coded string constant contains a
typographical error, it will escape detection at compile time and result in bugs
at runtime. Also, it might lead to performance problems, because it relies on
string comparisons.
Luckily, Java provides an alternative that avoids all the shortcomings of the
int and string enum patterns and provides many added benefits. It is the
enum type [JLS, 8.9]. Here’s how it looks in its simplest form:
Click here to view code image
public enum Apple { FUJI, PIPPIN, GRANNY_SMITH }
public enum Orange { NAVEL, TEMPLE, BLOOD }
On the surface, these enum types may appear similar to those of other
languages, such as C, C++, and C#, but appearances are deceiving. Java’s
enum types are full-fledged classes, far more powerful than their counterparts
in these other languages, where enums are essentially int values.
The basic idea behind Java’s enum types is simple: they are classes that
export one instance for each enumeration constant via a public static final
field. Enum types are effectively final, by virtue of having no accessible
constructors. Because clients can neither create instances of an enum type nor
extend it, there can be no instances but the declared enum constants. In other
words, enum types are instance-controlled (page 6). They are a generalization
of singletons (Item 3), which are essentially single-element enums.
Enums provide compile-time type safety. If you declare a parameter to be
of type Apple, you are guaranteed that any non-null object reference passed
to the parameter is one of the three valid Apple values. Attempts to pass
values of the wrong type will result in compile-time errors, as will attempts to
assign an expression of one enum type to a variable of another, or to use the
== operator to compare values of different enum types.
Enum types with identically named constants coexist peacefully because
each type has its own namespace. You can add or reorder constants in an
enum type without recompiling its clients because the fields that export the
185
constants provide a layer of insulation between an enum type and its clients:
constant values are not compiled into the clients as they are in the int enum
patterns. Finally, you can translate enums into printable strings by calling
their toString method.
In addition to rectifying the deficiencies of int enums, enum types let you
add arbitrary methods and fields and implement arbitrary interfaces. They
provide high-quality implementations of all the Object methods (Chapter
3), they implement Comparable (Item 14) and Serializable (Chapter
12), and their serialized form is designed to withstand most changes to the
enum type.
So why would you want to add methods or fields to an enum type? For
starters, you might want to associate data with its constants. Our Apple and
Orange types, for example, might benefit from a method that returns the
color of the fruit, or one that returns an image of it. You can augment an
enum type with any method that seems appropriate. An enum type can start
life as a simple collection of enum constants and evolve over time into a fullfeatured abstraction.
For a nice example of a rich enum type, consider the eight planets of our
solar system. Each planet has a mass and a radius, and from these two
attributes you can compute its surface gravity. This in turn lets you compute
the weight of an object on the planet’s surface, given the mass of the object.
Here’s how this enum looks. The numbers in parentheses after each enum
constant are parameters that are passed to its constructor. In this case, they are
the planet’s mass and radius:
Click here to view code image
// Enum type with data and behavior
public enum Planet {
MERCURY(3.302e+23, 2.439e6),
VENUS (4.869e+24, 6.052e6),
EARTH (5.975e+24, 6.378e6),
MARS (6.419e+23, 3.393e6),
JUPITER(1.899e+27, 7.149e7),
SATURN (5.685e+26, 6.027e7),
URANUS (8.683e+25, 2.556e7),
NEPTUNE(1.024e+26, 2.477e7);
private final double mass; // In kilograms
private final double radius; // In meters
private final double surfaceGravity; // In m / s^2
// Universal gravitational constant in m^3 / kg s^2
private static final double G = 6.67300E-11;
186
// Constructor
Planet(double mass, double radius) {
this.mass = mass;
this.radius = radius;
surfaceGravity = G * mass / (radius * radius);
}
public double mass() { return mass; }
public double radius() { return radius; }
public double surfaceGravity() { return
surfaceGravity; }
public double surfaceWeight(double mass) {
return mass * surfaceGravity; // F = ma
}
}
It is easy to write a rich enum type such as Planet. To associate data
with enum constants, declare instance fields and write a constructor that
takes the data and stores it in the fields. Enums are by their nature
immutable, so all fields should be final (Item 17). Fields can be public, but it
is better to make them private and provide public accessors (Item 16). In the
case of Planet, the constructor also computes and stores the surface gravity,
but this is just an optimization. The gravity could be recomputed from the
mass and radius each time it was used by the surfaceWeight method,
which takes an object’s mass and returns its weight on the planet represented
by the constant.
While the Planet enum is simple, it is surprisingly powerful. Here is a
short program that takes the earth weight of an object (in any unit) and prints
a nice table of the object’s weight on all eight planets (in the same unit):
Click here to view code image
public class WeightTable {
public static void main(String[] args) {
double earthWeight = Double.parseDouble(args[0]);
double mass = earthWeight /
Planet.EARTH.surfaceGravity();
for (Planet p : Planet.values())
System.out.printf("Weight on %s is %f%n",
p, p.surfaceWeight(mass));
}
}
Note that Planet, like all enums, has a static values method that returns
an array of its values in the order they were declared. Note also that the
toString method returns the declared name of each enum value, enabling
187
easy printing by println and printf. If you’re dissatisfied with this
string representation, you can change it by overriding the toString
method. Here is the result of running our WeightTable program (which
doesn’t override toString) with the command line argument 185:
Click here to view code image
Weight on MERCURY is 69.912739
Weight on VENUS is 167.434436
Weight on EARTH is 185.000000
Weight on MARS is 70.226739
Weight on JUPITER is 467.990696
Weight on SATURN is 197.120111
Weight on URANUS is 167.398264
Weight on NEPTUNE is 210.208751
Until 2006, two years after enums were added to Java, Pluto was a planet.
This raises the question “what happens when you remove an element from an
enum type?” The answer is that any client program that doesn’t refer to the
removed element will continue to work fine. So, for example, our
WeightTable program would simply print a table with one fewer row. And
what of a client program that refers to the removed element (in this case,
Planet.Pluto)? If you recompile the client program, the compilation will
fail with a helpful error message at the line that refers to the erstwhile planet;
if you fail to recompile the client, it will throw a helpful exception from this
line at runtime. This is the best behavior you could hope for, far better than
what you’d get with the int enum pattern.
Some behaviors associated with enum constants may need to be used only
from within the class or package in which the enum is defined. Such
behaviors are best implemented as private or package-private methods. Each
constant then carries with it a hidden collection of behaviors that allows the
class or package containing the enum to react appropriately when presented
with the constant. Just as with other classes, unless you have a compelling
reason to expose an enum method to its clients, declare it private or, if need
be, package-private (Item 15).
If an enum is generally useful, it should be a top-level class; if its use is tied
to a specific top-level class, it should be a member class of that top-level class
(Item 24). For example, the java.math.RoundingMode enum represents
a rounding mode for decimal fractions. These rounding modes are used by the
BigDecimal class, but they provide a useful abstraction that is not
fundamentally tied to BigDecimal. By making RoundingMode a toplevel enum, the library designers encourage any programmer who needs
rounding modes to reuse this enum, leading to increased consistency across
APIs.
188
The techniques demonstrated in the Planet example are sufficient for
most enum types, but sometimes you need more. There is different data
associated with each Planet constant, but sometimes you need to associate
fundamentally different behavior with each constant. For example, suppose
you are writing an enum type to represent the operations on a basic fourfunction calculator and you want to provide a method to perform the
arithmetic operation represented by each constant. One way to achieve this is
to switch on the value of the enum:
Click here to view code image
// Enum type that switches on its own value -
questionable
public enum Operation {
PLUS, MINUS, TIMES, DIVIDE;
// Do the arithmetic operation represented by this
constant
public double apply(double x, double y) {
switch(this) {
case PLUS: return x + y;
case MINUS: return x - y;
case TIMES: return x * y;
case DIVIDE: return x / y;
}
throw new AssertionError("Unknown op: " + this);
}
}
This code works, but it isn’t very pretty. It won’t compile without the
throw statement because the end of the method is technically reachable,
even though it will never be reached [JLS, 14.21]. Worse, the code is fragile.
If you add a new enum constant but forget to add a corresponding case to the
switch, the enum will still compile, but it will fail at runtime when you try
to apply the new operation.
Luckily, there is a better way to associate a different behavior with each
enum constant: declare an abstract apply method in the enum type, and
override it with a concrete method for each constant in a constant-specific
class body. Such methods are known as constant-specific method
implementations:
Click here to view code image
// Enum type with constant-specific method
implementations
public enum Operation {
PLUS {public double apply(double x, double y){return
189
x + y;}},
MINUS {public double apply(double x, double y){return
x - y;}},
TIMES {public double apply(double x, double y){return
x * y;}},
DIVIDE{public double apply(double x, double y){return
x / y;}};
public abstract double apply(double x, double y);
}
If you add a new constant to the second version of Operation, it is
unlikely that you’ll forget to provide an apply method, because the method
immediately follows each constant declaration. In the unlikely event that you
do forget, the compiler will remind you because abstract methods in an enum
type must be overridden with concrete methods in all of its constants.
Constant-specific method implementations can be combined with constantspecific data. For example, here is a version of Operation that overrides
the toString method to return the symbol commonly associated with the
operation:
Click here to view code image
// Enum type with constant-specific class bodies and
data
public enum Operation {
PLUS("+") {
public double apply(double x, double y) { return
x + y; }
},
MINUS("-") {
public double apply(double x, double y) { return
x - y; }
},
TIMES("*") {
public double apply(double x, double y) { return
x * y; }
},
DIVIDE("/") {
public double apply(double x, double y) { return
x / y; }
};
private final String symbol;
Operation(String symbol) { this.symbol = symbol; }
@Override public String toString() { return symbol;
190
}
public abstract double apply(double x, double y);
}
The toString implementation shown makes it easy to print arithmetic
expressions, as demonstrated by this little program:
Click here to view code image
public static void main(String[] args) {
double x = Double.parseDouble(args[0]);
double y = Double.parseDouble(args[1]);
for (Operation op : Operation.values())
System.out.printf("%f %s %f = %f%n",
x, op, y, op.apply(x, y));
}
Running this program with 2 and 4 as command line arguments produces
the following output:
Click here to view code image
2.000000 + 4.000000 = 6.000000
2.000000 - 4.000000 = -2.000000
2.000000 * 4.000000 = 8.000000
2.000000 / 4.000000 = 0.500000
Enum types have an automatically generated valueOf(String)
method that translates a constant’s name into the constant itself. If you
override the toString method in an enum type, consider writing a
fromString method to translate the custom string representation back to
the corresponding enum. The following code (with the type name changed
appropriately) will do the trick for any enum, so long as each constant has a
unique string representation:
Click here to view code image
// Implementing a fromString method on an enum type
private static final Map<String, Operation> stringToEnum
=
Stream.of(values()).collect(
toMap(Object::toString, e -> e));
// Returns Operation for string, if any
public static Optional<Operation> fromString(String
symbol) {
return
Optional.ofNullable(stringToEnum.get(symbol));
}
191
Note that the Operation constants are put into the stringToEnum
map from a static field initialization that runs after the enum constants have
been created. The previous code uses a stream (Chapter 7) over the array
returned by the values() method; prior to Java 8, we would have created
an empty hash map and iterated over the values array inserting the string-toenum mappings into the map, and you can still do it that way if you prefer.
But note that attempting to have each constant put itself into a map from its
own constructor does not work. It would cause a compilation error, which is
good thing because if it were legal, it would cause a
NullPointerException at runtime. Enum constructors aren’t permitted
to access the enum’s static fields, with the exception of constant variables
(Item 34). This restriction is necessary because static fields have not yet been
initialized when enum constructors run. A special case of this restriction is
that enum constants cannot access one another from their constructors.
Also note that the fromString method returns an
Optional<String>. This allows the method to indicate that the string
that was passed in does not represent a valid operation, and it forces the client
to confront this possibility (Item 55).
A disadvantage of constant-specific method implementations is that they
make it harder to share code among enum constants. For example, consider an
enum representing the days of the week in a payroll package. This enum has a
method that calculates a worker’s pay for that day given the worker’s base
salary (per hour) and the number of minutes worked on that day. On the five
weekdays, any time worked in excess of a normal shift generates overtime
pay; on the two weekend days, all work generates overtime pay. With a
switch statement, it’s easy to do this calculation by applying multiple case
labels to each of two code fragments:
Click here to view code image
// Enum that switches on its value to share code -
questionable
enum PayrollDay {
MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY,
SATURDAY, SUNDAY;
private static final int MINS_PER_SHIFT = 8 * 60;
int pay(int minutesWorked, int payRate) {
int basePay = minutesWorked * payRate;
int overtimePay;
switch(this) {
case SATURDAY: case SUNDAY: // Weekend
overtimePay = basePay / 2;
192
break;
default: // Weekday
overtimePay = minutesWorked <=
MINS_PER_SHIFT ?
0 : (minutesWorked - MINS_PER_SHIFT) *
payRate / 2;
}
return basePay + overtimePay;
}
}
This code is undeniably concise, but it is dangerous from a maintenance
perspective. Suppose you add an element to the enum, perhaps a special value
to represent a vacation day, but forget to add a corresponding case to the
switch statement. The program will still compile, but the pay method will
silently pay the worker the same amount for a vacation day as for an ordinary
weekday.
To perform the pay calculation safely with constant-specific method
implementations, you would have to duplicate the overtime pay computation
for each constant, or move the computation into two helper methods, one for
weekdays and one for weekend days, and invoke the appropriate helper
method from each constant. Either approach would result in a fair amount of
boilerplate code, substantially reducing readability and increasing the
opportunity for error.
The boilerplate could be reduced by replacing the abstract overtimePay
method on PayrollDay with a concrete method that performs the overtime
calculation for weekdays. Then only the weekend days would have to
override the method. But this would have the same disadvantage as the
switch statement: if you added another day without overriding the
overtimePay method, you would silently inherit the weekday calculation.
What you really want is to be forced to choose an overtime pay strategy
each time you add an enum constant. Luckily, there is a nice way to achieve
this. The idea is to move the overtime pay computation into a private nested
enum, and to pass an instance of this strategy enum to the constructor for the
PayrollDay enum. The PayrollDay enum then delegates the overtime
pay calculation to the strategy enum, eliminating the need for a switch
statement or constant-specific method implementation in PayrollDay.
While this pattern is less concise than the switch statement, it is safer and
more flexible:
Click here to view code image
// The strategy enum pattern
enum PayrollDay {
193
MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY,
SATURDAY(PayType.WEEKEND), SUNDAY(PayType.WEEKEND);
private final PayType payType;
PayrollDay(PayType payType) { this.payType =
payType; }
PayrollDay() { this(PayType.WEEKDAY); } // Default
int pay(int minutesWorked, int payRate) {
return payType.pay(minutesWorked, payRate);
}
// The strategy enum type
private enum PayType {
WEEKDAY {
int overtimePay(int minsWorked, int payRate)
{
return minsWorked <= MINS_PER_SHIFT ? 0
:
(minsWorked - MINS_PER_SHIFT) *
payRate / 2;
}
},
WEEKEND {
int overtimePay(int minsWorked, int payRate)
{
return minsWorked * payRate / 2;
}
};
abstract int overtimePay(int mins, int payRate);
private static final int MINS_PER_SHIFT = 8 *
60;
int pay(int minsWorked, int payRate) {
int basePay = minsWorked * payRate;
return basePay + overtimePay(minsWorked,
payRate);
}
}
}
If switch statements on enums are not a good choice for implementing
constant-specific behavior on enums, what are they good for? Switches on
enums are good for augmenting enum types with constant-specific
behavior. For example, suppose the Operation enum is not under your
control and you wish it had an instance method to return the inverse of each
194
operation. You could simulate the effect with the following static method:
Click here to view code image
// Switch on an enum to simulate a missing method
public static Operation inverse(Operation op) {
switch(op) {
case PLUS: return Operation.MINUS;
case MINUS: return Operation.PLUS;
case TIMES: return Operation.DIVIDE;
case DIVIDE: return Operation.TIMES;
default: throw new AssertionError("Unknown op:
" + op);
}
}
You should also use this technique on enum types that are under your control
if a method simply doesn’t belong in the enum type. The method may be
required for some use but is not generally useful enough to merit inclusion in
the enum type.
Enums are, generally speaking, comparable in performance to int
constants. A minor performance disadvantage of enums is that there is a space
and time cost to load and initialize enum types, but it is unlikely to be
noticeable in practice.
So when should you use enums? Use enums any time you need a set of
constants whose members are known at compile time. Of course, this
includes “natural enumerated types,” such as the planets, the days of the
week, and the chess pieces. But it also includes other sets for which you know
all the possible values at compile time, such as choices on a menu, operation
codes, and command line flags. It is not necessary that the set of constants
in an enum type stay fixed for all time. The enum feature was specifically
designed to allow for binary compatible evolution of enum types.
In summary, the advantages of enum types over int constants are
compelling. Enums are more readable, safer, and more powerful. Many
enums require no explicit constructors or members, but others benefit from
associating data with each constant and providing methods whose behavior is
affected by this data. Fewer enums benefit from associating multiple
behaviors with a single method. In this relatively rare case, prefer constantspecific methods to enums that switch on their own values. Consider the
strategy enum pattern if some, but not all, enum constants share common
behaviors.
Item 35: Use instance fields instead of ordinals
195
Many enums are naturally associated with a single int value. All enums
have an ordinal method, which returns the numerical position of each
enum constant in its type. You may be tempted to derive an associated int
value from the ordinal:
Click here to view code image
// Abuse of ordinal to derive an associated value -
DON'T DO THIS
public enum Ensemble {
SOLO, DUET, TRIO, QUARTET, QUINTET,
SEXTET, SEPTET, OCTET, NONET, DECTET;
public int numberOfMusicians() { return ordinal() +
1; }
}
While this enum works, it is a maintenance nightmare. If the constants are
reordered, the numberOfMusicians method will break. If you want to
add a second enum constant associated with an int value that you’ve already
used, you’re out of luck. For example, it might be nice to add a constant for
double quartet, which, like an octet, consists of eight musicians, but there is
no way to do it.
Also, you can’t add a constant for an int value without adding constants
for all intervening int values. For example, suppose you want to add a
constant representing a triple quartet, which consists of twelve musicians.
There is no standard term for an ensemble consisting of eleven musicians, so
you are forced to add a dummy constant for the unused int value (11). At
best, this is ugly. If many int values are unused, it’s impractical.
Luckily, there is a simple solution to these problems. Never derive a value
associated with an enum from its ordinal; store it in an instance field
instead:
Click here to view code image
public enum Ensemble {
SOLO(1), DUET(2), TRIO(3), QUARTET(4), QUINTET(5),
SEXTET(6), SEPTET(7), OCTET(8), DOUBLE_QUARTET(8),
NONET(9), DECTET(10), TRIPLE_QUARTET(12);
private final int numberOfMusicians;
Ensemble(int size) { this.numberOfMusicians = size;
}
public int numberOfMusicians() { return
numberOfMusicians; }
}
196
The Enum specification has this to say about ordinal: “Most
programmers will have no use for this method. It is designed for use by
general-purpose enum-based data structures such as EnumSet and
EnumMap.” Unless you are writing code with this character, you are best off
avoiding the ordinal method entirely.
Item 36: Use EnumSet instead of bit fields
If the elements of an enumerated type are used primarily in sets, it is
traditional to use the int enum pattern (Item 34), assigning a different power
of 2 to each constant:
Click here to view code image
// Bit field enumeration constants - OBSOLETE!
public class Text {
public static final int STYLE_BOLD = 1 <<
0; // 1
public static final int STYLE_ITALIC = 1 <<
1; // 2
public static final int STYLE_UNDERLINE = 1 <<
2; // 4
public static final int STYLE_STRIKETHROUGH = 1 <<
3; // 8
// Parameter is bitwise OR of zero or more STYLE_
constants
public void applyStyles(int styles) { ... }
}
This representation lets you use the bitwise OR operation to combine several
constants into a set, known as a bit field:
Click here to view code image
text.applyStyles(STYLE_BOLD | STYLE_ITALIC);
The bit field representation also lets you perform set operations such as
union and intersection efficiently using bitwise arithmetic. But bit fields have
all the disadvantages of int enum constants and more. It is even harder to
interpret a bit field than a simple int enum constant when it is printed as a
number. There is no easy way to iterate over all of the elements represented
by a bit field. Finally, you have to predict the maximum number of bits you’ll
ever need at the time you’re writing the API and choose a type for the bit field
(typically int or long) accordingly. Once you’ve picked a type, you can’t
exceed its width (32 or 64 bits) without changing the API.
197
Some programmers who use enums in preference to int constants still
cling to the use of bit fields when they need to pass around sets of constants.
There is no reason to do this, because a better alternative exists. The
java.util package provides the EnumSet class to efficiently represent
sets of values drawn from a single enum type. This class implements the Set
interface, providing all of the richness, type safety, and interoperability you
get with any other Set implementation. But internally, each EnumSet is
represented as a bit vector. If the underlying enum type has sixty-four or
fewer elements—and most do—the entire EnumSet is represented with a
single long, so its performance is comparable to that of a bit field. Bulk
operations, such as removeAll and retainAll, are implemented using
bitwise arithmetic, just as you’d do manually for bit fields. But you are
insulated from the ugliness and error-proneness of manual bit twiddling: the
EnumSet does the hard work for you.
Here is how the previous example looks when modified to use enums and
enum sets instead of bit fields. It is shorter, clearer, and safer:
Click here to view code image
// EnumSet - a modern replacement for bit fields
public class Text {
public enum Style { BOLD, ITALIC, UNDERLINE,
STRIKETHROUGH }
// Any Set could be passed in, but EnumSet is
clearly best
public void applyStyles(Set<Style> styles) { ... }
}
Here is client code that passes an EnumSet instance to the applyStyles
method. The EnumSet class provides a rich set of static factories for easy set
creation, one of which is illustrated in this code:
Click here to view code image
text.applyStyles(EnumSet.of(Style.BOLD, Style.ITALIC));
Note that the applyStyles method takes a Set<Style> rather than an
EnumSet<Style>. While it seems likely that all clients would pass an
EnumSet to the method, it is generally good practice to accept the interface
type rather than the implementation type (Item 64). This allows for the
possibility of an unusual client to pass in some other Set implementation.
In summary, just because an enumerated type will be used in sets, there
is no reason to represent it with bit fields. The EnumSet class combines
the conciseness and performance of bit fields with all the many advantages of
198
enum types described in Item 34. The one real disadvantage of EnumSet is
that it is not, as of Java 9, possible to create an immutable EnumSet, but this
will likely be remedied in an upcoming release. In the meantime, you can
wrap an EnumSet with Collections.unmodifiableSet, but
conciseness and performance will suffer.
Item 37: Use EnumMap instead of ordinal indexing
Occasionally you may see code that uses the ordinal method (Item 35) to
index into an array or list. For example, consider this simplistic class meant to
represent a plant:
Click here to view code image
class Plant {
enum LifeCycle { ANNUAL, PERENNIAL, BIENNIAL }
final String name;
final LifeCycle lifeCycle;
Plant(String name, LifeCycle lifeCycle) {
this.name = name;
this.lifeCycle = lifeCycle;
}
@Override public String toString() {
return name;
}
}
Now suppose you have an array of plants representing a garden, and you
want to list these plants organized by life cycle (annual, perennial, or
biennial). To do this, you construct three sets, one for each life cycle, and
iterate through the garden, placing each plant in the appropriate set. Some
programmers would do this by putting the sets into an array indexed by the
life cycle’s ordinal:
Click here to view code image
// Using ordinal() to index into an array - DON'T DO
THIS!
Set<Plant>[] plantsByLifeCycle =
(Set<Plant>[]) new
Set[Plant.LifeCycle.values().length];
for (int i = 0; i < plantsByLifeCycle.length; i++)
plantsByLifeCycle[i] = new HashSet<>();
199
for (Plant p : garden)
plantsByLifeCycle[p.lifeCycle.ordinal()].add(p);
// Print the results
for (int i = 0; i < plantsByLifeCycle.length; i++) {
System.out.printf("%s: %s%n",
Plant.LifeCycle.values()[i],
plantsByLifeCycle[i]);
}
This technique works, but it is fraught with problems. Because arrays are
not compatible with generics (Item 28), the program requires an unchecked
cast and will not compile cleanly. Because the array does not know what its
index represents, you have to label the output manually. But the most serious
problem with this technique is that when you access an array that is indexed
by an enum’s ordinal, it is your responsibility to use the correct int value;
ints do not provide the type safety of enums. If you use the wrong value, the
program will silently do the wrong thing or—if you’re lucky—throw an
ArrayIndexOutOfBoundsException.
There is a much better way to achieve the same effect. The array is
effectively serving as a map from the enum to a value, so you might as well
use a Map. More specifically, there is a very fast Map implementation
designed for use with enum keys, known as java.util.EnumMap. Here is
how the program looks when it is rewritten to use EnumMap:
Click here to view code image
// Using an EnumMap to associate data with an enum
Map<Plant.LifeCycle, Set<Plant>> plantsByLifeCycle =
new EnumMap<>(Plant.LifeCycle.class);
for (Plant.LifeCycle lc : Plant.LifeCycle.values())
plantsByLifeCycle.put(lc, new HashSet<>());
for (Plant p : garden)
plantsByLifeCycle.get(p.lifeCycle).add(p);
System.out.println(plantsByLifeCycle);
This program is shorter, clearer, safer, and comparable in speed to the
original version. There is no unsafe cast; no need to label the output manually
because the map keys are enums that know how to translate themselves to
printable strings; and no possibility for error in computing array indices. The
reason that EnumMap is comparable in speed to an ordinal-indexed array is
that EnumMap uses such an array internally, but it hides this implementation
detail from the programmer, combining the richness and type safety of a Map
with the speed of an array. Note that the EnumMap constructor takes the
Class object of the key type: this is a bounded type token, which provides
200
runtime generic type information (Item 33).
The previous program can be further shortened by using a stream (Item 45)
to manage the map. Here is the simplest stream-based code that largely
duplicates the behavior of the previous example:
Click here to view code image
// Naive stream-based approach - unlikely to produce an
EnumMap!
System.out.println(Arrays.stream(garden)
.collect(groupingBy(p -> p.lifeCycle)));
The problem with this code is that it chooses its own map implementation,
and in practice it won’t be an EnumMap, so it won’t match the space and time
performance of the version with the explicit EnumMap. To rectify this
problem, use the three-parameter form of Collectors.groupingBy,
which allows the caller to specify the map implementation using the
mapFactory parameter:
Click here to view code image
// Using a stream and an EnumMap to associate data with
an enum
System.out.println(Arrays.stream(garden)
.collect(groupingBy(p -> p.lifeCycle,
() -> new EnumMap<>(LifeCycle.class),
toSet())));
This optimization would not be worth doing in a toy program like this one but
could be critical in a program that made heavy use of the map.
The behavior of the stream-based versions differs slightly from that of the
EmumMap version. The EnumMap version always makes a nested map for
each plant lifecycle, while the stream-based versions only make a nested map
if the garden contains one or more plants with that lifecycle. So, for example,
if the garden contains annuals and perennials but no biennials, the size of
plantsByLifeCycle will be three in the EnumMap version and two in
both of the stream-based versions.
You may see an array of arrays indexed (twice!) by ordinals used to
represent a mapping from two enum values. For example, this program uses
such an array to map two phases to a phase transition (liquid to solid is
freezing, liquid to gas is boiling, and so forth):
Click here to view code image
// Using ordinal() to index array of arrays - DON'T DO
THIS!
public enum Phase {
SOLID, LIQUID, GAS;
201
public enum Transition {
MELT, FREEZE, BOIL, CONDENSE, SUBLIME, DEPOSIT;
// Rows indexed by from-ordinal, cols by toordinal
private static final Transition[][] TRANSITIONS
= {
{ null, MELT, SUBLIME },
{ FREEZE, null, BOIL },
{ DEPOSIT, CONDENSE, null }
};
// Returns the phase transition from one phase
to another
public static Transition from(Phase from, Phase
to) {
return TRANSITIONS[from.ordinal()]
[to.ordinal()];
}
}
}
This program works and may even appear elegant, but appearances can be
deceiving. Like the simpler garden example shown earlier, the compiler has
no way of knowing the relationship between ordinals and array indices. If you
make a mistake in the transition table or forget to update it when you modify
the Phase or Phase.Transition enum type, your program will fail at
runtime. The failure may be an ArrayIndexOutOfBoundsException,
a NullPointerException, or (worse) silent erroneous behavior. And
the size of the table is quadratic in the number of phases, even if the number
of non-null entries is smaller.
Again, you can do much better with EnumMap. Because each phase
transition is indexed by a pair of phase enums, you are best off representing
the relationship as a map from one enum (the “from” phase) to a map from
the second enum (the “to” phase) to the result (the phase transition). The two
phases associated with a phase transition are best captured by associating
them with the phase transition enum, which can then be used to initialize the
nested EnumMap:
Click here to view code image
// Using a nested EnumMap to associate data with enum
pairs
public enum Phase {
SOLID, LIQUID, GAS;
202
public enum Transition {
MELT(SOLID, LIQUID), FREEZE(LIQUID, SOLID),
BOIL(LIQUID, GAS), CONDENSE(GAS, LIQUID),
SUBLIME(SOLID, GAS), DEPOSIT(GAS, SOLID);
private final Phase from;
private final Phase to;
Transition(Phase from, Phase to) {
this.from = from;
this.to = to;
}
// Initialize the phase transition map
private static final Map<Phase, Map<Phase,
Transition>>
m = Stream.of(values()).collect(groupingBy(t ->
t.from,
() -> new EnumMap<>(Phase.class),
toMap(t -> t.to, t -> t,
(x, y) -> y, () -> new EnumMap<>
(Phase.class))));
public static Transition from(Phase from, Phase
to) {
return m.get(from).get(to);
}
}
}
The code to initialize the phase transition map is a bit complicated. The
type of the map is Map<Phase, Map<Phase, Transition>>, which
means “map from (source) phase to map from (destination) phase to
transition.” This map-of-maps is initialized using a cascaded sequence of two
collectors. The first collector groups the transitions by source phase, and the
second creates an EnumMap with mappings from destination phase to
transition. The merge function in the second collector ((x, y) -> y)) is
unused; it is required only because we need to specify a map factory in order
to get an EnumMap, and Collectors provides telescoping factories. The
previous edition of this book used explicit iteration to initialize the phase
transition map. The code was more verbose but arguably easier to understand.
Now suppose you want to add a new phase to the system: plasma, or
ionized gas. There are only two transitions associated with this phase:
ionization, which takes a gas to a plasma; and deionization, which takes a
plasma to a gas. To update the array-based program, you would have to add
one new constant to Phase and two to Phase.Transition, and replace
203
the original nine-element array of arrays with a new sixteen-element version.
If you add too many or too few elements to the array or place an element out
of order, you are out of luck: the program will compile, but it will fail at
runtime. To update the EnumMap-based version, all you have to do is add
PLASMA to the list of phases, and IONIZE(GAS, PLASMA) and
DEIONIZE(PLASMA, GAS) to the list of phase transitions:
Click here to view code image
// Adding a new phase using the nested EnumMap
implementation
public enum Phase {
SOLID, LIQUID, GAS, PLASMA;
public enum Transition {
MELT(SOLID, LIQUID), FREEZE(LIQUID, SOLID),
BOIL(LIQUID, GAS), CONDENSE(GAS, LIQUID),
SUBLIME(SOLID, GAS), DEPOSIT(GAS, SOLID),
IONIZE(GAS, PLASMA), DEIONIZE(PLASMA, GAS);
... // Remainder unchanged
}
}
The program takes care of everything else and leaves you virtually no
opportunity for error. Internally, the map of maps is implemented with an
array of arrays, so you pay little in space or time cost for the added clarity,
safety, and ease of maintenance.
In the interest of brevity, the above examples use null to indicate the
absence of a state change (wherein to and from are identical). This is not
good practice and is likely to result in a NullPointerException at
runtime. Designing a clean, elegant solution to this problem is surprisingly
tricky, and the resulting programs are sufficiently long that they would detract
from the primary material in this item.
In summary, it is rarely appropriate to use ordinals to index into
arrays: use EnumMap instead. If the relationship you are representing is
multidimensional, use EnumMap<..., EnumMap<...>>. This is a
special case of the general principle that application programmers should
rarely, if ever, use Enum.ordinal (Item 35).
Item 38: Emulate extensible enums with interfaces
In almost all respects, enum types are superior to the typesafe enum pattern
described in the first edition of this book [Bloch01]. On the face of it, one
exception concerns extensibility, which was possible under the original
pattern but is not supported by the language construct. In other words, using
204
the pattern, it was possible to have one enumerated type extend another; using
the language feature, it is not. This is no accident. For the most part,
extensibility of enums turns out to be a bad idea. It is confusing that elements
of an extension type are instances of the base type and not vice versa. There is
no good way to enumerate over all of the elements of a base type and its
extensions. Finally, extensibility would complicate many aspects of the
design and implementation.
That said, there is at least one compelling use case for extensible
enumerated types, which is operation codes, also known as opcodes. An
opcode is an enumerated type whose elements represent operations on some
machine, such as the Operation type in Item 34, which represents the
functions on a simple calculator. Sometimes it is desirable to let the users of
an API provide their own operations, effectively extending the set of
operations provided by the API.
Luckily, there is a nice way to achieve this effect using enum types. The
basic idea is to take advantage of the fact that enum types can implement
arbitrary interfaces by defining an interface for the opcode type and an enum
that is the standard implementation of the interface. For example, here is an
extensible version of the Operation type from Item 34:
Click here to view code image
// Emulated extensible enum using an interface
public interface Operation {
double apply(double x, double y);
}
public enum BasicOperation implements Operation {
PLUS("+") {
public double apply(double x, double y) { return
x + y; }
},
MINUS("-") {
public double apply(double x, double y) { return
x - y; }
},
TIMES("*") {
public double apply(double x, double y) { return
x * y; }
},
DIVIDE("/") {
public double apply(double x, double y) { return
x / y; }
};
private final String symbol;
205
BasicOperation(String symbol) {
this.symbol = symbol;
}
@Override public String toString() {
return symbol;
}
}
While the enum type (BasicOperation) is not extensible, the interface
type (Operation) is, and it is the interface type that is used to represent
operations in APIs. You can define another enum type that implements this
interface and use instances of this new type in place of the base type. For
example, suppose you want to define an extension to the operation type
shown earlier, consisting of the exponentiation and remainder operations. All
you have to do is write an enum type that implements the Operation
interface:
Click here to view code image
// Emulated extension enum
public enum ExtendedOperation implements Operation {
EXP("^") {
public double apply(double x, double y) {
return Math.pow(x, y);
}
},
REMAINDER("%") {
public double apply(double x, double y) {
return x % y;
}
};
private final String symbol;
ExtendedOperation(String symbol) {
this.symbol = symbol;
}
@Override public String toString() {
return symbol;
}
}
You can now use your new operations anywhere you could use the basic
operations, provided that APIs are written to take the interface type
(Operation), not the implementation (BasicOperation). Note that you
don’t have to declare the abstract apply method in the enum as you do in a
206
nonextensible enum with instance-specific method implementations (page
162). This is because the abstract method (apply) is a member of the
interface (Operation).
Not only is it possible to pass a single instance of an “extension enum”
anywhere a “base enum” is expected, but it is possible to pass in an entire
extension enum type and use its elements in addition to or instead of those of
the base type. For example, here is a version of the test program on page 163
that exercises all of the extended operations defined previously:
Click here to view code image
public static void main(String[] args) {
double x = Double.parseDouble(args[0]);
double y = Double.parseDouble(args[1]);
test(ExtendedOperation.class, x, y);
}
private static <T extends Enum<T> & Operation> void
test(
Class<T> opEnumType, double x, double y) {
for (Operation op : opEnumType.getEnumConstants())
System.out.printf("%f %s %f = %f%n",
x, op, y, op.apply(x, y));
}
Note that the class literal for the extended operation type
(ExtendedOperation.class) is passed from main to test to
describe the set of extended operations. The class literal serves as a bounded
type token (Item 33). The admittedly complex declaration for the
opEnumType parameter (<T extends Enum<T> & Operation>
Class<T>) ensures that the Class object represents both an enum and a
subtype of Operation, which is exactly what is required to iterate over the
elements and perform the operation associated with each one.
A second alternative is to pass a Collection<? extends
Operation>, which is a bounded wildcard type (Item 31), instead of
passing a class object:
Click here to view code image
public static void main(String[] args) {
double x = Double.parseDouble(args[0]);
double y = Double.parseDouble(args[1]);
test(Arrays.asList(ExtendedOperation.values()), x,
y);
}
private static void test(Collection<? extends Operation>
207
opSet,
double x, double y) {
for (Operation op : opSet)
System.out.printf("%f %s %f = %f%n",
x, op, y, op.apply(x, y));
}
The resulting code is a bit less complex, and the test method is a bit more
flexible: it allows the caller to combine operations from multiple
implementation types. On the other hand, you forgo the ability to use
EnumSet (Item 36) and EnumMap (Item 37) on the specified operations.
Both programs shown previously will produce this output when run with
command line arguments 4 and 2:
Click here to view code image
4.000000 ^ 2.000000 = 16.000000
4.000000 % 2.000000 = 0.000000
A minor disadvantage of the use of interfaces to emulate extensible enums
is that implementations cannot be inherited from one enum type to another. If
the implementation code does not rely on any state, it can be placed in the
interface, using default implementations (Item 20). In the case of our
Operation example, the logic to store and retrieve the symbol associated
with an operation must be duplicated in BasicOperation and
ExtendedOperation. In this case it doesn’t matter because very little
code is duplicated. If there were a larger amount of shared functionality, you
could encapsulate it in a helper class or a static helper method to eliminate the
code duplication.
The pattern described in this item is used in the Java libraries. For example,
the java.nio.file.LinkOption enum type implements the
CopyOption and OpenOption interfaces.
In summary, while you cannot write an extensible enum type, you can
emulate it by writing an interface to accompany a basic enum type that
implements the interface. This allows clients to write their own enums (or
other types) that implement the interface. Instances of these types can then be
used wherever instances of the basic enum type can be used, assuming APIs
are written in terms of the interface.
Item 39: Prefer annotations to naming patterns
Historically, it was common to use naming patterns to indicate that some
program elements demanded special treatment by a tool or framework. For
example, prior to release 4, the JUnit testing framework required its users to
208
designate test methods by beginning their names with the characters test
[Beck04]. This technique works, but it has several big disadvantages. First,
typographical errors result in silent failures. For example, suppose you
accidentally named a test method tsetSafetyOverride instead of
testSafetyOverride. JUnit 3 wouldn’t complain, but it wouldn’t
execute the test either, leading to a false sense of security.
A second disadvantage of naming patterns is that there is no way to ensure
that they are used only on appropriate program elements. For example,
suppose you called a class TestSafetyMechanisms in hopes that JUnit 3
would automatically test all of its methods, regardless of their names. Again,
JUnit 3 wouldn’t complain, but it wouldn’t execute the tests either.
A third disadvantage of naming patterns is that they provide no good way
to associate parameter values with program elements. For example, suppose
you want to support a category of test that succeeds only if it throws a
particular exception. The exception type is essentially a parameter of the test.
You could encode the exception type name into the test method name using
some elaborate naming pattern, but this would be ugly and fragile (Item 62).
The compiler would have no way of knowing to check that the string that was
supposed to name an exception actually did. If the named class didn’t exist or
wasn’t an exception, you wouldn’t find out until you tried to run the test.
Annotations [JLS, 9.7] solve all of these problems nicely, and JUnit
adopted them starting with release 4. In this item, we’ll write our own toy
testing framework to show how annotations work. Suppose you want to
define an annotation type to designate simple tests that are run automatically
and fail if they throw an exception. Here’s how such an annotation type,
named Test, might look:
Click here to view code image
// Marker annotation type declaration
import java.lang.annotation.*;
/**
* Indicates that the annotated method is a test method.
* Use only on parameterless static methods.
*/
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface Test {
}
The declaration for the Test annotation type is itself annotated with
Retention and Target annotations. Such annotations on annotation type
declarations are known as meta-annotations. The
209
@Retention(RetentionPolicy.RUNTIME) meta-annotation
indicates that Test annotations should be retained at runtime. Without it,
Test annotations would be invisible to the test tool. The
@Target.get(ElementType.METHOD) meta-annotation indicates that
the Test annotation is legal only on method declarations: it cannot be
applied to class declarations, field declarations, or other program elements.
The comment before the Test annotation declaration says, “Use only on
parameterless static methods.” It would be nice if the compiler could enforce
this, but it can’t, unless you write an annotation processor to do so. For more
on this topic, see the documentation for
javax.annotation.processing. In the absence of such an annotation
processor, if you put a Test annotation on the declaration of an instance
method or on a method with one or more parameters, the test program will
still compile, leaving it to the testing tool to deal with the problem at runtime.
Here is how the Test annotation looks in practice. It is called a marker
annotation because it has no parameters but simply “marks” the annotated
element. If the programmer were to misspell Test or to apply the Test
annotation to a program element other than a method declaration, the program
wouldn’t compile:
Click here to view code image
// Program containing marker annotations
public class Sample {
@Test public static void m1() { } // Test should
pass
public static void m2() { }
@Test public static void m3() { // Test should
fail
throw new RuntimeException("Boom");
}
public static void m4() { }
@Test public void m5() { } // INVALID USE: nonstatic
method
public static void m6() { }
@Test public static void m7() { // Test should
fail
throw new RuntimeException("Crash");
}
public static void m8() { }
}
The Sample class has seven static methods, four of which are annotated as
tests. Two of these, m3 and m7, throw exceptions, and two, m1 and m5, do
not. But one of the annotated methods that does not throw an exception, m5,
210
is an instance method, so it is not a valid use of the annotation. In sum,
Sample contains four tests: one will pass, two will fail, and one is invalid.
The four methods that are not annotated with the Test annotation will be
ignored by the testing tool.
The Test annotations have no direct effect on the semantics of the
Sample class. They serve only to provide information for use by interested
programs. More generally, annotations don’t change the semantics of the
annotated code but enable it for special treatment by tools such as this simple
test runner:
Click here to view code image
// Program to process marker annotations
import java.lang.reflect.*;
public class RunTests {
public static void main(String[] args) throws
Exception {
int tests = 0;
int passed = 0;
Class<?> testClass = Class.forName(args[0]);
for (Method m : testClass.getDeclaredMethods())
{
if (m.isAnnotationPresent(Test.class)) {
tests++;
try {
m.invoke(null);
passed++;
} catch (InvocationTargetException
wrappedExc) {
Throwable exc =
wrappedExc.getCause();
System.out.println(m + " failed: " +
exc);
} catch (Exception exc) {
System.out.println("Invalid @Test: "
+ m);
}
}
}
System.out.printf("Passed: %d, Failed: %d%n",
passed, tests - passed);
}
}
The test runner tool takes a fully qualified class name on the command line
and runs all of the class’s Test-annotated methods reflectively, by calling
211
Method.invoke. The isAnnotationPresent method tells the tool
which methods to run. If a test method throws an exception, the reflection
facility wraps it in an InvocationTargetException. The tool catches
this exception and prints a failure report containing the original exception
thrown by the test method, which is extracted from the
InvocationTargetException with the getCause method.
If an attempt to invoke a test method by reflection throws any exception
other than InvocationTargetException, it indicates an invalid use of
the Test annotation that was not caught at compile time. Such uses include
annotation of an instance method, of a method with one or more parameters,
or of an inaccessible method. The second catch block in the test runner
catches these Test usage errors and prints an appropriate error message.
Here is the output that is printed if RunTests is run on Sample:
Click here to view code image
public static void Sample.m3() failed: RuntimeException:
Boom
Invalid @Test: public void Sample.m5()
public static void Sample.m7() failed: RuntimeException:
Crash
Passed: 1, Failed: 3
Now let’s add support for tests that succeed only if they throw a particular
exception. We’ll need a new annotation type for this:
Click here to view code image
// Annotation type with a parameter
import java.lang.annotation.*;
/**
* Indicates that the annotated method is a test method
that
* must throw the designated exception to succeed.
*/
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface ExceptionTest {
Class<? extends Throwable> value();
}
The type of the parameter for this annotation is Class<? extends
Throwable>. This wildcard type is, admittedly, a mouthful. In English, it
means “the Class object for some class that extends Throwable,” and it
allows the user of the annotation to specify any exception (or error) type. This
usage is an example of a bounded type token (Item 33). Here’s how the
annotation looks in practice. Note that class literals are used as the values for
212
the annotation parameter:
Click here to view code image
// Program containing annotations with a parameter
public class Sample2 {
@ExceptionTest(ArithmeticException.class)
public static void m1() { // Test should pass
int i = 0;
i = i / i;
}
@ExceptionTest(ArithmeticException.class)
public static void m2() { // Should fail (wrong
exception)
int[] a = new int[0];
int i = a[1];
}
@ExceptionTest(ArithmeticException.class)
public static void m3() { } // Should fail (no
exception)
}
Now let’s modify the test runner tool to process the new annotation. Doing
so consists of adding the following code to the main method:
Click here to view code image
if (m.isAnnotationPresent(ExceptionTest.class)) {
tests++;
try {
m.invoke(null);
System.out.printf("Test %s failed: no
exception%n", m);
} catch (InvocationTargetException wrappedEx) {
Throwable exc = wrappedEx.getCause();
Class<? extends Throwable> excType =
m.getAnnotation(ExceptionTest.class).value();
if (excType.isInstance(exc)) {
passed++;
} else {
System.out.printf(
"Test %s failed: expected %s, got %s%n",
m, excType.getName(), exc);
}
} catch (Exception exc) {
System.out.println("Invalid @Test: " + m);
}
}
This code is similar to the code we used to process Test annotations, with
213
one exception: this code extracts the value of the annotation parameter and
uses it to check if the exception thrown by the test is of the right type. There
are no explicit casts, and hence no danger of a ClassCastException.
The fact that the test program compiled guarantees that its annotation
parameters represent valid exception types, with one caveat: if the annotation
parameters were valid at compile time but the class file representing a
specified exception type is no longer present at runtime, the test runner will
throw TypeNotPresentException.
Taking our exception testing example one step further, it is possible to
envision a test that passes if it throws any one of several specified exceptions.
The annotation mechanism has a facility that makes it easy to support this
usage. Suppose we change the parameter type of the ExceptionTest
annotation to be an array of Class objects:
Click here to view code image
// Annotation type with an array parameter
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface ExceptionTest {
Class<? extends Exception>[] value();
}
The syntax for array parameters in annotations is flexible. It is optimized
for single-element arrays. All of the previous ExceptionTest annotations
are still valid with the new array-parameter version of ExceptionTest and
result in single-element arrays. To specify a multiple-element array, surround
the elements with curly braces and separate them with commas:
Click here to view code image
// Code containing an annotation with an array parameter
@ExceptionTest({ IndexOutOfBoundsException.class,
NullPointerException.class })
public static void doublyBad() {
List<String> list = new ArrayList<>();
// The spec permits this method to throw either
// IndexOutOfBoundsException or NullPointerException
list.addAll(5, null);
}
It is reasonably straightforward to modify the test runner tool to process the
new version of ExceptionTest. This code replaces the original version:
Click here to view code image
if (m.isAnnotationPresent(ExceptionTest.class)) {
214
tests++;
try {
m.invoke(null);
System.out.printf("Test %s failed: no
exception%n", m);
} catch (Throwable wrappedExc) {
Throwable exc = wrappedExc.getCause();
int oldPassed = passed;
Class<? extends Exception>[] excTypes =
m.getAnnotation(ExceptionTest.class).value();
for (Class<? extends Exception> excType :
excTypes) {
if (excType.isInstance(exc)) {
passed++;
break;
}
}
if (passed == oldPassed)
System.out.printf("Test %s failed: %s %n",
m, exc);
}
}
As of Java 8, there is another way to do multivalued annotations. Instead of
declaring an annotation type with an array parameter, you can annotate the
declaration of an annotation with the @Repeatable meta-annotation, to
indicate that the annotation may be applied repeatedly to a single element.
This meta-annotation takes a single parameter, which is the class object of a
containing annotation type, whose sole parameter is an array of the
annotation type [JLS, 9.6.3]. Here’s how the annotation declarations look if
we take this approach with our ExceptionTest annotation. Note that the
containing annotation type must be annotated with an appropriate retention
policy and target, or the declarations won’t compile:
Click here to view code image
// Repeatable annotation type
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
@Repeatable(ExceptionTestContainer.class)
public @interface ExceptionTest {
Class<? extends Exception> value();
}
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface ExceptionTestContainer {
ExceptionTest[] value();
215
}
Here’s how our doublyBad test looks with a repeated annotation in place
of an array-valued annotation:
Click here to view code image
// Code containing a repeated annotation
@ExceptionTest(IndexOutOfBoundsException.class)
@ExceptionTest(NullPointerException.class)
public static void doublyBad() { ... }
Processing repeatable annotations requires care. A repeated annotation
generates a synthetic annotation of the containing annotation type. The
getAnnotationsByType method glosses over this fact, and can be used
to access both repeated and non-repeated annotations of a repeatable
annotation type. But isAnnotationPresent makes it explicit that
repeated annotations are not of the annotation type, but of the containing
annotation type. If an element has a repeated annotation of some type and you
use the isAnnotationPresent method to check if the element has an
annotation of that type, you’ll find that it does not. Using this method to
check for the presence of an annotation type will therefore cause your
program to silently ignore repeated annotations. Similarly, using this method
to check for the containing annotation type will cause the program to silently
ignore non-repeated annotations. To detect repeated and non-repeated
annotations with isAnnotationPresent, you much check for both the
annotation type and its containing annotation type. Here’s how the relevant
part of our RunTests program looks when modified to use the repeatable
version of the ExceptionTest annotation:
Click here to view code image
// Processing repeatable annotations
if (m.isAnnotationPresent(ExceptionTest.class)
||
m.isAnnotationPresent(ExceptionTestContainer.class)) {
tests++;
try {
m.invoke(null);
System.out.printf("Test %s failed: no
exception%n", m);
} catch (Throwable wrappedExc) {
Throwable exc = wrappedExc.getCause();
int oldPassed = passed;
ExceptionTest[] excTests =
m.getAnnotationsByType(ExceptionTest.class);
for (ExceptionTest excTest : excTests) {
216
if (excTest.value().isInstance(exc)) {
passed++;
break;
}
}
if (passed == oldPassed)
System.out.printf("Test %s failed: %s %n",
m, exc);
}
}
Repeatable annotations were added to improve the readability of source
code that logically applies multiple instances of the same annotation type to a
given program element. If you feel they enhance the readability of your
source code, use them, but remember that there is more boilerplate in
declaring and processing repeatable annotations, and that processing
repeatable annotations is error-prone.
The testing framework in this item is just a toy, but it clearly demonstrates
the superiority of annotations over naming patterns, and it only scratches the
surface of what you can do with them. If you write a tool that requires
programmers to add information to source code, define appropriate annotation
types. There is simply no reason to use naming patterns when you can use
annotations instead.
That said, with the exception of toolsmiths, most programmers will have no
need to define annotation types. But all programmers should use the
predefined annotation types that Java provides (Items 40, 27). Also,
consider using the annotations provided by your IDE or static analysis tools.
Such annotations can improve the quality of the diagnostic information
provided by these tools. Note, however, that these annotations have yet to be
standardized, so you may have some work to do if you switch tools or if a
standard emerges.
Item 40: Consistently use the Override annotation
The Java libraries contain several annotation types. For the typical
programmer, the most important of these is @Override. This annotation can
be used only on method declarations, and it indicates that the annotated
method declaration overrides a declaration in a supertype. If you consistently
use this annotation, it will protect you from a large class of nefarious bugs.
Consider this program, in which the class Bigram represents a bigram, or
ordered pair of letters:
Click here to view code image
217
// Can you spot the bug?
public class Bigram {
private final char first;
private final char second;
public Bigram(char first, char second) {
this.first = first;
this.second = second;
}
public boolean equals(Bigram b) {
return b.first == first && b.second == second;
}
public int hashCode() {
return 31 * first + second;
}
public static void main(String[] args) {
Set<Bigram> s = new HashSet<>();
for (int i = 0; i < 10; i++)
for (char ch = 'a'; ch <= 'z'; ch++)
s.add(new Bigram(ch, ch));
System.out.println(s.size());
}
}
The main program repeatedly adds twenty-six bigrams, each consisting of
two identical lowercase letters, to a set. Then it prints the size of the set. You
might expect the program to print 26, as sets cannot contain duplicates. If you
try running the program, you’ll find that it prints not 26 but 260. What is
wrong with it?
Clearly, the author of the Bigram class intended to override the equals
method (Item 10) and even remembered to override hashCode in tandem
(Item 11). Unfortunately, our hapless programmer failed to override equals,
overloading it instead (Item 52). To override Object.equals, you must
define an equals method whose parameter is of type Object, but the
parameter of Bigram’s equals method is not of type Object, so Bigram
inherits the equals method from Object. This equals method tests for
object identity, just like the == operator. Each of the ten copies of each
bigram is distinct from the other nine, so they are deemed unequal by
Object.equals, which explains why the program prints 260.
Luckily, the compiler can help you find this error, but only if you help it by
telling it that you intend to override Object.equals. To do this, annotate
Bigram.equals with @Override, as shown here:
Click here to view code image
218
@Override public boolean equals(Bigram b) {
return b.first == first && b.second == second;
}
If you insert this annotation and try to recompile the program, the compiler
will generate an error message like this:
Click here to view code image
Bigram.java:10: method does not override or implement a
method
from a supertype
@Override public boolean equals(Bigram b) {
^
You will immediately realize what you did wrong, slap yourself on the
forehead, and replace the broken equals implementation with a correct one
(Item 10):
Click here to view code image
@Override public boolean equals(Object o) {
if (!(o instanceof Bigram))
return false;
Bigram b = (Bigram) o;
return b.first == first && b.second == second;
}
Therefore, you should use the Override annotation on every method
declaration that you believe to override a superclass declaration. There is
one minor exception to this rule. If you are writing a class that is not labeled
abstract and you believe that it overrides an abstract method in its superclass,
you needn’t bother putting the Override annotation on that method. In a
class that is not declared abstract, the compiler will emit an error message if
you fail to override an abstract superclass method. However, you might wish
to draw attention to all of the methods in your class that override superclass
methods, in which case you should feel free to annotate these methods too.
Most IDEs can be set to insert Override annotations automatically when
you elect to override a method.
Most IDEs provide another reason to use the Override annotation
consistently. If you enable the appropriate check, the IDE will generate a
warning if you have a method that doesn’t have an Override annotation but
does override a superclass method. If you use the Override annotation
consistently, these warnings will alert you to unintentional overriding. They
complement the compiler’s error messages, which alert you to unintentional
failure to override. Between the IDE and the compiler, you can be sure that
you’re overriding methods everywhere you want to and nowhere else.
219
The Override annotation may be used on method declarations that
override declarations from interfaces as well as classes. With the advent of
default methods, it is good practice to use Override on concrete
implementations of interface methods to ensure that the signature is correct. If
you know that an interface does not have default methods, you may choose to
omit Override annotations on concrete implementations of interface
methods to reduce clutter.
In an abstract class or an interface, however, it is worth annotating all
methods that you believe to override superclass or superinterface methods,
whether concrete or abstract. For example, the Set interface adds no new
methods to the Collection interface, so it should include Override
annotations on all of its method declarations to ensure that it does not
accidentally add any new methods to the Collection interface.
In summary, the compiler can protect you from a great many errors if you
use the Override annotation on every method declaration that you believe
to override a supertype declaration, with one exception. In concrete classes,
you need not annotate methods that you believe to override abstract method
declarations (though it is not harmful to do so).
Item 41: Use marker interfaces to define types
A marker interface is an interface that contains no method declarations but
merely designates (or “marks”) a class that implements the interface as having
some property. For example, consider the Serializable interface
(Chapter 12). By implementing this interface, a class indicates that its
instances can be written to an ObjectOutputStream (or “serialized”).
You may hear it said that marker annotations (Item 39) make marker
interfaces obsolete. This assertion is incorrect. Marker interfaces have two
advantages over marker annotations. First and foremost, marker interfaces
define a type that is implemented by instances of the marked class;
marker annotations do not. The existence of a marker interface type allows
you to catch errors at compile time that you couldn’t catch until runtime if
you used a marker annotation.
Java’s serialization facility (Chapter 6) uses the Serializable marker
interface to indicate that a type is serializable. The
ObjectOutputStream.writeObject method, which serializes the
object that is passed to it, requires that its argument be serializable. Had the
argument of this method been of type Serializable, an attempt to
serialize an inappropriate object would have been detected at compile time
(by type checking). Compile-time error detection is the intent of marker
interfaces, but unfortunately, the ObjectOutputStream.write API
220
does not take advantage of the Serializable interface: its argument is
declared to be of type Object, so attempts to serialize an unserializable
object won’t fail until runtime.
Another advantage of marker interfaces over marker annotations is
that they can be targeted more precisely. If an annotation type is declared
with target ElementType.TYPE, it can be applied to any class or interface.
Suppose you have a marker that is applicable only to implementations of a
particular interface. If you define it as a marker interface, you can have it
extend the sole interface to which it is applicable, guaranteeing that all
marked types are also subtypes of the sole interface to which it is applicable.
Arguably, the Set interface is just such a restricted marker interface. It is
applicable only to Collection subtypes, but it adds no methods beyond
those defined by Collection. It is not generally considered to be a marker
interface because it refines the contracts of several Collection methods,
including add, equals, and hashCode. But it is easy to imagine a marker
interface that is applicable only to subtypes of some particular interface and
does not refine the contracts of any of the interface’s methods. Such a marker
interface might describe some invariant of the entire object or indicate that
instances are eligible for processing by a method of some other class (in the
way that the Serializable interface indicates that instances are eligible
for processing by ObjectOutputStream).
The chief advantage of marker annotations over marker interfaces is
that they are part of the larger annotation facility. Therefore, marker
annotations allow for consistency in annotation-based frameworks.
So when should you use a marker annotation and when should you use a
marker interface? Clearly you must use an annotation if the marker applies to
any program element other than a class or interface, because only classes and
interfaces can be made to implement or extend an interface. If the marker
applies only to classes and interfaces, ask yourself the question “Might I want
to write one or more methods that accept only objects that have this
marking?” If so, you should use a marker interface in preference to an
annotation. This will make it possible for you to use the interface as a
parameter type for the methods in question, which will result in the benefit of
compile-time type checking. If you can convince yourself that you’ll never
want to write a method that accepts only objects with the marking, then
you’re probably better off using a marker annotation. If, additionally, the
marking is part of a framework that makes heavy use of annotations, then a
marker annotation is the clear choice.
In summary, marker interfaces and marker annotations both have their
uses. If you want to define a type that does not have any new methods
associated with it, a marker interface is the way to go. If you want to mark
221
program elements other than classes and interfaces or to fit the marker into a
framework that already makes heavy use of annotation types, then a marker
annotation is the correct choice. If you find yourself writing a marker
annotation type whose target is ElementType.TYPE, take the time to
figure out whether it really should be an annotation type or whether a
marker interface would be more appropriate.
In a sense, this item is the inverse of Item 22, which says, “If you don’t
want to define a type, don’t use an interface.” To a first approximation, this
item says, “If you do want to define a type, do use an interface.”
222
Chapter 7. Lambdas and Streams
In Java 8, functional interfaces, lambdas, and method references were added
to make it easier to create function objects. The streams API was added in
tandem with these language changes to provide library support for processing
sequences of data elements. In this chapter, we discuss how to make best use
of these facilities.
Item 42: Prefer lambdas to anonymous classes
Historically, interfaces (or, rarely, abstract classes) with a single abstract
method were used as function types. Their instances, known as function
objects, represent functions or actions. Since JDK 1.1 was released in 1997,
the primary means of creating a function object was the anonymous class
(Item 24). Here’s a code snippet to sort a list of strings in order of length,
using an anonymous class to create the sort’s comparison function (which
imposes the sort order):
Click here to view code image
// Anonymous class instance as a function object -
obsolete!
Collections.sort(words, new Comparator<String>() {
public int compare(String s1, String s2) {
return Integer.compare(s1.length(),
s2.length());
}
});
Anonymous classes were adequate for the classic objected-oriented design
patterns requiring function objects, notably the Strategy pattern [Gamma95].
The Comparator interface represents an abstract strategy for sorting; the
anonymous class above is a concrete strategy for sorting strings. The
verbosity of anonymous classes, however, made functional programming in
Java an unappealing prospect.
In Java 8, the language formalized the notion that interfaces with a single
abstract method are special and deserve special treatment. These interfaces
are now known as functional interfaces, and the language allows you to create
instances of these interfaces using lambda expressions, or lambdas for short.
Lambdas are similar in function to anonymous classes, but far more concise.
Here’s how the code snippet above looks with the anonymous class replaced
by a lambda. The boilerplate is gone, and the behavior is clearly evident:
223
Click here to view code image
// Lambda expression as function object (replaces
anonymous class)
Collections.sort(words,
(s1, s2) -> Integer.compare(s1.length(),
s2.length()));
Note that the types of the lambda (Comparator<String>), of its
parameters (s1 and s2, both String), and of its return value (int) are not
present in the code. The compiler deduces these types from context, using a
process known as type inference. In some cases, the compiler won’t be able to
determine the types, and you’ll have to specify them. The rules for type
inference are complex: they take up an entire chapter in the JLS [JLS, 18].
Few programmers understand these rules in detail, but that’s OK. Omit the
types of all lambda parameters unless their presence makes your
program clearer. If the compiler generates an error telling you it can’t infer
the type of a lambda parameter, then specify it. Sometimes you may have to
cast the return value or the entire lambda expression, but this is rare.
One caveat should be added concerning type inference. Item 26 tells you
not to use raw types, Item 29 tells you to favor generic types, and Item 30
tells you to favor generic methods. This advice is doubly important when
you’re using lambdas, because the compiler obtains most of the type
information that allows it to perform type inference from generics. If you
don’t provide this information, the compiler will be unable to do type
inference, and you’ll have to specify types manually in your lambdas, which
will greatly increase their verbosity. By way of example, the code snippet
above won’t compile if the variable words is declared to be of the raw type
List instead of the parameterized type List<String>.
Incidentally, the comparator in the snippet can be made even more succinct
if a comparator construction method is used in place of a lambda (Items 14.
43):
Click here to view code image
Collections.sort(words, comparingInt(String::length));
In fact, the snippet can be made still shorter by taking advantage of the sort
method that was added to the List interface in Java 8:
Click here to view code image
words.sort(comparingInt(String::length));
The addition of lambdas to the language makes it practical to use function
objects where it would not previously have made sense. For example,
224
consider the Operation enum type in Item 34. Because each enum required
different behavior for its apply method, we used constant-specific class
bodies and overrode the apply method in each enum constant. To refresh
your memory, here is the code:
Click here to view code image
// Enum type with constant-specific class bodies & data
(Item 34)
public enum Operation {
PLUS("+") {
public double apply(double x, double y) { return
x + y; }
},
MINUS("-") {
public double apply(double x, double y) { return
x - y; }
},
TIMES("*") {
public double apply(double x, double y) { return
x * y; }
},
DIVIDE("/") {
public double apply(double x, double y) { return
x / y; }
};
private final String symbol;
Operation(String symbol) { this.symbol = symbol; }
@Override public String toString() { return symbol;
}
public abstract double apply(double x, double y);
}
Item 34 says that enum instance fields are preferable to constant-specific class
bodies. Lambdas make it easy to implement constant-specific behavior using
the former instead of the latter. Merely pass a lambda implementing each
enum constant’s behavior to its constructor. The constructor stores the lambda
in an instance field, and the apply method forwards invocations to the
lambda. The resulting code is simpler and clearer than the original version:
Click here to view code image
// Enum with function object fields & constant-specific
behavior
public enum Operation {
PLUS ("+", (x, y) -> x + y),
MINUS ("-", (x, y) -> x - y),
225
TIMES ("*", (x, y) -> x * y),
DIVIDE("/", (x, y) -> x / y);
private final String symbol;
private final DoubleBinaryOperator op;
Operation(String symbol, DoubleBinaryOperator op) {
this.symbol = symbol;
this.op = op;
}
@Override public String toString() { return symbol;
}
public double apply(double x, double y) {
return op.applyAsDouble(x, y);
}
}
Note that we’re using the DoubleBinaryOperator interface for the
lambdas that represent the enum constant’s behavior. This is one of the many
predefined functional interfaces in java.util.function (Item 44). It
represents a function that takes two double arguments and returns a
double result.
Looking at the lambda-based Operation enum, you might think
constant-specific method bodies have outlived their usefulness, but this is not
the case. Unlike methods and classes, lambdas lack names and
documentation; if a computation isn’t self-explanatory, or exceeds a few
lines, don’t put it in a lambda. One line is ideal for a lambda, and three lines
is a reasonable maximum. If you violate this rule, it can cause serious harm to
the readability of your programs. If a lambda is long or difficult to read, either
find a way to simplify it or refactor your program to eliminate it. Also, the
arguments passed to enum constructors are evaluated in a static context. Thus,
lambdas in enum constructors can’t access instance members of the enum.
Constant-specific class bodies are still the way to go if an enum type has
constant-specific behavior that is difficult to understand, that can’t be
implemented in a few lines, or that requires access to instance fields or
methods.
Likewise, you might think that anonymous classes are obsolete in the era of
lambdas. This is closer to the truth, but there are a few things you can do with
anonymous classes that you can’t do with lambdas. Lambdas are limited to
functional interfaces. If you want to create an instance of an abstract class,
you can do it with an anonymous class, but not a lambda. Similarly, you can
use anonymous classes to create instances of interfaces with multiple abstract
226
methods. Finally, a lambda cannot obtain a reference to itself. In a lambda,
the this keyword refers to the enclosing instance, which is typically what
you want. In an anonymous class, the this keyword refers to the anonymous
class instance. If you need access to the function object from within its body,
then you must use an anonymous class.
Lambdas share with anonymous classes the property that you can’t reliably
serialize and deserialize them across implementations. Therefore, you should
rarely, if ever, serialize a lambda (or an anonymous class instance). If you
have a function object that you want to make serializable, such as a
Comparator, use an instance of a private static nested class (Item 24).
In summary, as of Java 8, lambdas are by far the best way to represent
small function objects. Don’t use anonymous classes for function objects
unless you have to create instances of types that aren’t functional
interfaces. Also, remember that lambdas make it so easy to represent small
function objects that it opens the door to functional programming techniques
that were not previously practical in Java.
Item 43: Prefer method references to lambdas
The primary advantage of lambdas over anonymous classes is that they are
more succinct. Java provides a way to generate function objects even more
succinct than lambdas: method references. Here is a code snippet from a
program that maintains a map from arbitrary keys to Integer values. If the
value is interpreted as a count of the number of instances of the key, then the
program is a multiset implementation. The function of the code snippet is to
associate the number 1 with the key if it is not in the map and to increment the
associated value if the key is already present:
Click here to view code image
map.merge(key, 1, (count, incr) -> count + incr);
Note that this code uses the merge method, which was added to the Map
interface in Java 8. If no mapping is present for the given key, the method
simply inserts the given value; if a mapping is already present, merge applies
the given function to the current value and the given value and overwrites the
current value with the result. This code represents a typical use case for the
merge method.
The code reads nicely, but there’s still some boilerplate. The parameters
count and incr don’t add much value, and they take up a fair amount of
space. Really, all the lambda tells you is that the function returns the sum of
its two arguments. As of Java 8, Integer (and all the other boxed numerical
primitive types) provides a static method sum that does exactly the same
227
thing. We can simply pass a reference to this method and get the same result
with less visual clutter:
Click here to view code image
map.merge(key, 1, Integer::sum);
The more parameters a method has, the more boilerplate you can eliminate
with a method reference. In some lambdas, however, the parameter names
you choose provide useful documentation, making the lambda more readable
and maintainable than a method reference, even if the lambda is longer.
There’s nothing you can do with a method reference that you can’t also do
with a lambda (with one obscure exception—see JLS, 9.9-2 if you’re
curious). That said, method references usually result in shorter, clearer code.
They also give you an out if a lambda gets too long or complex: You can
extract the code from the lambda into a new method and replace the lambda
with a reference to that method. You can give the method a good name and
document it to your heart’s content.
If you’re programming with an IDE, it will offer to replace a lambda with a
method reference wherever it can. You should usually, but not always, take
the IDE up on the offer. Occasionally, a lambda will be more succinct than a
method reference. This happens most often when the method is in the same
class as the lambda. For example, consider this snippet, which is presumed to
occur in a class named GoshThisClassNameIsHumongous:
Click here to view code image
service.execute(GoshThisClassNameIsHumongous::action);
The lambda equivalent looks like this:
Click here to view code image
service.execute(() -> action());
The snippet using the method reference is neither shorter nor clearer than the
snippet using the lambda, so prefer the latter. Along similar lines, the
Function interface provides a generic static factory method to return the
identity function, Function.identity(). It’s typically shorter and
cleaner not to use this method but to code the equivalent lambda inline: x ->
x.
Many method references refer to static methods, but there are four kinds
that do not. Two of them are bound and unbound instance method references.
In bound references, the receiving object is specified in the method reference.
Bound references are similar in nature to static references: the function object
takes the same arguments as the referenced method. In unbound references,
the receiving object is specified when the function object is applied, via an
228
additional parameter before the method’s declared parameters. Unbound
references are often used as mapping and filter functions in stream pipelines
(Item 45). Finally, there are two kinds of constructor references, for classes
and arrays. Constructor references serve as factory objects. All five kinds of
method references are summarized in the table below:
Method
Ref Type
Example Lambda Equivalent
Static Integer::parseInt str ->
Integer.parseInt(str)
Bound Instant.now()::isAfter Instant then =
Instant.now(); t ->
then.isAfter(t)
Unbound String::toLowerCase str ->
str.toLowerCase()
Class
Constructor
TreeMap<K,V>::new () -> new
TreeMap<K,V>
Array
Constructor
int[]::new len -> new int[len]
In summary, method references often provide a more succinct alternative to
lambdas. Where method references are shorter and clearer, use them;
where they aren’t, stick with lambdas.
Item 44: Favor the use of standard functional interfaces
Now that Java has lambdas, best practices for writing APIs have changed
considerably. For example, the Template Method pattern [Gamma95],
wherein a subclass overrides a primitive method to specialize the behavior of
its superclass, is far less attractive. The modern alternative is to provide a
static factory or constructor that accepts a function object to achieve the same
effect. More generally, you’ll be writing more constructors and methods that
take function objects as parameters. Choosing the right functional parameter
type demands care.
Consider LinkedHashMap. You can use this class as a cache by
overriding its protected removeEldestEntry method, which is invoked
by put each time a new key is added to the map. When this method returns
true, the map removes its eldest entry, which is passed to the method. The
229
following override allows the map to grow to one hundred entries and then
deletes the eldest entry each time a new key is added, maintaining the
hundred most recent entries:
Click here to view code image
protected boolean removeEldestEntry(Map.Entry<K,V>
eldest) {
return size() > 100;
}
This technique works fine, but you can do much better with lambdas. If
LinkedHashMap were written today, it would have a static factory or
constructor that took a function object. Looking at the declaration for
removeEldestEntry, you might think that the function object should
take a Map.Entry<K,V> and return a boolean, but that wouldn’t quite
do it: The removeEldestEntry method calls size() to get the number
of entries in the map, which works because removeEldestEntry is an
instance method on the map. The function object that you pass to the
constructor is not an instance method on the map and can’t capture it because
the map doesn’t exist yet when its factory or constructor is invoked. Thus, the
map must pass itself to the function object, which must therefore take the map
on input as well as its eldest entry. If you were to declare such a functional
interface, it would look something like this:
Click here to view code image
// Unnecessary functional interface; use a standard one
instead.
@FunctionalInterface interface
EldestEntryRemovalFunction<K,V>{
boolean remove(Map<K,V> map, Map.Entry<K,V> eldest);
}
This interface would work fine, but you shouldn’t use it, because you don’t
need to declare a new interface for this purpose. The
java.util.function package provides a large collection of standard
functional interfaces for your use. If one of the standard functional
interfaces does the job, you should generally use it in preference to a
purpose-built functional interface. This will make your API easier to learn,
by reducing its conceptual surface area, and will provide significant
interoperability benefits, as many of the standard functional interfaces provide
useful default methods. The Predicate interface, for instance, provides
methods to combine predicates. In the case of our LinkedHashMap
example, the standard BiPredicate<Map<K,V>, Map.Entry<K,V>>
interface should be used in preference to a custom
230
EldestEntryRemovalFunction interface.
There are forty-three interfaces in java.util.Function. You can’t be
expected to remember them all, but if you remember six basic interfaces, you
can derive the rest when you need them. The basic interfaces operate on
object reference types. The Operator interfaces represent functions whose
result and argument types are the same. The Predicate interface represents
a function that takes an argument and returns a boolean. The Function
interface represents a function whose argument and return types differ. The
Supplier interface represents a function that takes no arguments and
returns (or “supplies”) a value. Finally, Consumer represents a function that
takes an argument and returns nothing, essentially consuming its argument.
The six basic functional interfaces are summarized below:
Interface Function
Signature
Example
UnaryOperator<T> T apply(T t) String::toLowerCase
BinaryOperator<T> T apply(T t1,
T t2)
BigInteger::add
Predicate<T> boolean
test(T t)
Collection::isEmpty
Function<T,R> R apply(T t) Arrays::asList
Supplier<T> T get() Instant::now
Consumer<T> void accept(T
t)
System.out::println
There are also three variants of each of the six basic interfaces to operate
on the primitive types int, long, and double. Their names are derived
from the basic interfaces by prefixing them with a primitive type. So, for
example, a predicate that takes an int is an IntPredicate, and a binary
operator that takes two long values and returns a long is a
LongBinaryOperator. None of these variant types is parameterized
except for the Function variants, which are parameterized by return type.
For example, LongFunction<int[]> takes a long and returns an
int[].
There are nine additional variants of the Function interface, for use
when the result type is primitive. The source and result types always differ,
because a function from a type to itself is a UnaryOperator. If both the
231
source and result types are primitive, prefix Function with
SrcToResult, for example LongToIntFunction (six variants).
If the source is a primitive and the result is an object reference, prefix
Function with <Src>ToObj, for example DoubleToObjFunction
(three variants).
There are two-argument versions of the three basic functional interfaces for
which it makes sense to have them: BiPredicate<T,U>,
BiFunction<T,U,R>, and BiConsumer<T,U>. There are also
BiFunction variants returning the three relevant primitive types:
ToIntBiFunction<T,U>, ToLongBiFunction<T,U>, and
ToDoubleBiFunction<T,U>. There are two-argument variants of
Consumer that take one object reference and one primitive type:
ObjDoubleConsumer<T>, ObjIntConsumer<T>, and
ObjLongConsumer<T>. In total, there are nine two-argument versions of
the basic interfaces.
Finally, there is the BooleanSupplier interface, a variant of
Supplier that returns boolean values. This is the only explicit mention
of the boolean type in any of the standard functional interface names, but
boolean return values are supported via Predicate and its four variant
forms. The BooleanSupplier interface and the forty-two interfaces
described in the previous paragraphs account for all forty-three standard
functional interfaces. Admittedly, this is a lot to swallow, and not terribly
orthogonal. On the other hand, the bulk of the functional interfaces that you’ll
need have been written for you and their names are regular enough that you
shouldn’t have too much trouble coming up with one when you need it.
Most of the standard functional interfaces exist only to provide support for
primitive types. Don’t be tempted to use basic functional interfaces with
boxed primitives instead of primitive functional interfaces. While it
works, it violates the advice of Item 61, “prefer primitive types to boxed
primitives.” The performance consequences of using boxed primitives for
bulk operations can be deadly.
Now you know that you should typically use standard functional interfaces
in preference to writing your own. But when should you write your own? Of
course you need to write your own if none of the standard ones does what you
need, for example if you require a predicate that takes three parameters, or
one that throws a checked exception. But there are times you should write
your own functional interface even when one of the standard ones is
structurally identical.
Consider our old friend Comparator<T>, which is structurally identical
to the ToIntBiFunction<T,T> interface. Even if the latter interface had
existed when the former was added to the libraries, it would have been wrong
232
to use it. There are several reasons that Comparator deserves its own
interface. First, its name provides excellent documentation every time it is
used in an API, and it’s used a lot. Second, the Comparator interface has
strong requirements on what constitutes a valid instance, which comprise its
general contract. By implementing the interface, you are pledging to adhere
to its contract. Third, the interface is heavily outfitted with useful default
methods to transform and combine comparators.
You should seriously consider writing a purpose-built functional interface
in preference to using a standard one if you need a functional interface that
shares one or more of the following characteristics with Comparator:
• It will be commonly used and could benefit from a descriptive name.
• It has a strong contract associated with it.
• It would benefit from custom default methods.
If you elect to write your own functional interface, remember that it’s an
interface and hence should be designed with great care (Item 21).
Notice that the EldestEntryRemovalFunction interface (page 199)
is labeled with the @FunctionalInterface annotation. This annotation
type is similar in spirit to @Override. It is a statement of programmer intent
that serves three purposes: it tells readers of the class and its documentation
that the interface was designed to enable lambdas; it keeps you honest
because the interface won’t compile unless it has exactly one abstract method;
and it prevents maintainers from accidentally adding abstract methods to the
interface as it evolves. Always annotate your functional interfaces with the
@FunctionalInterface annotation.
A final point should be made concerning the use of functional interfaces in
APIs. Do not provide a method with multiple overloadings that take different
functional interfaces in the same argument position if it could create a
possible ambiguity in the client. This is not just a theoretical problem. The
submit method of ExecutorService can take either a Callable<T>
or a Runnable, and it is possible to write a client program that requires a
cast to indicate the correct overloading (Item 52). The easiest way to avoid
this problem is not to write overloadings that take different functional
interfaces in the same argument position. This is a special case of the advice
in Item 52, “use overloading judiciously.”
In summary, now that Java has lambdas, it is imperative that you design
your APIs with lambdas in mind. Accept functional interface types on input
and return them on output. It is generally best to use the standard interfaces
provided in java.util.function.Function, but keep your eyes open
for the relatively rare cases where you would be better off writing your own
functional interface.
233
Item 45: Use streams judiciously
The streams API was added in Java 8 to ease the task of performing bulk
operations, sequentially or in parallel. This API provides two key
abstractions: the stream, which represents a finite or infinite sequence of data
elements, and the stream pipeline, which represents a multistage computation
on these elements. The elements in a stream can come from anywhere.
Common sources include collections, arrays, files, regular expression pattern
matchers, pseudorandom number generators, and other streams. The data
elements in a stream can be object references or primitive values. Three
primitive types are supported: int, long, and double.
A stream pipeline consists of a source stream followed by zero or more
intermediate operations and one terminal operation. Each intermediate
operation transforms the stream in some way, such as mapping each element
to a function of that element or filtering out all elements that do not satisfy
some condition. Intermediate operations all transform one stream into
another, whose element type may be the same as the input stream or different
from it. The terminal operation performs a final computation on the stream
resulting from the last intermediate operation, such as storing its elements into
a collection, returning a certain element, or printing all of its elements.
Stream pipelines are evaluated lazily: evaluation doesn’t start until the
terminal operation is invoked, and data elements that aren’t required in order
to complete the terminal operation are never computed. This lazy evaluation
is what makes it possible to work with infinite streams. Note that a stream
pipeline without a terminal operation is a silent no-op, so don’t forget to
include one.
The streams API is fluent: it is designed to allow all of the calls that
comprise a pipeline to be chained into a single expression. In fact, multiple
pipelines can be chained together into a single expression.
By default, stream pipelines run sequentially. Making a pipeline execute in
parallel is as simple as invoking the parallel method on any stream in the
pipeline, but it is seldom appropriate to do so (Item 48).
The streams API is sufficiently versatile that practically any computation
can be performed using streams, but just because you can doesn’t mean you
should. When used appropriately, streams can make programs shorter and
clearer; when used inappropriately, they can make programs difficult to read
and maintain. There are no hard and fast rules for when to use streams, but
there are heuristics.
Consider the following program, which reads the words from a dictionary
file and prints all the anagram groups whose size meets a user-specified
minimum. Recall that two words are anagrams if they consist of the same
letters in a different order. The program reads each word from a user234
specified dictionary file and places the words into a map. The map key is the
word with its letters alphabetized, so the key for "staple" is "aelpst",
and the key for "petals" is also "aelpst": the two words are anagrams,
and all anagrams share the same alphabetized form (or alphagram, as it is
sometimes known). The map value is a list containing all of the words that
share an alphabetized form. After the dictionary has been processed, each list
is a complete anagram group. The program then iterates through the map’s
values() view and prints each list whose size meets the threshold:
Click here to view code image
// Prints all large anagram groups in a dictionary
iteratively
public class Anagrams {
public static void main(String[] args) throws
IOException {
File dictionary = new File(args[0]);
int minGroupSize = Integer.parseInt(args[1]);
Map<String, Set<String>> groups = new HashMap<>
();
try (Scanner s = new Scanner(dictionary)) {
while (s.hasNext()) {
String word = s.next();
groups.computeIfAbsent(alphabetize(word),
(unused) -> new TreeSet<>
()).add(word);
}
}
for (Set<String> group : groups.values())
if (group.size() >= minGroupSize)
System.out.println(group.size() + ": " +
group);
}
private static String alphabetize(String s) {
char[] a = s.toCharArray();
Arrays.sort(a);
return new String(a);
}
}
One step in this program is worthy of note. The insertion of each word into
the map, which is shown in bold, uses the computeIfAbsent method,
which was added in Java 8. This method looks up a key in the map: If the key
is present, the method simply returns the value associated with it. If not, the
235
method computes a value by applying the given function object to the key,
associates this value with the key, and returns the computed value. The
computeIfAbsent method simplifies the implementation of maps that
associate multiple values with each key.
Now consider the following program, which solves the same problem, but
makes heavy use of streams. Note that the entire program, with the exception
of the code that opens the dictionary file, is contained in a single expression.
The only reason the dictionary is opened in a separate expression is to allow
the use of the try-with-resources statement, which ensures that the
dictionary file is closed:
Click here to view code image
// Overuse of streams - don't do this!
public class Anagrams {
public static void main(String[] args) throws
IOException {
Path dictionary = Paths.get(args[0]);
int minGroupSize = Integer.parseInt(args[1]);
try (Stream<String> words =
Files.lines(dictionary)) {
words.collect(
groupingBy(word -> word.chars().sorted()
.collect(StringBuilder::new,
(sb, c) -> sb.append((char) c),
StringBuilder::append).toString()))
.values().stream()
.filter(group -> group.size() >=
minGroupSize)
.map(group -> group.size() + ": " + group)
.forEach(System.out::println);
}
}
}
If you find this code hard to read, don’t worry; you’re not alone. It is shorter,
but it is also less readable, especially to programmers who are not experts in
the use of streams. Overusing streams makes programs hard to read and
maintain.
Luckily, there is a happy medium. The following program solves the same
problem, using streams without overusing them. The result is a program that’s
both shorter and clearer than the original:
Click here to view code image
// Tasteful use of streams enhances clarity and
conciseness
236
public class Anagrams {
public static void main(String[] args) throws
IOException {
Path dictionary = Paths.get(args[0]);
int minGroupSize = Integer.parseInt(args[1]);
try (Stream<String> words =
Files.lines(dictionary)) {
words.collect(groupingBy(word ->
alphabetize(word)))
.values().stream()
.filter(group -> group.size() >=
minGroupSize)
.forEach(g -> System.out.println(g.size() +
": " + g));
}
}
// alphabetize method is the same as in original
version
}
Even if you have little previous exposure to streams, this program is not
hard to understand. It opens the dictionary file in a try-with-resources block,
obtaining a stream consisting of all the lines in the file. The stream variable
is named words to suggest that each element in the stream is a word. The
pipeline on this stream has no intermediate operations; its terminal operation
collects all the words into a map that groups the words by their alphabetized
form (Item 46). This is exactly the same map that was constructed in both
previous versions of the program. Then a new Stream<List<String>>
is opened on the values() view of the map. The elements in this stream
are, of course, the anagram groups. The stream is filtered so that all of the
groups whose size is less than minGroupSize are ignored, and finally, the
remaining groups are printed by the terminal operation forEach.
Note that the lambda parameter names were chosen carefully. The
parameter g should really be named group, but the resulting line of code
would be too wide for the book. In the absence of explicit types, careful
naming of lambda parameters is essential to the readability of stream
pipelines.
Note also that word alphabetization is done in a separate alphabetize
method. This enhances readability by providing a name for the operation and
keeping implementation details out of the main program. Using helper
methods is even more important for readability in stream pipelines than
in iterative code because pipelines lack explicit type information and named
237
temporary variables.
The alphabetize method could have been reimplemented to use
streams, but a stream-based alphabetize method would have been less
clear, more difficult to write correctly, and probably slower. These
deficiencies result from Java’s lack of support for primitive char streams
(which is not to imply that Java should have supported char streams; it
would have been infeasible to do so). To demonstrate the hazards of
processing char values with streams, consider the following code:
Click here to view code image
"Hello world!".chars().forEach(System.out::print);
You might expect it to print Hello world!, but if you run it, you’ll find
that it prints 721011081081113211911111410810033. This happens
because the elements of the stream returned by "Hello
world!".chars() are not char values but int values, so the int
overloading of print is invoked. It is admittedly confusing that a method
named chars returns a stream of int values. You could fix the program by
using a cast to force the invocation of the correct overloading:
Click here to view code image
"Hello world!".chars().forEach(x ->
System.out.print((char) x));
but ideally you should refrain from using streams to process char values.
When you start using streams, you may feel the urge to convert all your
loops into streams, but resist the urge. While it may be possible, it will likely
harm the readability and maintainability of your code base. As a rule, even
moderately complex tasks are best accomplished using some combination of
streams and iteration, as illustrated by the Anagrams programs above. So
refactor existing code to use streams and use them in new code only
where it makes sense to do so.
As shown in the programs in this item, stream pipelines express repeated
computation using function objects (typically lambdas or method references),
while iterative code expresses repeated computation using code blocks. There
are some things you can do from code blocks that you can’t do from function
objects:
• From a code block, you can read or modify any local variable in scope;
from a lambda, you can only read final or effectively final variables [JLS
4.12.4], and you can’t modify any local variables.
• From a code block, you can return from the enclosing method, break
or continue an enclosing loop, or throw any checked exception that this
238
method is declared to throw; from a lambda you can do none of these
things.
If a computation is best expressed using these techniques, then it’s probably
not a good match for streams. Conversely, streams make it very easy to do
some things:
• Uniformly transform sequences of elements
• Filter sequences of elements
• Combine sequences of elements using a single operation (for example to
add them, concatenate them, or compute their minimum)
• Accumulate sequences of elements into a collection, perhaps grouping
them by some common attribute
• Search a sequence of elements for an element satisfying some criterion
If a computation is best expressed using these techniques, then it is a good
candidate for streams.
One thing that is hard to do with streams is to access corresponding
elements from multiple stages of a pipeline simultaneously: once you map a
value to some other value, the original value is lost. One workaround is to
map each value to a pair object containing the original value and the new
value, but this is not a satisfying solution, especially if the pair objects are
required for multiple stages of a pipeline. The resulting code is messy and
verbose, which defeats a primary purpose of streams. When it is applicable, a
better workaround is to invert the mapping when you need access to the
earlier-stage value.
For example, let’s write a program to print the first twenty Mersenne
primes. To refresh your memory, a Mersenne number is a number of the form
2
p − 1. If p is prime, the corresponding Mersenne number may be prime; if so,
it’s a Mersenne prime. As the initial stream in our pipeline, we want all the
prime numbers. Here’s a method to return that (infinite) stream. We assume a
static import has been used for easy access to the static members of
BigInteger:
Click here to view code image
static Stream<BigInteger> primes() {
return Stream.iterate(TWO,
BigInteger::nextProbablePrime);
}
The name of the method (primes) is a plural noun describing the elements
of the stream. This naming convention is highly recommended for all
methods that return streams because it enhances the readability of stream
pipelines. The method uses the static factory Stream.iterate, which
239
takes two parameters: the first element in the stream, and a function to
generate the next element in the stream from the previous one. Here is the
program to print the first twenty Mersenne primes:
Click here to view code image
public static void main(String[] args) {
primes().map(p ->
TWO.pow(p.intValueExact()).subtract(ONE))
.filter(mersenne ->
mersenne.isProbablePrime(50))
.limit(20)
.forEach(System.out::println);
}
This program is a straightforward encoding of the prose description above: it
starts with the primes, computes the corresponding Mersenne numbers, filters
out all but the primes (the magic number 50 controls the probabilistic
primality test), limits the resulting stream to twenty elements, and prints them
out.
Now suppose that we want to precede each Mersenne prime with its
exponent (p). This value is present only in the initial stream, so it is
inaccessible in the terminal operation, which prints the results. Luckily, it’s
easy to compute the exponent of a Mersenne number by inverting the
mapping that took place in the first intermediate operation. The exponent is
simply the number of bits in the binary representation, so this terminal
operation generates the desired result:
Click here to view code image
.forEach(mp -> System.out.println(mp.bitLength() + ": "
+ mp));
There are plenty of tasks where it is not obvious whether to use streams or
iteration. For example, consider the task of initializing a new deck of cards.
Assume that Card is an immutable value class that encapsulates a Rank and
a Suit, both of which are enum types. This task is representative of any task
that requires computing all the pairs of elements that can be chosen from two
sets. Mathematicians call this the Cartesian product of the two sets. Here’s an
iterative implementation with a nested for-each loop that should look very
familiar to you:
Click here to view code image
// Iterative Cartesian product computation
private static List<Card> newDeck() {
List<Card> result = new ArrayList<>();
for (Suit suit : Suit.values())
240
for (Rank rank : Rank.values())
result.add(new Card(suit, rank));
return result;
}
And here is a stream-based implementation that makes use of the intermediate
operation flatMap. This operation maps each element in a stream to a
stream and then concatenates all of these new streams into a single stream (or
flattens them). Note that this implementation contains a nested lambda, shown
in boldface:
Click here to view code image
// Stream-based Cartesian product computation
private static List<Card> newDeck() {
return Stream.of(Suit.values())
.flatMap(suit ->
Stream.of(Rank.values())
.map(rank -> new Card(suit, rank)))
.collect(toList());
}
Which of the two versions of newDeck is better? It boils down to personal
preference and the environment in which you’re programming. The first
version is simpler and perhaps feels more natural. A larger fraction of Java
programmers will be able to understand and maintain it, but some
programmers will feel more comfortable with the second (stream-based)
version. It’s a bit more concise and not too difficult to understand if you’re
reasonably well-versed in streams and functional programming. If you’re not
sure which version you prefer, the iterative version is probably the safer
choice. If you prefer the stream version and you believe that other
programmers who will work with the code will share your preference, then
you should use it.
In summary, some tasks are best accomplished with streams, and others
with iteration. Many tasks are best accomplished by combining the two
approaches. There are no hard and fast rules for choosing which approach to
use for a task, but there are some useful heuristics. In many cases, it will be
clear which approach to use; in some cases, it won’t. If you’re not sure
whether a task is better served by streams or iteration, try both and see
which works better.
Item 46: Prefer side-effect-free functions in streams
If you’re new to streams, it can be difficult to get the hang of them. Merely
expressing your computation as a stream pipeline can be hard. When you
241
succeed, your program will run, but you may realize little if any benefit.
Streams isn’t just an API, it’s a paradigm based on functional programming.
In order to obtain the expressiveness, speed, and in some cases
parallelizability that streams have to offer, you have to adopt the paradigm as
well as the API.
The most important part of the streams paradigm is to structure your
computation as a sequence of transformations where the result of each stage is
as close as possible to a pure function of the result of the previous stage. A
pure function is one whose result depends only on its input: it does not
depend on any mutable state, nor does it update any state. In order to achieve
this, any function objects that you pass into stream operations, both
intermediate and terminal, should be free of side-effects.
Occasionally, you may see streams code that looks like this snippet, which
builds a frequency table of the words in a text file:
Click here to view code image
// Uses the streams API but not the paradigm--Don't do
this!
Map<String, Long> freq = new HashMap<>();
try (Stream<String> words = new Scanner(file).tokens())
{
words.forEach(word -> {
freq.merge(word.toLowerCase(), 1L, Long::sum);
});
}
What’s wrong with this code? After all, it uses streams, lambdas, and
method references, and gets the right answer. Simply put, it’s not streams
code at all; it’s iterative code masquerading as streams code. It derives no
benefits from the streams API, and it’s (a bit) longer, harder to read, and less
maintainable than the corresponding iterative code. The problem stems from
the fact that this code is doing all its work in a terminal forEach operation,
using a lambda that mutates external state (the frequency table). A forEach
operation that does anything more than present the result of the computation
performed by a stream is a “bad smell in code,” as is a lambda that mutates
state. So how should this code look?
Click here to view code image
// Proper use of streams to initialize a frequency table
Map<String, Long> freq;
try (Stream<String> words = new Scanner(file).tokens())
{
freq = words
.collect(groupingBy(String::toLowerCase,
242
counting()));
}
This snippet does the same thing as the previous one but makes proper use
of the streams API. It’s shorter and clearer. So why would anyone write it the
other way? Because it uses tools they’re already familiar with. Java
programmers know how to use for-each loops, and the forEach terminal
operation is similar. But the forEach operation is among the least powerful
of the terminal operations and the least stream-friendly. It’s explicitly
iterative, and hence not amenable to parallelization. The forEach
operation should be used only to report the result of a stream
computation, not to perform the computation. Occasionally, it makes
sense to use forEach for some other purpose, such as adding the results of a
stream computation to a preexisting collection.
The improved code uses a collector, which is a new concept that you have
to learn in order to use streams. The Collectors API is intimidating: it has
thirty-nine methods, some of which have as many as five type parameters.
The good news is that you can derive most of the benefit from this API
without delving into its full complexity. For starters, you can ignore the
Collector interface and think of a collector as an opaque object that
encapsulates a reduction strategy. In this context, reduction means combining
the elements of a stream into a single object. The object produced by a
collector is typically a collection (which accounts for the name collector).
The collectors for gathering the elements of a stream into a true
Collection are straightforward. There are three such collectors:
toList(), toSet(), and toCollection(collectionFactory).
They return, respectively, a set, a list, and a programmer-specified collection
type. Armed with this knowledge, we can write a stream pipeline to extract a
top-ten list from our frequency table.
Click here to view code image
// Pipeline to get a top-ten list of words from a
frequency table
List<String> topTen = freq.keySet().stream()
.sorted(comparing(freq::get).reversed())
.limit(10)
.collect(toList());
Note that we haven’t qualified the toList method with its class,
Collectors. It is customary and wise to statically import all members
of Collectors because it makes stream pipelines more readable.
The only tricky part of this code is the comparator that we pass to sorted,
comparing(freq::get).reversed(). The comparing method is a
243
comparator construction method (Item 14) that takes a key extraction
function. The function takes a word, and the “extraction” is actually a table
lookup: the bound method reference freq::get looks up the word in the
frequency table and returns the number of times the word appears in the file.
Finally, we call reversed on the comparator, so we’re sorting the words
from most frequent to least frequent. Then it’s a simple matter to limit the
stream to ten words and collect them into a list.
The previous code snippets use Scanner’s stream method to get a
stream over the scanner. This method was added in Java 9. If you’re using an
earlier release, you can translate the scanner, which implements Iterator,
into a stream using an adapter similar to the one in Item 47
(streamOf(Iterable<E>)).
So what about the other thirty-six methods in Collectors? Most of
them exist to let you collect streams into maps, which is far more complicated
than collecting them into true collections. Each stream element is associated
with a key and a value, and multiple stream elements can be associated with
the same key.
The simplest map collector is toMap(keyMapper, valueMapper),
which takes two functions, one of which maps a stream element to a key, the
other, to a value. We used this collector in our fromString implementation
in Item 34 to make a map from the string form of an enum to the enum itself:
Click here to view code image
// Using a toMap collector to make a map from string to
enum
private static final Map<String, Operation> stringToEnum
=
Stream.of(values()).collect(
toMap(Object::toString, e -> e));
This simple form of toMap is perfect if each element in the stream maps to a
unique key. If multiple stream elements map to the same key, the pipeline will
terminate with an IllegalStateException.
The more complicated forms of toMap, as well as the groupingBy
method, give you various ways to provide strategies for dealing with such
collisions. One way is to provide the toMap method with a merge function in
addition to its key and value mappers. The merge function is a
BinaryOperator<V>, where V is the value type of the map. Any
additional values associated with a key are combined with the existing value
using the merge function, so, for example, if the merge function is
multiplication, you end up with a value that is the product of all the values
associated with the key by the value mapper.
244
The three-argument form of toMap is also useful to make a map from a
key to a chosen element associated with that key. For example, suppose we
have a stream of record albums by various artists, and we want a map from
recording artist to best-selling album. This collector will do the job.
Click here to view code image
// Collector to generate a map from key to chosen
element for key
Map<Artist, Album> topHits = albums.collect(
toMap(Album::artist, a->a,
maxBy(comparing(Album::sales))));
Note that the comparator uses the static factory method maxBy, which is
statically imported from BinaryOperator. This method converts a
Comparator<T> into a BinaryOperator<T> that computes the
maximum implied by the specified comparator. In this case, the comparator is
returned by the comparator construction method comparing, which takes
the key extractor function Album::sales. This may seem a bit convoluted,
but the code reads nicely. Loosely speaking, it says, “convert the stream of
albums to a map, mapping each artist to the album that has the best album by
sales.” This is surprisingly close to the problem statement.
Another use of the three-argument form of toMap is to produce a collector
that imposes a last-write-wins policy when there are collisions. For many
streams, the results will be nondeterministic, but if all the values that may be
associated with a key by the mapping functions are identical, or if they are all
acceptable, this collector’s s behavior may be just what you want:
Click here to view code image
// Collector to impose last-write-wins policy
toMap(keyMapper, valueMapper, (v1, v2) -> v2)
The third and final version of toMap takes a fourth argument, which is a
map factory, for use when you want to specify a particular map
implementation such as an EnumMap or a TreeMap.
There are also variant forms of the first three versions of toMap, named
toConcurrentMap, that run efficiently in parallel and produce
ConcurrentHashMap instances.
In addition to the toMap method, the Collectors API provides the
groupingBy method, which returns collectors to produce maps that group
elements into categories based on a classifier function. The classifier function
takes an element and returns the category into which it falls. This category
serves as the element’s map key. The simplest version of the groupingBy
method takes only a classifier and returns a map whose values are lists of all
245
the elements in each category. This is the collector that we used in the
Anagram program in Item 45 to generate a map from alphabetized word to a
list of the words sharing the alphabetization:
Click here to view code image
words.collect(groupingBy(word -> alphabetize(word)))
If you want groupingBy to return a collector that produces a map with
values other than lists, you can specify a downstream collector in addition to
a classifier. A downstream collector produces a value from a stream
containing all the elements in a category. The simplest use of this parameter is
to pass toSet(), which results in a map whose values are sets of elements
rather than lists.
Alternatively, you can pass toCollection(collectionFactory),
which lets you create the collections into which each category of elements is
placed. This gives you the flexibility to choose any collection type you want.
Another simple use of the two-argument form of groupingBy is to pass
counting() as the downstream collector. This results in a map that
associates each category with the number of elements in the category, rather
than a collection containing the elements. That’s what you saw in the
frequency table example at the beginning of this item:
Click here to view code image
Map<String, Long> freq = words
.collect(groupingBy(String::toLowerCase,
counting()));
The third version of groupingBy lets you specify a map factory in
addition to a downstream collector. Note that this method violates the
standard telescoping argument list pattern: the mapFactory parameter
precedes, rather than follows, the downStream parameter. This version of
groupingBy gives you control over the containing map as well as the
contained collections, so, for example, you can specify a collector that returns
a TreeMap whose values are TreeSets.
The groupingByConcurrent method provides variants of all three
overloadings of groupingBy. These variants run efficiently in parallel and
produce ConcurrentHashMap instances. There is also a rarely used
relative of groupingBy called partitioningBy. In lieu of a classifier
method, it takes a predicate and returns a map whose key is a Boolean.
There are two overloadings of this method, one of which takes a downstream
collector in addition to a predicate.
The collectors returned by the counting method are intended only for
use as downstream collectors. The same functionality is available directly on
246
Stream, via the count method, so there is never a reason to say
collect(counting()). There are fifteen more Collectors methods
with this property. They include the nine methods whose names begin with
summing, averaging, and summarizing (whose functionality is
available on the corresponding primitive stream types). They also include all
overloadings of the reducing method, and the filtering, mapping,
flatMapping, and collectingAndThen methods. Most programmers
can safely ignore the majority of these methods. From a design perspective,
these collectors represent an attempt to partially duplicate the functionality of
streams in collectors so that downstream collectors can act as “ministreams.”
There are three Collectors methods we have yet to mention. Though
they are in Collectors, they don’t involve collections. The first two are
minBy and maxBy, which take a comparator and return the minimum or
maximum element in the stream as determined by the comparator. They are
minor generalizations of the min and max methods in the Stream interface
and are the collector analogues of the binary operators returned by the likenamed methods in BinaryOperator. Recall that we used
BinaryOperator.maxBy in our best-selling album example.
The final Collectors method is joining, which operates only on
streams of CharSequence instances such as strings. In its parameterless
form, it returns a collector that simply concatenates the elements. Its one
argument form takes a single CharSequence parameter named
delimiter and returns a collector that joins the stream elements, inserting
the delimiter between adjacent elements. If you pass in a comma as the
delimiter, the collector returns a comma-separated values string (but beware
that the string will be ambiguous if any of the elements in the stream contain
commas). The three argument form takes a prefix and suffix in addition to the
delimiter. The resulting collector generates strings like the ones that you get
when you print a collection, for example [came, saw, conquered].
In summary, the essence of programming stream pipelines is side-effectfree function objects. This applies to all of the many function objects passed
to streams and related objects. The terminal operation forEach should only
be used to report the result of a computation performed by a stream, not to
perform the computation. In order to use streams properly, you have to know
about collectors. The most important collector factories are toList, toSet,
toMap, groupingBy, and joining.
Item 47: Prefer Collection to Stream as a return type
Many methods return sequences of elements. Prior to Java 8, the obvious
return types for such methods were the collection interfaces Collection,
247
Set, and List; Iterable; and the array types. Usually, it was easy to
decide which of these types to return. The norm was a collection interface. If
the method existed solely to enable for-each loops or the returned sequence
couldn’t be made to implement some Collection method (typically,
contains(Object)), the Iterable interface was used. If the returned
elements were primitive values or there were stringent performance
requirements, arrays were used. In Java 8, streams were added to the
platform, substantially complicating the task of choosing the appropriate
return type for a sequence-returning method.
You may hear it said that streams are now the obvious choice to return a
sequence of elements, but as discussed in Item 45, streams do not make
iteration obsolete: writing good code requires combining streams and iteration
judiciously. If an API returns only a stream and some users want to iterate
over the returned sequence with a for-each loop, those users will be justifiably
upset. It is especially frustrating because the Stream interface contains the
sole abstract method in the Iterable interface, and Stream’s
specification for this method is compatible with Iterable’s. The only thing
preventing programmers from using a for-each loop to iterate over a stream is
Stream’s failure to extend Iterable.
Sadly, there is no good workaround for this problem. At first glance, it
might appear that passing a method reference to Stream’s iterator
method would work. The resulting code is perhaps a bit noisy and opaque, but
not unreasonable:
Click here to view code image
// Won't compile, due to limitations on Java's type
inference
for (ProcessHandle ph :
ProcessHandle.allProcesses()::iterator) {
// Process the process
}
Unfortunately, if you attempt to compile this code, you’ll get an error
message:
Click here to view code image
Test.java:6: error: method reference not expected here
for (ProcessHandle ph :
ProcessHandle.allProcesses()::iterator) {
^
In order to make the code compile, you have to cast the method reference to
an appropriately parameterized Iterable:
Click here to view code image
248
// Hideous workaround to iterate over a stream
for (ProcessHandle ph : (Iterable<ProcessHandle>)
ProcessHandle.allProcesses()::iterator)
This client code works, but it is too noisy and opaque to use in practice. A
better workaround is to use an adapter method. The JDK does not provide
such a method, but it’s easy to write one, using the same technique used inline in the snippets above. Note that no cast is necessary in the adapter
method because Java’s type inference works properly in this context:
Click here to view code image
// Adapter from Stream<E> to Iterable<E>
public static <E> Iterable<E> iterableOf(Stream<E>
stream) {
return stream::iterator;
}
With this adapter, you can iterate over any stream with a for-each statement:
Click here to view code image
for (ProcessHandle p :
iterableOf(ProcessHandle.allProcesses())) {
// Process the process
}
Note that the stream versions of the Anagrams program in Item 34 use the
Files.lines method to read the dictionary, while the iterative version
uses a scanner. The Files.lines method is superior to a scanner, which
silently swallows any exceptions encountered while reading the file. Ideally,
we would have used Files.lines in the iterative version too. This is the
sort of compromise that programmers will make if an API provides only
stream access to a sequence and they want to iterate over the sequence with a
for-each statement.
Conversely, a programmer who wants to process a sequence using a stream
pipeline will be justifiably upset by an API that provides only an Iterable.
Again the JDK does not provide an adapter, but it’s easy enough to write one:
Click here to view code image
// Adapter from Iterable<E> to Stream<E>
public static <E> Stream<E> streamOf(Iterable<E>
iterable) {
return StreamSupport.stream(iterable.spliterator(),
false);
}
If you’re writing a method that returns a sequence of objects and you know
249
that it will only be used in a stream pipeline, then of course you should feel
free to return a stream. Similarly, a method returning a sequence that will only
be used for iteration should return an Iterable. But if you’re writing a
public API that returns a sequence, you should provide for users who want to
write stream pipelines as well as those who want to write for-each statements,
unless you have a good reason to believe that most of your users will want to
use the same mechanism.
The Collection interface is a subtype of Iterable and has a
stream method, so it provides for both iteration and stream access.
Therefore, Collection or an appropriate subtype is generally the best
return type for a public, sequence-returning method. Arrays also provide
for easy iteration and stream access with the Arrays.asList and
Stream.of methods. If the sequence you’re returning is small enough to fit
easily in memory, you’re probably best off returning one of the standard
collection implementations, such as ArrayList or HashSet. But do not
store a large sequence in memory just to return it as a collection.
If the sequence you’re returning is large but can be represented concisely,
consider implementing a special-purpose collection. For example, suppose
you want to return the power set of a given set, which consists of all of its
subsets. The power set of {a, b, c} is {{}, {a}, {b}, {c}, {a, b}, {a, c}, {b, c},
{a, b, c}}. If a set has n elements, its power set has 2
n
. Therefore, you
shouldn’t even consider storing the power set in a standard collection
implementation. It is, however, easy to implement a custom collection for the
job with the help of AbstractList.
The trick is to use the index of each element in the power set as a bit
vector, where the nth bit in the index indicates the presence or absence of the
nth element from the source set. In essence, there is a natural mapping
between the binary numbers from 0 to 2
n − 1 and the power set of an nelement set. Here’s the code:
Click here to view code image
// Returns the power set of an input set as custom
collection
public class PowerSet {
public static final <E> Collection<Set<E>> of(Set<E>
s) {
List<E> src = new ArrayList<>(s);
if (src.size() > 30)
throw new IllegalArgumentException("Set too big
" + s);
return new AbstractList<Set<E>>() {
@Override public int size() {
return 1 << src.size(); // 2 to the power
250
srcSize
}
@Override public boolean contains(Object o) {
return o instanceof Set &&
src.containsAll((Set)o);
}
@Override public Set<E> get(int index) {
Set<E> result = new HashSet<>();
for (int i = 0; index != 0; i++, index >>=
1)
if ((index & 1) == 1)
result.add(src.get(i));
return result;
}
};
}
}
Note that PowerSet.of throws an exception if the input set has more
than 30 elements. This highlights a disadvantage of using Collection as a
return type rather than Stream or Iterable: Collection has an intreturning size method, which limits the length of the returned sequence to
Integer.MAX_VALUE, or 2
31 − 1. The Collection specification does
allow the size method to return 2
31 − 1 if the collection is larger, even
infinite, but this is not a wholly satisfying solution.
In order to write a Collection implementation atop
AbstractCollection, you need implement only two methods beyond
the one required for Iterable: contains and size. Often it’s easy to
write efficient implementations of these methods. If it isn’t feasible, perhaps
because the contents of the sequence aren’t predetermined before iteration
takes place, return a stream or iterable, whichever feels more natural. If you
choose, you can return both using two separate methods.
There are times when you’ll choose the return type based solely on ease of
implementation. For example, suppose you want to write a method that
returns all of the (contiguous) sublists of an input list. It takes only three lines
of code to generate these sublists and put them in a standard collection, but
the memory required to hold this collection is quadratic in the size of the
source list. While this is not as bad as the power set, which is exponential, it is
clearly unacceptable. Implementing a custom collection, as we did for the
power set, would be tedious, more so because the JDK lacks a skeletal
Iterator implementation to help us.
It is, however, straightforward to implement a stream of all the sublists of
251
an input list, though it does require a minor insight. Let’s call a sublist that
contains the first element of a list a prefix of the list. For example, the prefixes
of (a, b, c) are (a), (a, b), and (a, b, c). Similarly, let’s call a sublist that
contains the last element a suffix, so the suffixes of (a, b, c) are (a, b, c), (b,
c), and (c). The insight is that the sublists of a list are simply the suffixes of
the prefixes (or identically, the prefixes of the suffixes) and the empty list.
This observation leads directly to a clear, reasonably concise implementation:
Click here to view code image
// Returns a stream of all the sublists of its input
list
public class SubLists {
public static <E> Stream<List<E>> of(List<E> list) {
return
Stream.concat(Stream.of(Collections.emptyList()),
prefixes(list).flatMap(SubLists::suffixes));
}
private static <E> Stream<List<E>> prefixes(List<E>
list) {
return IntStream.rangeClosed(1, list.size())
.mapToObj(end -> list.subList(0, end));
}
private static <E> Stream<List<E>> suffixes(List<E>
list) {
return IntStream.range(0, list.size())
.mapToObj(start -> list.subList(start,
list.size()));
}
}
Note that the Stream.concat method is used to add the empty list into
the returned stream. Also note that the flatMap method (Item 45) is used to
generate a single stream consisting of all the suffixes of all the prefixes.
Finally, note that we generate the prefixes and suffixes by mapping a stream
of consecutive int values returned by IntStream.range and
IntStream.rangeClosed. This idiom is, roughly speaking, the stream
equivalent of the standard for-loop on integer indices. Thus, our sublist
implementation is similar in spirit to the obvious nested for-loop:
Click here to view code image
for (int start = 0; start < src.size(); start++)
for (int end = start + 1; end <= src.size(); end++)
System.out.println(src.subList(start, end));
252
It is possible to translate this for-loop directly into a stream. The result is
more concise than our previous implementation, but perhaps a bit less
readable. It is similar in spirit to the streams code for the Cartesian product in
Item 45:
Click here to view code image
// Returns a stream of all the sublists of its input
list
public static <E> Stream<List<E>> of(List<E> list) {
return IntStream.range(0, list.size())
.mapToObj(start ->
IntStream.rangeClosed(start + 1, list.size())
.mapToObj(end -> list.subList(start, end)))
.flatMap(x -> x);
}
Like the for-loop that precedes it, this code does not emit the empty list. In
order to fix this deficiency, you could either use concat, as we did in the
previous version, or replace 1 by (int) Math.signum(start) in the
rangeClosed call.
Either of these stream implementations of sublists is fine, but both will
require some users to employ a Stream-to-Iterable adapter or to use a
stream in places where iteration would be more natural. Not only does the
Stream-to-Iterable adapter clutter up client code, but it slows down the
loop by a factor of 2.3 on my machine. A purpose-built Collection
implementation (not shown here) is considerably more verbose but runs about
1.4 times as fast as our stream-based implementation on my machine.
In summary, when writing a method that returns a sequence of elements,
remember that some of your users may want to process them as a stream
while others may want to iterate over them. Try to accommodate both groups.
If it’s feasible to return a collection, do so. If you already have the elements in
a collection or the number of elements in the sequence is small enough to
justify creating a new one, return a standard collection such as ArrayList.
Otherwise, consider implementing a custom collection as we did for the
power set. If it isn’t feasible to return a collection, return a stream or iterable,
whichever seems more natural. If, in a future Java release, the Stream
interface declaration is modified to extend Iterable, then you should feel
free to return streams because they will allow for both stream processing and
iteration.
Item 48: Use caution when making streams parallel
Among mainstream languages, Java has always been at the forefront of
253
providing facilities to ease the task of concurrent programming. When Java
was released in 1996, it had built-in support for threads, with synchronization
and wait/notify. Java 5 introduced the java.util.concurrent
library, with concurrent collections and the executor framework. Java 7
introduced the fork-join package, a high-performance framework for parallel
decomposition. Java 8 introduced streams, which can be parallelized with a
single call to the parallel method. Writing concurrent programs in Java
keeps getting easier, but writing concurrent programs that are correct and fast
is as difficult as it ever was. Safety and liveness violations are a fact of life in
concurrent programming, and parallel stream pipelines are no exception.
Consider this program from Item 45:
Click here to view code image
// Stream-based program to generate the first 20
Mersenne primes
public static void main(String[] args) {
primes().map(p ->
TWO.pow(p.intValueExact()).subtract(ONE))
.filter(mersenne ->
mersenne.isProbablePrime(50))
.limit(20)
.forEach(System.out::println);
}
static Stream<BigInteger> primes() {
return Stream.iterate(TWO,
BigInteger::nextProbablePrime);
}
On my machine, this program immediately starts printing primes and takes
12.5 seconds to run to completion. Suppose I naively try to speed it up by
adding a call to parallel() to the stream pipeline. What do you think will
happen to its performance? Will it get a few percent faster? A few percent
slower? Sadly, what happens is that it doesn’t print anything, but CPU usage
spikes to 90 percent and stays there indefinitely (a liveness failure). The
program might terminate eventually, but I was unwilling to find out; I stopped
it forcibly after half an hour.
What’s going on here? Simply put, the streams library has no idea how to
parallelize this pipeline and the heuristics fail. Even under the best of
circumstances, parallelizing a pipeline is unlikely to increase its
performance if the source is from Stream.iterate, or the
intermediate operation limit is used. This pipeline has to contend with
both of these issues. Worse, the default parallelization strategy deals with the
unpredictability of limit by assuming there’s no harm in processing a few
254
extra elements and discarding any unneeded results. In this case, it takes
roughly twice as long to find each Mersenne prime as it did to find the
previous one. Thus, the cost of computing a single extra element is roughly
equal to the cost of computing all previous elements combined, and this
innocuous-looking pipeline brings the automatic parallelization algorithm to
its knees. The moral of this story is simple: Do not parallelize stream
pipelines indiscriminately. The performance consequences may be
disastrous.
As a rule, performance gains from parallelism are best on streams over
ArrayList, HashMap, HashSet, and ConcurrentHashMap
instances; arrays; int ranges; and long ranges. What these data
structures have in common is that they can all be accurately and cheaply split
into subranges of any desired sizes, which makes it easy to divide work
among parallel threads. The abstraction used by the streams library to perform
this task is the spliterator, which is returned by the spliterator method
on Stream and Iterable.
Another important factor that all of these data structures have in common is
that they provide good-to-excellent locality of reference when processed
sequentially: sequential element references are stored together in memory.
The objects referred to by those references may not be close to one another in
memory, which reduces locality-of-reference. Locality-of-reference turns out
to be critically important for parallelizing bulk operations: without it, threads
spend much of their time idle, waiting for data to be transferred from memory
into the processor’s cache. The data structures with the best locality of
reference are primitive arrays because the data itself is stored contiguously in
memory.
The nature of a stream pipeline’s terminal operation also affects the
effectiveness of parallel execution. If a significant amount of work is done in
the terminal operation compared to the overall work of the pipeline and that
operation is inherently sequential, then parallelizing the pipeline will have
limited effectiveness. The best terminal operations for parallelism are
reductions, where all of the elements emerging from the pipeline are
combined using one of Stream’s reduce methods, or prepackaged
reductions such as min, max, count, and sum. The short-circuiting
operations anyMatch, allMatch, and noneMatch are also amenable to
parallelism. The operations performed by Stream’s collect method,
which are known as mutable reductions, are not good candidates for
parallelism because the overhead of combining collections is costly.
If you write your own Stream, Iterable, or Collection
implementation and you want decent parallel performance, you must override
the spliterator method and test the parallel performance of the resulting
255
streams extensively. Writing high-quality spliterators is difficult and beyond
the scope of this book.
Not only can parallelizing a stream lead to poor performance,
including liveness failures; it can lead to incorrect results and
unpredictable behavior (safety failures). Safety failures may result from
parallelizing a pipeline that uses mappers, filters, and other programmersupplied function objects that fail to adhere to their specifications. The
Stream specification places stringent requirements on these function
objects. For example, the accumulator and combiner functions passed to
Stream’s reduce operation must be associative, non-interfering, and
stateless. If you violate these requirements (some of which are discussed in
Item 46) but run your pipeline sequentially, it will likely yield correct results;
if you parallelize it, it will likely fail, perhaps catastrophically.
Along these lines, it’s worth noting that even if the parallelized Mersenne
primes program had run to completion, it would not have printed the primes
in the correct (ascending) order. To preserve the order displayed by the
sequential version, you’d have to replace the forEach terminal operation
with forEachOrdered, which is guaranteed to traverse parallel streams in
encounter order.
Even assuming that you’re using an efficiently splittable source stream, a
parallelizable or cheap terminal operation, and non-interfering function
objects, you won’t get a good speedup from parallelization unless the pipeline
is doing enough real work to offset the costs associated with parallelism. As a
very rough estimate, the number of elements in the stream times the number
of lines of code executed per element should be at least a hundred thousand
[Lea14].
It’s important to remember that parallelizing a stream is strictly a
performance optimization. As is the case for any optimization, you must test
the performance before and after the change to ensure that it is worth doing
(Item 67). Ideally, you should perform the test in a realistic system setting.
Normally, all parallel stream pipelines in a program run in a common forkjoin pool. A single misbehaving pipeline can harm the performance of others
in unrelated parts of the system.
If it sounds like the odds are stacked against you when parallelizing stream
pipelines, it’s because they are. An acquaintance who maintains a
multimillion-line codebase that makes heavy use of streams found only a
handful of places where parallel streams were effective. This does not mean
that you should refrain from parallelizing streams. Under the right
circumstances, it is possible to achieve near-linear speedup in the number
of processor cores simply by adding a parallel call to a stream
pipeline. Certain domains, such as machine learning and data processing, are
particularly amenable to these speedups.
256
As a simple example of a stream pipeline where parallelism is effective,
consider this function for computing π(n), the number of primes less than or
equal to n:
Click here to view code image
// Prime-counting stream pipeline - benefits from
parallelization
static long pi(long n) {
return LongStream.rangeClosed(2, n)
.mapToObj(BigInteger::valueOf)
.filter(i -> i.isProbablePrime(50))
.count();
}
On my machine, it takes 31 seconds to compute π(10
8
) using this function.
Simply adding a parallel() call reduces the time to 9.2 seconds:
Click here to view code image
// Prime-counting stream pipeline - parallel version
static long pi(long n) {
return LongStream.rangeClosed(2, n)
.parallel()
.mapToObj(BigInteger::valueOf)
.filter(i -> i.isProbablePrime(50))
.count();
}
In other words, parallelizing the computation speeds it up by a factor of 3.7
on my quad-core machine. It’s worth noting that this is not how you’d
compute π(n) for large values of n in practice. There are far more efficient
algorithms, notably Lehmer’s formula.
If you are going to parallelize a stream of random numbers, start with a
SplittableRandom instance rather than a ThreadLocalRandom (or
the essentially obsolete Random). SplittableRandom is designed for
precisely this use, and has the potential for linear speedup.
ThreadLocalRandom is designed for use by a single thread, and will adapt
itself to function as a parallel stream source, but won’t be as fast as
SplittableRandom. Random synchronizes on every operation, so it will
result in excessive, parallelism-killing contention.
In summary, do not even attempt to parallelize a stream pipeline unless you
have good reason to believe that it will preserve the correctness of the
computation and increase its speed. The cost of inappropriately parallelizing a
stream can be a program failure or performance disaster. If you believe that
parallelism may be justified, ensure that your code remains correct when run
in parallel, and do careful performance measurements under realistic
257
conditions. If your code remains correct and these experiments bear out your
suspicion of increased performance, then and only then parallelize the stream
in production code.
258
Chapter 8. Methods
THIS chapter discusses several aspects of method design: how to treat
parameters and return values, how to design method signatures, and how to
document methods. Much of the material in this chapter applies to
constructors as well as to methods. Like Chapter 4, this chapter focuses on
usability, robustness, and flexibility.
Item 49: Check parameters for validity
Most methods and constructors have some restrictions on what values may be
passed into their parameters. For example, it is not uncommon that index
values must be non-negative and object references must be non-null. You
should clearly document all such restrictions and enforce them with checks at
the beginning of the method body. This is a special case of the general
principle that you should attempt to detect errors as soon as possible after they
occur. Failing to do so makes it less likely that an error will be detected and
makes it harder to determine the source of an error once it has been detected.
If an invalid parameter value is passed to a method and the method checks
its parameters before execution, it will fail quickly and cleanly with an
appropriate exception. If the method fails to check its parameters, several
things could happen. The method could fail with a confusing exception in the
midst of processing. Worse, the method could return normally but silently
compute the wrong result. Worst of all, the method could return normally but
leave some object in a compromised state, causing an error at some unrelated
point in the code at some undetermined time in the future. In other words,
failure to validate parameters, can result in a violation of failure atomicity
(Item 76).
For public and protected methods, use the Javadoc @throws tag to
document the exception that will be thrown if a restriction on parameter
values is violated (Item 74). Typically, the resulting exception will be
IllegalArgumentException, IndexOutOfBoundsException, or
NullPointerException (Item 72). Once you’ve documented the
restrictions on a method’s parameters and you’ve documented the exceptions
that will be thrown if these restrictions are violated, it is a simple matter to
enforce the restrictions. Here’s a typical example:
Click here to view code image
/**
259
* Returns a BigInteger whose value is (this mod m).
This method
* differs from the remainder method in that it always
returns a
* non-negative BigInteger.
*
* @param m the modulus, which must be positive
* @return this mod m
* @throws ArithmeticException if m is less than or
equal to 0
*/
public BigInteger mod(BigInteger m) {
if (m.signum() <= 0)
throw new ArithmeticException("Modulus <= 0: " +
m);
... // Do the computation
}
Note that the doc comment does not say “mod throws
NullPointerException if m is null,” even though the method does
exactly that, as a byproduct of invoking m.signum(). This exception is
documented in the class-level doc comment for the enclosing BigInteger
class. The class-level comment applies to all parameters in all of the class’s
public methods. This is a good way to avoid the clutter of documenting every
NullPointerException on every method individually. It may be
combined with the use of @Nullable or a similar annotation to indicate that
a particular parameter may be null, but this practice is not standard, and
multiple annotations are in use for this purpose.
The Objects.requireNonNull method, added in Java 7, is flexible
and convenient, so there’s no reason to perform null checks manually
anymore. You can specify your own exception detail message if you wish.
The method returns its input, so you can perform a null check at the same
time as you use a value:
Click here to view code image
// Inline use of Java's null-checking facility
this.strategy = Objects.requireNonNull(strategy,
"strategy");
You can also ignore the return value and use Objects.requireNonNull
as a freestanding null check where that suits your needs.
In Java 9, a range-checking facility was added to java.util.Objects.
This facility consists of three methods: checkFromIndexSize,
checkFromToIndex, and checkIndex. This facility is not as flexible as
the null-checking method. It doesn’t let you specify your own exception detail
260
message, and it is designed solely for use on list and array indices. It does not
handle closed ranges (which contain both of their endpoints). But if it does
what you need, it’s a useful convenience.
For an unexported method, you, as the package author, control the
circumstances under which the method is called, so you can and should
ensure that only valid parameter values are ever passed in. Therefore,
nonpublic methods can check their parameters using assertions, as shown
below:
Click here to view code image
// Private helper function for a recursive sort
private static void sort(long a[], int offset, int
length) {
assert a != null;
assert offset >= 0 && offset <= a.length;
assert length >= 0 && length <= a.length - offset;
... // Do the computation
}
In essence, these assertions are claims that the asserted condition will be true,
regardless of how the enclosing package is used by its clients. Unlike normal
validity checks, assertions throw AssertionError if they fail. And unlike
normal validity checks, they have no effect and essentially no cost unless you
enable them, which you do by passing the -ea (or -enableassertions)
flag to the java command. For more information on assertions, see the
tutorial [Asserts].
It is particularly important to check the validity of parameters that are not
used by a method, but stored for later use. For example, consider the static
factory method on page 101, which takes an int array and returns a List
view of the array. If a client were to pass in null, the method would throw a
NullPointerException because the method has an explicit check (the
call to Objects.requireNonNull). Had the check been omitted, the
method would return a reference to a newly created List instance that would
throw a NullPointerException as soon as a client attempted to use it.
By that time, the origin of the List instance might be difficult to determine,
which could greatly complicate the task of debugging.
Constructors represent a special case of the principle that you should check
the validity of parameters that are to be stored away for later use. It is critical
to check the validity of constructor parameters to prevent the construction of
an object that violates its class invariants.
There are exceptions to the rule that you should explicitly check a method’s
parameters before performing its computation. An important exception is the
case in which the validity check would be expensive or impractical and the
261
check is performed implicitly in the process of doing the computation. For
example, consider a method that sorts a list of objects, such as
Collections.sort(List). All of the objects in the list must be
mutually comparable. In the process of sorting the list, every object in the list
will be compared to some other object in the list. If the objects aren’t
mutually comparable, one of these comparisons will throw a
ClassCastException, which is exactly what the sort method should
do. Therefore, there would be little point in checking ahead of time that the
elements in the list were mutually comparable. Note, however, that
indiscriminate reliance on implicit validity checks can result in the loss of
failure atomicity (Item 76).
Occasionally, a computation implicitly performs a required validity check
but throws the wrong exception if the check fails. In other words, the
exception that the computation would naturally throw as the result of an
invalid parameter value doesn’t match the exception that the method is
documented to throw. Under these circumstances, you should use the
exception translation idiom, described in Item 73, to translate the natural
exception into the correct one.
Do not infer from this item that arbitrary restrictions on parameters are a
good thing. On the contrary, you should design methods to be as general as it
is practical to make them. The fewer restrictions that you place on parameters,
the better, assuming the method can do something reasonable with all of the
parameter values that it accepts. Often, however, some restrictions are
intrinsic to the abstraction being implemented.
To summarize, each time you write a method or constructor, you should
think about what restrictions exist on its parameters. You should document
these restrictions and enforce them with explicit checks at the beginning of
the method body. It is important to get into the habit of doing this. The
modest work that it entails will be paid back with interest the first time a
validity check fails.
Item 50: Make defensive copies when needed
One thing that makes Java a pleasure to use is that it is a safe language. This
means that in the absence of native methods it is immune to buffer overruns,
array overruns, wild pointers, and other memory corruption errors that plague
unsafe languages such as C and C++. In a safe language, it is possible to write
classes and to know with certainty that their invariants will hold, no matter
what happens in any other part of the system. This is not possible in
languages that treat all of memory as one giant array.
Even in a safe language, you aren’t insulated from other classes without
some effort on your part. You must program defensively, with the
262
assumption that clients of your class will do their best to destroy its
invariants. This is increasingly true as people try harder to break the security
of systems, but more commonly, your class will have to cope with unexpected
behavior resulting from the honest mistakes of well-intentioned programmers.
Either way, it is worth taking the time to write classes that are robust in the
face of ill-behaved clients.
While it is impossible for another class to modify an object’s internal state
without some assistance from the object, it is surprisingly easy to provide
such assistance without meaning to do so. For example, consider the
following class, which purports to represent an immutable time period:
Click here to view code image
// Broken "immutable" time period class
public final class Period {
private final Date start;
private final Date end;
/**
* @param start the beginning of the period
* @param end the end of the period; must not
precede start
* @throws IllegalArgumentException if start is
after end
* @throws NullPointerException if start or end is
null
*/
public Period(Date start, Date end) {
if (start.compareTo(end) > 0)
throw new IllegalArgumentException(
start + " after " + end);
this.start = start;
this.end = end;
}
public Date start() {
return start;
}
public Date end() {
return end;
}
... // Remainder omitted
}
At first glance, this class may appear to be immutable and to enforce the
263
invariant that the start of a period does not follow its end. It is, however, easy
to violate this invariant by exploiting the fact that Date is mutable:
Click here to view code image
// Attack the internals of a Period instance
Date start = new Date();
Date end = new Date();
Period p = new Period(start, end);
end.setYear(78); // Modifies internals of p!
As of Java 8, the obvious way to fix this problem is to use Instant (or
Local-DateTime or ZonedDateTime) in place of a Date because
Instant (and the other java.time classes) are immutable (Item 17).
Date is obsolete and should no longer be used in new code. That said, the
problem still exists: there are times when you’ll have to use mutable value
types in your APIs and internal representations, and the techniques discussed
in this item are appropriate for those times.
To protect the internals of a Period instance from this sort of attack, it is
essential to make a defensive copy of each mutable parameter to the
constructor and to use the copies as components of the Period instance in
place of the originals:
Click here to view code image
// Repaired constructor - makes defensive copies of
parameters
public Period(Date start, Date end) {
this.start = new Date(start.getTime());
this.end = new Date(end.getTime());
if (this.start.compareTo(this.end) > 0)
throw new IllegalArgumentException(
this.start + " after " + this.end);
}
With the new constructor in place, the previous attack will have no effect
on the Period instance. Note that defensive copies are made before
checking the validity of the parameters (Item 49), and the validity check
is performed on the copies rather than on the originals. While this may
seem unnatural, it is necessary. It protects the class against changes to the
parameters from another thread during the window of vulnerability between
the time the parameters are checked and the time they are copied. In the
computer security community, this is known as a time-of-check/time-of-use or
TOCTOU attack [Viega01].
Note also that we did not use Date’s clone method to make the
264
defensive copies. Because Date is nonfinal, the clone method is not
guaranteed to return an object whose class is java.util.Date: it could
return an instance of an untrusted subclass that is specifically designed for
malicious mischief. Such a subclass could, for example, record a reference to
each instance in a private static list at the time of its creation and allow the
attacker to access this list. This would give the attacker free rein over all
instances. To prevent this sort of attack, do not use the clone method to
make a defensive copy of a parameter whose type is subclassable by
untrusted parties.
While the replacement constructor successfully defends against the
previous attack, it is still possible to mutate a Period instance, because its
accessors offer access to its mutable internals:
Click here to view code image
// Second attack on the internals of a Period instance
Date start = new Date();
Date end = new Date();
Period p = new Period(start, end);
p.end().setYear(78); // Modifies internals of p!
To defend against the second attack, merely modify the accessors to return
defensive copies of mutable internal fields:
Click here to view code image
// Repaired accessors - make defensive copies of
internal fields
public Date start() {
return new Date(start.getTime());
}
public Date end() {
return new Date(end.getTime());
}
With the new constructor and the new accessors in place, Period is truly
immutable. No matter how malicious or incompetent a programmer, there is
simply no way to violate the invariant that the start of a period does not
follow its end (without resorting to extralinguistic means such as native
methods and reflection). This is true because there is no way for any class
other than Period itself to gain access to either of the mutable fields in a
Period instance. These fields are truly encapsulated within the object.
In the accessors, unlike the constructor, it would be permissible to use the
clone method to make the defensive copies. This is so because we know
that the class of Period’s internal Date objects is java.util.Date,
265
and not some untrusted subclass. That said, you are generally better off using
a constructor or static factory to copy an instance, for reasons outlined in Item
13.
Defensive copying of parameters is not just for immutable classes. Any
time you write a method or constructor that stores a reference to a clientprovided object in an internal data structure, think about whether the clientprovided object is potentially mutable. If it is, think about whether your class
could tolerate a change in the object after it was entered into the data
structure. If the answer is no, you must defensively copy the object and enter
the copy into the data structure in place of the original. For example, if you
are considering using a client-provided object reference as an element in an
internal Set instance or as a key in an internal Map instance, you should be
aware that the invariants of the set or map would be corrupted if the object
were modified after it is inserted.
The same is true for defensive copying of internal components prior to
returning them to clients. Whether or not your class is immutable, you should
think twice before returning a reference to an internal component that is
mutable. Chances are, you should return a defensive copy. Remember that
nonzero-length arrays are always mutable. Therefore, you should always
make a defensive copy of an internal array before returning it to a client.
Alternatively, you could return an immutable view of the array. Both of these
techniques are shown in Item 15.
Arguably, the real lesson in all of this is that you should, where possible,
use immutable objects as components of your objects so that you that don’t
have to worry about defensive copying (Item 17). In the case of our Period
example, use Instant (or LocalDateTime or ZonedDateTime),
unless you’re using a release prior to Java 8. If you are using an earlier
release, one option is to store the primitive long returned by
Date.getTime() in place of a Date reference.
There may be a performance penalty associated with defensive copying and
it isn’t always justified. If a class trusts its caller not to modify an internal
component, perhaps because the class and its client are both part of the same
package, then it may be appropriate to dispense with defensive copying.
Under these circumstances, the class documentation should make it clear that
the caller must not modify the affected parameters or return values.
Even across package boundaries, it is not always appropriate to make a
defensive copy of a mutable parameter before integrating it into an object.
There are some methods and constructors whose invocation indicates an
explicit handoff of the object referenced by a parameter. When invoking such
a method, the client promises that it will no longer modify the object directly.
A method or constructor that expects to take ownership of a client-provided
mutable object must make this clear in its documentation.
266
Classes containing methods or constructors whose invocation indicates a
transfer of control cannot defend themselves against malicious clients. Such
classes are acceptable only when there is mutual trust between a class and its
client or when damage to the class’s invariants would harm no one but the
client. An example of the latter situation is the wrapper class pattern (Item
18). Depending on the nature of the wrapper class, the client could destroy the
class’s invariants by directly accessing an object after it has been wrapped,
but this typically would harm only the client.
In summary, if a class has mutable components that it gets from or returns
to its clients, the class must defensively copy these components. If the cost of
the copy would be prohibitive and the class trusts its clients not to modify the
components inappropriately, then the defensive copy may be replaced by
documentation outlining the client’s responsibility not to modify the affected
components.
Item 51: Design method signatures carefully
This item is a grab bag of API design hints that don’t quite deserve items of
their own. Taken together, they’ll help make your API easier to learn and use
and less prone to errors.
Choose method names carefully. Names should always obey the standard
naming conventions (Item 68). Your primary goal should be to choose names
that are understandable and consistent with other names in the same package.
Your secondary goal should be to choose names consistent with the broader
consensus, where it exists. Avoid long method names. When in doubt, look to
the Java library APIs for guidance. While there are plenty of inconsistencies
—inevitable, given the size and scope of these libraries—there is also a fair
amount of consensus.
Don’t go overboard in providing convenience methods. Every method
should “pull its weight.” Too many methods make a class difficult to learn,
use, document, test, and maintain. This is doubly true for interfaces, where
too many methods complicate life for implementors as well as users. For each
action supported by your class or interface, provide a fully functional method.
Consider providing a “shorthand” only if it will be used often. When in
doubt, leave it out.
Avoid long parameter lists. Aim for four parameters or fewer. Most
programmers can’t remember longer parameter lists. If many of your methods
exceed this limit, your API won’t be usable without constant reference to its
documentation. Modern IDEs help, but you are still much better off with short
parameter lists. Long sequences of identically typed parameters are
especially harmful. Not only won’t users be able to remember the order of
the parameters, but when they transpose parameters accidentally, their
267
programs will still compile and run. They just won’t do what their authors
intended.
There are three techniques for shortening overly long parameter lists. One
is to break the method up into multiple methods, each of which requires only
a subset of the parameters. If done carelessly, this can lead to too many
methods, but it can also help reduce the method count by increasing
orthogonality. For example, consider the java.util.List interface. It
does not provide methods to find the first or last index of an element in a
sublist, both of which would require three parameters. Instead it provides the
subList method, which takes two parameters and returns a view of a
sublist. This method can be combined with the indexOf or lastIndexOf
method, each of which has a single parameter, to yield the desired
functionality. Moreover, the subList method can be combined with any
method that operates on a List instance to perform arbitrary computations
on sublists. The resulting API has a very high power-to-weight ratio.
A second technique for shortening long parameter lists is to create helper
classes to hold groups of parameters. Typically these helper classes are static
member classes (Item 24). This technique is recommended if a frequently
occurring sequence of parameters is seen to represent some distinct entity. For
example, suppose you are writing a class representing a card game, and you
find yourself constantly passing a sequence of two parameters representing a
card’s rank and its suit. Your API, as well as the internals of your class,
would probably benefit if you added a helper class to represent a card and
replaced every occurrence of the parameter sequence with a single parameter
of the helper class.
A third technique that combines aspects of the first two is to adapt the
Builder pattern (Item 2) from object construction to method invocation. If you
have a method with many parameters, especially if some of them are optional,
it can be beneficial to define an object that represents all of the parameters
and to allow the client to make multiple “setter” calls on this object, each of
which sets a single parameter or a small, related group. Once the desired
parameters have been set, the client invokes the object’s “execute” method,
which does any final validity checks on the parameters and performs the
actual computation.
For parameter types, favor interfaces over classes (Item 64). If there is
an appropriate interface to define a parameter, use it in favor of a class that
implements the interface. For example, there is no reason to ever write a
method that takes HashMap on input—use Map instead. This lets you pass in
a HashMap, a TreeMap, a ConcurrentHashMap, a submap of a
TreeMap, or any Map implementation yet to be written. By using a class
instead of an interface, you restrict your client to a particular implementation
268
and force an unnecessary and potentially expensive copy operation if the
input data happens to exist in some other form.
Prefer two-element enum types to boolean parameters, unless the
meaning of the boolean is clear from the method name. Enums make your
code easier to read and to write. Also, they make it easy to add more options
later. For example, you might have a Thermometer type with a static
factory that takes this enum:
Click here to view code image
public enum TemperatureScale { FAHRENHEIT, CELSIUS }
Not only does
Thermometer.newInstance(TemperatureScale.CELSIUS)
make a lot more sense than Thermometer.newInstance(true), but
you can add KELVIN to TemperatureScale in a future release without
having to add a new static factory to Thermometer. Also, you can refactor
temperature-scale dependencies into methods on the enum constants (Item
34). For example, each scale constant could have a method that took a
double value and converted it to Celsius.
Item 52: Use overloading judiciously
The following program is a well-intentioned attempt to classify collections
according to whether they are sets, lists, or some other kind of collection:
Click here to view code image
// Broken! - What does this program print?
public class CollectionClassifier {
public static String classify(Set<?> s) {
return "Set";
}
public static String classify(List<?> lst) {
return "List";
}
public static String classify(Collection<?> c) {
return "Unknown Collection";
}
public static void main(String[] args) {
Collection<?>[] collections = {
new HashSet<String>(),
new ArrayList<BigInteger>(),
new HashMap<String, String>().values()
269
};
for (Collection<?> c : collections)
System.out.println(classify(c));
}
}
You might expect this program to print Set, followed by List and
Unknown Collection, but it doesn’t. It prints Unknown Collection
three times. Why does this happen? Because the classify method is
overloaded, and the choice of which overloading to invoke is made at
compile time. For all three iterations of the loop, the compile-time type of the
parameter is the same: Collection<?>. The runtime type is different in
each iteration, but this does not affect the choice of overloading. Because the
compile-time type of the parameter is Collection<?>, the only applicable
overloading is the third one, classify(Collection<?>), and this
overloading is invoked in each iteration of the loop.
The behavior of this program is counterintuitive because selection among
overloaded methods is static, while selection among overridden methods
is dynamic. The correct version of an overridden method is chosen at
runtime, based on the runtime type of the object on which the method is
invoked. As a reminder, a method is overridden when a subclass contains a
method declaration with the same signature as a method declaration in an
ancestor. If an instance method is overridden in a subclass and this method is
invoked on an instance of the subclass, the subclass’s overriding method
executes, regardless of the compile-time type of the subclass instance. To
make this concrete, consider the following program:
Click here to view code image
class Wine {
String name() { return "wine"; }
}
class SparklingWine extends Wine {
@Override String name() { return "sparkling wine"; }
}
class Champagne extends SparklingWine {
@Override String name() { return "champagne"; }
}
public class Overriding {
public static void main(String[] args) {
List<Wine> wineList = List.of(
new Wine(), new SparklingWine(), new
270
Champagne());
for (Wine wine : wineList)
System.out.println(wine.name());
}
}
The name method is declared in class Wine and overridden in subclasses
SparklingWine and Champagne. As you would expect, this program
prints out wine, sparkling wine, and champagne, even though the
compile-time type of the instance is Wine in each iteration of the loop. The
compile-time type of an object has no effect on which method is executed
when an overridden method is invoked; the “most specific” overriding
method always gets executed. Compare this to overloading, where the
runtime type of an object has no effect on which overloading is executed; the
selection is made at compile time, based entirely on the compile-time types of
the parameters.
In the CollectionClassifier example, the intent of the program
was to discern the type of the parameter by dispatching automatically to the
appropriate method overloading based on the runtime type of the parameter,
just as the name method did in the Wine example. Method overloading
simply does not provide this functionality. Assuming a static method is
required, the best way to fix the CollectionClassifier program is to
replace all three overloadings of classify with a single method that does
explicit instanceof tests:
Click here to view code image
public static String classify(Collection<?> c) {
return c instanceof Set ? "Set" :
c instanceof List ? "List" : "Unknown
Collection";
}
Because overriding is the norm and overloading is the exception,
overriding sets people’s expectations for the behavior of method invocation.
As demonstrated by the CollectionClassifier example, overloading
can easily confound these expectations. It is bad practice to write code whose
behavior is likely to confuse programmers. This is especially true for APIs. If
the typical user of an API does not know which of several method
overloadings will get invoked for a given set of parameters, use of the API is
likely to result in errors. These errors will likely manifest themselves as
erratic behavior at runtime, and many programmers will have a hard time
diagnosing them. Therefore you should avoid confusing uses of overloading.
Exactly what constitutes a confusing use of overloading is open to some
271
debate. A safe, conservative policy is never to export two overloadings
with the same number of parameters. If a method uses varargs, a
conservative policy is not to overload it at all, except as described in Item 53.
If you adhere to these restrictions, programmers will never be in doubt as to
which overloading applies to any set of actual parameters. These restrictions
are not terribly onerous because you can always give methods different
names instead of overloading them.
For example, consider the ObjectOutputStream class. It has a variant
of its write method for every primitive type and for several reference types.
Rather than overloading the write method, these variants all have different
names, such as writeBoolean(boolean), writeInt(int), and
writeLong(long). An added benefit of this naming pattern, when
compared to overloading, is that it is possible to provide read methods with
corresponding names, for example, readBoolean(), readInt(), and
readLong(). The ObjectInputStream class does, in fact, provide
such read methods.
For constructors, you don’t have the option of using different names:
multiple constructors for a class are always overloaded. You do, in many
cases, have the option of exporting static factories instead of constructors
(Item 1). Also, with constructors you don’t have to worry about interactions
between overloading and overriding, because constructors can’t be
overridden. You will probably have occasion to export multiple constructors
with the same number of parameters, so it pays to know how to do it safely.
Exporting multiple overloadings with the same number of parameters is
unlikely to confuse programmers if it is always clear which overloading will
apply to any given set of actual parameters. This is the case when at least one
corresponding formal parameter in each pair of overloadings has a “radically
different” type in the two overloadings. Two types are radically different if it
is clearly impossible to cast any non-null expression to both types. Under
these circumstances, which overloading applies to a given set of actual
parameters is fully determined by the runtime types of the parameters and
cannot be affected by their compile-time types, so a major source of
confusion goes away. For example, ArrayList has one constructor that
takes an int and a second constructor that takes a Collection. It is hard
to imagine any confusion over which of these two constructors will be
invoked under any circumstances.
Prior to Java 5, all primitive types were radically different from all
reference types, but this is not true in the presence of autoboxing, and it has
caused real trouble. Consider the following program:
Click here to view code image
public class SetList {
272
public static void main(String[] args) {
Set<Integer> set = new TreeSet<>();
List<Integer> list = new ArrayList<>();
for (int i = -3; i < 3; i++) {
set.add(i);
list.add(i);
}
for (int i = 0; i < 3; i++) {
set.remove(i);
list.remove(i);
}
System.out.println(set + " " + list);
}
}
First, the program adds the integers from −3 to 2, inclusive, to a sorted set and
a list. Then, it makes three identical calls to remove on the set and the list. If
you’re like most people, you’d expect the program to remove the nonnegative values (0, 1, and 2) from the set and the list and to print [-3, -2,
-1] [-3, -2, -1]. In fact, the program removes the non-negative
values from the set and the odd values from the list and prints [-3, -2,
-1] [-2, 0, 2]. It is an understatement to call this behavior confusing.
Here’s what’s happening: The call to set.remove(i) selects the
overloading remove(E), where E is the element type of the set (Integer),
and autoboxes i from int to Integer. This is the behavior you’d expect,
so the program ends up removing the positive values from the set. The call to
list.remove(i), on the other hand, selects the overloading
remove(int i), which removes the element at the specified position in
the list. If you start with the list [-3, -2, -1, 0, 1, 2] and remove
the zeroth element, then the first, and then the second, you’re left with [-2,
0, 2], and the mystery is solved. To fix the problem, cast list.remove’s
argument to Integer, forcing the correct overloading to be selected.
Alternatively, you could invoke Integer.valueOf on i and pass the
result to list.remove. Either way, the program prints [-3, -2, -1]
[-3, -2, -1], as expected:
Click here to view code image
for (int i = 0; i < 3; i++) {
set.remove(i);
list.remove((Integer) i); // or
remove(Integer.valueOf(i))
}
The confusing behavior demonstrated by the previous example came about
273
because the List<E> interface has two overloadings of the remove
method: remove(E) and remove(int). Prior to Java 5 when the List
interface was “generified,” it had a remove(Object) method in place of
remove(E), and the corresponding parameter types, Object and int,
were radically different. But in the presence of generics and autoboxing, the
two parameter types are no longer radically different. In other words, adding
generics and autoboxing to the language damaged the List interface.
Luckily, few if any other APIs in the Java libraries were similarly damaged,
but this tale makes it clear that autoboxing and generics increased the
importance of caution when overloading.
The addition of lambdas and method references in Java 8 further increased
the potential for confusion in overloading. For example, consider these two
snippets:
Click here to view code image
new Thread(System.out::println).start();
ExecutorService exec = Executors.newCachedThreadPool();
exec.submit(System.out::println);
While the Thread constructor invocation and the submit method
invocation look similar, the former compiles while the latter does not. The
arguments are identical (System.out::println), and both the
constructor and the method have an overloading that takes a Runnable.
What’s going on here? The surprising answer is that the submit method has
an overloading that takes a Callable<T>, while the Thread constructor
does not. You might think that this shouldn’t make any difference because all
overloadings of println return void, so the method reference couldn’t
possibly be a Callable. This makes perfect sense, but it’s not the way the
overload resolution algorithm works. Perhaps equally surprising is that the
submit method invocation would be legal if the println method weren’t
also overloaded. It is the combination of the overloading of the referenced
method (println) and the invoked method (submit) that prevents the
overload resolution algorithm from behaving as you’d expect.
Technically speaking, the problem is that System.out::println is an
inexact method reference [JLS, 15.13.1] and that “certain argument
expressions that contain implicitly typed lambda expressions or inexact
method references are ignored by the applicability tests, because their
meaning cannot be determined until a target type is selected [JLS, 15.12.2].”
Don’t worry if you don’t understand this passage; it is aimed at compiler
writers. The key point is that overloading methods or constructors with
different functional interfaces in the same argument position causes
274
confusion. Therefore, do not overload methods to take different functional
interfaces in the same argument position. In the parlance of this item,
different functional interfaces are not radically different. The Java compiler
will warn you about this sort of problematic overload if you pass the
command line switch -Xlint:overloads.
Array types and class types other than Object are radically different.
Also, array types and interface types other than Serializable and
Cloneable are radically different. Two distinct classes are said to be
unrelated if neither class is a descendant of the other [JLS, 5.5]. For example,
String and Throwable are unrelated. It is impossible for any object to be
an instance of two unrelated classes, so unrelated classes are radically
different, too.
There are other pairs of types that can’t be converted in either direction
[JLS, 5.1.12], but once you go beyond the simple cases described above, it
becomes very difficult for most programmers to discern which, if any,
overloading applies to a set of actual parameters. The rules that determine
which overloading is selected are extremely complex and grow more complex
with every release. Few programmers understand all of their subtleties.
There may be times when you feel the need to violate the guidelines in this
item, especially when evolving existing classes. For example, consider
String, which has had a contentEquals(StringBuffer) method
since Java 4. In Java 5, CharSequence was added to provide a common
interface for StringBuffer, StringBuilder, String,
CharBuffer, and other similar types. At the same time that
CharSequence was added, String was outfitted with an overloading of
the contentEquals method that takes a CharSequence.
While the resulting overloading clearly violates the guidelines in this item,
it causes no harm because both overloaded methods do exactly the same thing
when they are invoked on the same object reference. The programmer may
not know which overloading will be invoked, but it is of no consequence so
long as they behave identically. The standard way to ensure this behavior is to
have the more specific overloading forward to the more general:
Click here to view code image
// Ensuring that 2 methods have identical behavior by
forwarding
public boolean contentEquals(StringBuffer sb) {
return contentEquals((CharSequence) sb);
}
While the Java libraries largely adhere to the spirit of the advice in this
item, there are a number of classes that violate it. For example, String
275
exports two overloaded static factory methods, valueOf(char[]) and
valueOf(Object), that do completely different things when passed the
same object reference. There is no real justification for this, and it should be
regarded as an anomaly with the potential for real confusion.
To summarize, just because you can overload methods doesn’t mean you
should. It is generally best to refrain from overloading methods with multiple
signatures that have the same number of parameters. In some cases, especially
where constructors are involved, it may be impossible to follow this advice.
In these cases, you should at least avoid situations where the same set of
parameters can be passed to different overloadings by the addition of casts. If
this cannot be avoided, for example, because you are retrofitting an existing
class to implement a new interface, you should ensure that all overloadings
behave identically when passed the same parameters. If you fail to do this,
programmers will be hard pressed to make effective use of the overloaded
method or constructor, and they won’t understand why it doesn’t work.
Item 53: Use varargs judiciously
Varargs methods, formally known as variable arity methods [JLS, 8.4.1],
accept zero or more arguments of a specified type. The varargs facility works
by first creating an array whose size is the number of arguments passed at the
call site, then putting the argument values into the array, and finally passing
the array to the method.
For example, here is a varargs method that takes a sequence of int
arguments and returns their sum. As you would expect, the value of sum(1,
2, 3) is 6, and the value of sum() is 0:
Click here to view code image
// Simple use of varargs
static int sum(int... args) {
int sum = 0;
for (int arg : args)
sum += arg;
return sum;
}
Sometimes it’s appropriate to write a method that requires one or more
arguments of some type, rather than zero or more. For example, suppose you
want to write a function that computes the minimum of its arguments. This
function is not well defined if the client passes no arguments. You could
check the array length at runtime:
Click here to view code image
276
// The WRONG way to use varargs to pass one or more
arguments!
static int min(int... args) {
if (args.length == 0)
throw new IllegalArgumentException("Too few
arguments");
int min = args[0];
for (int i = 1; i < args.length; i++)
if (args[i] < min)
min = args[i];
return min;
}
This solution has several problems. The most serious is that if the client
invokes this method with no arguments, it fails at runtime rather than compile
time. Another problem is that it is ugly. You have to include an explicit
validity check on args, and you can’t use a for-each loop unless you
initialize min to Integer.MAX_VALUE, which is also ugly.
Luckily there’s a much better way to achieve the desired effect. Declare the
method to take two parameters, one normal parameter of the specified type
and one varargs parameter of this type. This solution corrects all the
deficiencies of the previous one:
Click here to view code image
// The right way to use varargs to pass one or more
arguments
static int min(int firstArg, int... remainingArgs) {
int min = firstArg;
for (int arg : remainingArgs)
if (arg < min)
min = arg;
return min;
}
As you can see from this example, varargs are effective in circumstances
where you want a method with a variable number of arguments. Varargs were
designed for printf, which was added to the platform at the same time as
varargs, and for the core reflection facility (Item 65), which was retrofitted.
Both printf and reflection benefited enormously from varargs.
Exercise care when using varargs in performance-critical situations. Every
invocation of a varargs method causes an array allocation and initialization. If
you have determined empirically that you can’t afford this cost but you need
the flexibility of varargs, there is a pattern that lets you have your cake and
eat it too. Suppose you’ve determined that 95 percent of the calls to a method
have three or fewer parameters. Then declare five overloadings of the method,
277
one each with zero through three ordinary parameters, and a single varargs
method for use when the number of arguments exceeds three:
Click here to view code image
public void foo() { }
public void foo(int a1) { }
public void foo(int a1, int a2) { }
public void foo(int a1, int a2, int a3) { }
public void foo(int a1, int a2, int a3, int... rest) { }
Now you know that you’ll pay the cost of the array creation only in the 5
percent of all invocations where the number of parameters exceeds three. Like
most performance optimizations, this technique usually isn’t appropriate, but
when it is, it’s a lifesaver.
The static factories for EnumSet use this technique to reduce the cost of
creating enum sets to a minimum. This was appropriate because it was critical
that enum sets provide a performance-competitive replacement for bit fields
(Item 36).
In summary, varargs are invaluable when you need to define methods with
a variable number of arguments. Precede the varargs parameter with any
required parameters, and be aware of the performance consequences of using
varargs.
Item 54: Return empty collections or arrays, not nulls
It is not uncommon to see methods that look something like this:
Click here to view code image
// Returns null to indicate an empty collection. Don't
do this!
private final List<Cheese> cheesesInStock = ...;
/**
* @return a list containing all of the cheeses in the
shop,
* or null if no cheeses are available for purchase.
*/
public List<Cheese> getCheeses() {
return cheesesInStock.isEmpty() ? null
: new ArrayList<>(cheesesInStock);
}
There is no reason to special-case the situation where no cheeses are
available for purchase. Doing so requires extra code in the client to handle the
possibly null return value, for example:
278
Click here to view code image
List<Cheese> cheeses = shop.getCheeses();
if (cheeses != null && cheeses.contains(Cheese.STILTON))
System.out.println("Jolly good, just the thing.");
This sort of circumlocution is required in nearly every use of a method that
returns null in place of an empty collection or array. It is error-prone,
because the programmer writing the client might forget to write the specialcase code to handle a null return. Such an error may go unnoticed for years
because such methods usually return one or more objects. Also, returning
null in place of an empty container complicates the implementation of the
method returning the container.
It is sometimes argued that a null return value is preferable to an empty
collection or array because it avoids the expense of allocating the empty
container. This argument fails on two counts. First, it is inadvisable to worry
about performance at this level unless measurements have shown that the
allocation in question is a real contributor to performance problems (Item 67).
Second, it is possible to return empty collections and arrays without allocating
them. Here is the typical code to return a possibly empty collection. Usually,
this is all you need:
Click here to view code image
//The right way to return a possibly empty collection
public List<Cheese> getCheeses() {
return new ArrayList<>(cheesesInStock);
}
In the unlikely event that you have evidence suggesting that allocating
empty collections is harming performance, you can avoid the allocations by
returning the same immutable empty collection repeatedly, as immutable
objects may be shared freely (Item 17). Here is the code to do it, using the
Collections.emptyList method. If you were returning a set, you’d use
Collections.emptySet; if you were returning a map, you’d use
Collections.emptyMap. But remember, this is an optimization, and it’s
seldom called for. If you think you need it, measure performance before and
after, to ensure that it’s actually helping:
Click here to view code image
// Optimization - avoids allocating empty collections
public List<Cheese> getCheeses() {
return cheesesInStock.isEmpty() ?
Collections.emptyList()
: new ArrayList<>(cheesesInStock);
}
279
The situation for arrays is identical to that for collections. Never return null
instead of a zero-length array. Normally, you should simply return an array of
the correct length, which may be zero. Note that we’re passing a zero-length
array into the toArray method to indicate the desired return type, which is
Cheese[]:
Click here to view code image
//The right way to return a possibly empty array
public Cheese[] getCheeses() {
return cheesesInStock.toArray(new Cheese[0]);
}
If you believe that allocating zero-length arrays is harming performance,
you can return the same zero-length array repeatedly because all zero-length
arrays are immutable:
Click here to view code image
// Optimization - avoids allocating empty arrays
private static final Cheese[] EMPTY_CHEESE_ARRAY = new
Cheese[0];
public Cheese[] getCheeses() {
return cheesesInStock.toArray(EMPTY_CHEESE_ARRAY);
}
In the optimized version, we pass the same empty array into every toArray
call, and this array will be returned from getCheeses whenever
cheesesInStock is empty. Do not preallocate the array passed to
toArray in hopes of improving performance. Studies have shown that it is
counterproductive [Shipilëv16]:
Click here to view code image
// Don’t do this - preallocating the array harms
performance!
return cheesesInStock.toArray(new
Cheese[cheesesInStock.size()]);
In summary, never return null in place of an empty array or
collection. It makes your API more difficult to use and more prone to error,
and it has no performance advantages.
Item 55: Return optionals judiciously
Prior to Java 8, there were two approaches you could take when writing a
method that was unable to return a value under certain circumstances. Either
280
you could throw an exception, or you could return null (assuming the return
type was an object reference type). Neither of these approaches is perfect.
Exceptions should be reserved for exceptional conditions (Item 69), and
throwing an exception is expensive because the entire stack trace is captured
when an exception is created. Returning null doesn’t have these
shortcomings, but it has its own. If a method returns null, clients must
contain special-case code to deal with the possibility of a null return, unless
the programmer can prove that a null return is impossible. If a client neglects
to check for a null return and stores a null return value away in some data
structure, a NullPointerException may result at some arbitrary time in
the future, at some place in the code that has nothing to do with the problem.
In Java 8, there is a third approach to writing methods that may not be able
to return a value. The Optional<T> class represents an immutable
container that can hold either a single non-null T reference or nothing at all.
An optional that contains nothing is said to be empty. A value is said to be
present in an optional that is not empty. An optional is essentially an
immutable collection that can hold at most one element. Optional<T> does
not implement Collection<T>, but it could in principle.
A method that conceptually returns a T but may be unable to do so under
certain circumstances can instead be declared to return an Optional<T>.
This allows the method to return an empty result to indicate that it couldn’t
return a valid result. An Optional-returning method is more flexible and
easier to use than one that throws an exception, and it is less error-prone than
one that returns null.
In Item 30, we showed this method to calculate the maximum value in a
collection, according to its elements’ natural order.
Click here to view code image
// Returns maximum value in collection - throws
exception if empty
public static <E extends Comparable<E>> E
max(Collection<E> c) {
if (c.isEmpty())
throw new IllegalArgumentException("Empty
collection");
E result = null;
for (E e : c)
if (result == null || e.compareTo(result) > 0)
result = Objects.requireNonNull(e);
return result;
}
281
This method throws an IllegalArgumentException if the given
collection is empty. We mentioned in Item 30 that a better alternative would
be to return Optional<E>. Here’s how the method looks when it is
modified to do so:
Click here to view code image
// Returns maximum value in collection as an Optional<E>
public static <E extends Comparable<E>>
Optional<E> max(Collection<E> c) {
if (c.isEmpty())
return Optional.empty();
E result = null;
for (E e : c)
if (result == null || e.compareTo(result) > 0)
result = Objects.requireNonNull(e);
return Optional.of(result);
}
As you can see, it is straightforward to return an optional. All you have to
do is to create the optional with the appropriate static factory. In this program,
we use two: Optional.empty() returns an empty optional, and
Optional.of(value) returns an optional containing the given non-null
value. It is a programming error to pass null to Optional.of(value).
If you do this, the method responds by throwing a
NullPointerException. The Optional.ofNullable(value)
method accepts a possibly null value and returns an empty optional if null is
passed in. Never return a null value from an Optional-returning
method: it defeats the entire purpose of the facility.
Many terminal operations on streams return optionals. If we rewrite the
max method to use a stream, Stream’s max operation does the work of
generating an optional for us (though we do have to pass in an explicit
comparator):